{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ba0492eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# open file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3f09a43a",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = open(\"chakravardhan.docx.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a0b2d074",
   "metadata": {},
   "outputs": [],
   "source": [
    "#read file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "f808a6f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = open(\"chakravardhan.docx.txt\",mode = 'r')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "24dd1536",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\ufeffCLUSTERED FOREST FIRE PREDICTION USING DEEP LEARNING WITH EFFECTIVE SCALING AND THRESHOLD BASED FEATURE SELECTION\\nA PROJECT REPORT\\nSubmitted by\\nRAMANNAGARI CHAKRAVARDHAN [RA2111003010767]\\n       PERVATHANENI LIKITH CHOWDARY [RA2111003010789]\\nUnder the Guidance of\\nDr. M. SENTHIL RAJA\\nAssistant Professor, Department of Computing Technologies\\n\\n\\nin partial fulfillment of the requirements for the degree of\\n\\n\\nBACHELOR OF TECHNOLOGY\\nin\\nCOMPUTER SCIENCE AND ENGINEERING\\n\\n\\n\\n\\n Logo, company name  Description automatically generated \\n\\n\\n\\nDEPARTMENT OF COMPUTING TECHNOLOGIES COLLEGE OF ENGINEERING AND TECHNOLOGY SRM INSTITUTE OF SCIENCE AND TECHNOLOGY KATTANKULATHUR– 603 203\\nMAY 2025\\n Logo \\n\\n\\n\\nSRM INSTITUTE OF SCIENCE AND TECHNOLOGY KATTANKULATHUR–603 203\\nBONAFIDE CERTIFICATE\\n\\n\\nCertified that 18CSP109L / I8CSP111L project report titled “CLUSTERED FOREST FIRE PREDICTION USING DEEPLEARNING WITH EFFECTIVE THRESHOLD BASED FEATURE SELECTION” is the Bonafide work of RAMANNAGARI CHAKRAVARDHAN [RA2111003010767] and PERVATHANENI LIKITH CHOWDARY [RA2111003010767] who carried out the project work under my supervision. Certified further, that to the best of my knowledge the work reported here in does not form part of any other thesis or dissertation on the basis of which a degree or award was conferred on an earlier occasion for this or any other candidate.\\n\\n\\n\\n\\n\\n\\nDr. M.SENTHIL.RAJA\\nSUPERVISOR\\nAssistant Professor\\nDepartment of Computing Technologies\\n________________\\nDr. B.SIVAKUMAR\\nPANEL HEAD\\nAssociate Professor\\nDepartment of Computing Technologies\\n\\n\\n\\n\\n\\n\\nDr. G.NIRANJANA\\nHEAD OF THE DEPARTMENT\\nDepartment of Computing Technologies\\n\\n\\n\\n\\n\\n\\nINTERNAL EXAMINER        EXTERNAL EXAMINER\\n\\n\\nDepartment of Computing Technologies Logo \\nSRM Institute of Science and Technology Own Work Declaration Form\\nDegree/Course        : B.Tech in Computer Science and Engineering Student Names        : CHAKRAVARDHAN,LIKITH CHOWDARY\\nRegistration Number: RA2111003010767,RA2111003010789\\n\\n\\nTitle of Work        : Clustered Forest Fire Prediction using deep learning with effective scaling and threshold-based feature selection.\\nI/We here by certify that this assessment compiles with the University’s Rules and Regulations relating to Academic misconduct and plagiarism, as listed in the University Website, Regulations, and the Education Committee guidelines.\\nI / We confirm that all the work contained in this assessment is our own except where indicated, and that we have met the following conditions:\\n* Clearly references / listed all sources as appropriate\\n* Referenced and put in inverted commas all quoted text(from books, web,etc.)\\n* Given the sources of all pictures, data etc that are not my own.\\n* Not made any use of the report(s) or essay(s) of any other student(s) either past or present\\n* Acknowledged in appropriate places any help that I have received from others(e.g fellow students, technicians, statisticians, external sources)\\n* Compiled with any other plagiarism criteria specified in the Course hand book / University website\\nI understand that any false claim for this work will be penalized in accordance with the University policies and regulations.\\n\\n\\nDECLARATION:\\n\\tI am aware of and understand the University’s policy on Academic misconduct and plagiarism and I certify that this assessment is my / our own work, except where indicated by referring, and that I have followed the good academic practices noted above.\\nchakravardhan Signature: likith chowdary Signature: Date:\\n\\tIf you are working in a group, please write your registration numbers and sign with the date for every student in your group.\\n\\t\\n\\nACKNOWLEDGEMENT              \\nWe express our humble gratitude to Dr. C. Muthamizhchelvan, Vice-Chancellor, SRM Institute of Science and Technology, for the facilities extended for the project work and his continued support.\\nWe extend our sincere thanks to Dr.LEENUS JESU MARTIN, Dean-CET, SRM Institute of Science and Technology, for his invaluable support. \\n We wish to thank Dr. Revathi Venkataraman, Professor and Chairperson, School of Computing, SRM Institute of Science and Technology, for her support throughout the project work. \\n We encompass our sincere thanks to, Dr. M. Pushpalatha, Professor and Associate Chairperson - CS, School of Computing, for their invaluable support.\\n We are incredibly grateful to our Head of the DEPARTMENT, Dr. G Niranjana, Professor & Head, SRM Institute of Science and Technology, for her suggestions and encouragement at all the stages of the project work. \\nWe want to convey our thanks to our Project Coordinators ,M.Senthil Raja, B.Sivakumar J Nithyashri DEPARTMENT of COMPUTING TECHNOLOGIES, SRM Institute of Science and Technology, for their inputs during the project reviews and support.\\nOur inexpressible respect and thanks to our guide, Dr. M.Senthil Raja , DEPARTMENT of COMPUTING TECHNOLOGIES, SRM Institute of Science and Technology, for providing us with an opportunity to pursue our project under his / her mentorship. He / She provided us with the freedom and support to explore the research topics of our interest. His / Her passion for solving problems and making a difference in the world has always been inspiring. \\nWe sincerely thank all the staff members of COMPUTING TECHNOLOGIES, School of Computing, S.R.M Institute of Science and Technology, for their help during our project. Finally, we would like to thank our parents, family members, and friends for their unconditional love, constant support and encouragement.\\n\\n\\n                                         RAMANNAGARICHAKRAVARDHAN[RA2111003010767]\\n\\n\\n                                                   PERVATHANENILIKITHCHOWDARY[RA2111003010789]\\nABSTRACT\\n\\n\\nEcosystems that are prone to wildfires as well as those that are not have been devastated. We can anticipate a reorganization in many ecosystems around the world, possibly shifting towards a rise in federal species, as a result of the components of the fire regime changing due to global change (including the frequency, intensity, and timing of forest fires). Fire is one of the most common factors in deforestation and species extinction on a global scale. It\\'s not always possible to contain a fire and get to the wooded region in time. As a result, there is frequently a lot of destruction. As a result, it is crucial to anticipate fires and act quickly when they occur. The natural vegetation and forest life suffer from a number of damaging repercussions as a result of forest fires. Everyone\\'s lives and the environment are significantly impacted by forest fires. Several ecosystems, including grasslands and temperate forests, depend heavily on forest fires. Optimizing the situation will be made easier with the capacity to anticipate the potential forest fire\\'s location. The major goal of this study is to demonstrate how a machine learning and deep learning algorithm may be used to estimate the risk of forest fires using weather data. The major goal is to foretell the likelihood of a forest fire and its intensity under particular climatic circumstances in a specific place. Assessing the likelihood of forest wildfires can help in forest wildfire prevention, management, and supervision because forest wildfires are a significant disturbance element in forest ecosystems.\\nTABLE OF CONTENTS\\n\\n\\n\\n\\nABSTRACT\\n\\tvi\\n\\t\\n\\n\\tLIST OF TABLES\\n\\tix\\n\\t\\n\\n\\tLIST OF FIGURES\\n\\tx\\n\\t\\n\\n\\tLIST OF EQUATIONS\\n\\txi\\n\\t\\n\\n\\tABBREVATIONS\\n\\txii\\n\\t1.\\n\\tINTRODUCTION\\n\\t1\\n\\t\\n\\n\\t1.1\\n\\tOverview\\n\\t1\\n\\t\\n\\n\\t1.2\\n\\tObjective\\n\\t1\\n\\t\\n\\n\\t1.3\\n\\tContribution\\n\\t2\\n\\t\\n\\n\\t1.4\\n\\tHardware Requirements\\n\\t2\\n\\t\\n\\n\\t1.5\\n\\tSoftware Requirements\\n\\t3\\n\\t2\\n\\tLITERATURE SURVEY\\n\\t4\\n\\t3\\n\\tSYSTEM ARCHITECTURE AND MODULES\\n\\t11\\n\\t\\n\\n\\t3.1\\n\\tArchitecture Diagram\\n\\t11\\n\\t\\n\\n\\t3.2\\n\\tData Flow Diagram\\n\\t12\\n\\t\\n\\n\\t3.3\\n\\tModules\\n\\t13\\n\\t\\n\\n\\t\\n\\n\\t3.3.1  Clustering\\n\\t13\\n\\t\\n\\n\\t\\n\\n\\t3.3.2        Feature Scaling\\n\\t13\\n\\t\\n\\n\\t\\n\\n\\t3.3.3        Feature Selection\\n\\t14\\n\\t\\n\\n\\t\\n\\n\\t3.3.4        Neural Network\\n\\t14\\n\\t4\\n\\tMETHODOLOGY\\n\\t16\\n\\t\\n\\n\\t4.1\\n\\tData Preprocessing\\n\\t16\\n\\t\\n\\n\\t\\n\\n\\t4.1.1  Clustering\\n\\t16\\n\\t\\n\\n\\t\\n\\n\\t4.1.2  Feature Scaling\\n\\t18\\n\\t\\n\\n\\t\\n\\n\\t4.1.3  Data Exploration\\n\\t21\\n\\t\\n\\n\\t4.2\\n\\tFeature Selection\\n\\t22\\n\\t\\n\\n\\t4.3\\n\\tDeep Neural Network\\n\\t23\\n\\t5\\n\\tRESULTS AND DISCUSSION\\n\\t26\\n\\t\\n\\n\\t5.1\\n\\tExperimental Setup\\n\\t26\\n\\t\\n\\n\\t5.2\\n\\tDataset and Exploration\\n\\t26\\n\\t\\n\\n\\t5.3\\n\\tExperimental Discussion\\n\\t38\\n\\t\\n\\n\\t\\n\\n\\t5.3.1  Experiment-1 Clustering\\n\\t38\\n\\t\\n\\n\\n\\n\\n\\n\\t\\n\\n\\t5.3.2  Experiment-2 Feature Scaling\\n\\t39\\n\\t\\n\\n\\t\\n\\n\\t5.3.3  Experiment-3 Feature Selection\\n\\t40\\n\\t\\n\\n\\t\\n\\n\\t5.3.4  Experiment-4 Classification\\n\\t43\\n\\t\\n\\n\\t5.5\\n\\tComparative Study\\n\\t49\\n\\t6\\n\\tCONCLUSION AND FUTURE SCOPE\\n\\t50\\n\\t\\n\\n\\tREFERENCES\\n\\t51\\n\\t\\n\\n\\tAPPENDIX\\n\\t55\\n\\t\\n\\n\\tPLAGIARISM REPORT\\n\\t\\n\\n\\t\\n\\n\\tJOURNAL PUBLICATION\\n\\t\\n\\n\\t\\n\\nLIST OF TABLES\\n\\n\\n\\n\\n\\n\\n1.1\\n\\tHardware Requirements…………………………………….……………..\\n\\t5\\n\\t1.2\\n\\tSoftware Requirments……….……………………………….……………\\n\\t40\\n\\t5.1\\n\\tComparative Study of Montesinho Natural park dataset….....…………….\\n\\t48\\n\\t5.2\\n\\tComparative Study of Algerian forest fires dataset………..………………\\n\\t48\\n\\t\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n‘\\nLIST OF FIGURES\\n\\n\\n\\n\\n3.1\\n\\tArchitectural Design of Forest fire prediction……. ………………………………\\n\\t11\\n\\t3.2\\n\\tData Flow Diagram………………………..............................................................\\n\\t12\\n\\t4.1\\n\\tArchitecture Diagram of the Neural Network…..…………………………...…….\\n\\t25\\n\\t5.1\\n\\tFire Weather Observation….....................................................................................\\n\\t27\\n\\t5.2\\n\\tMontensinho Natural Park Dataset Sample..............................................................\\n\\t27\\n\\t5.3\\n\\tInformation of Montensinho Natural Park Dataset Sample……………………….\\n\\t28\\n\\t5.4\\n\\tHistogram of Montesinho Natural Park Dataset…………………………………..\\n\\t29\\n\\t5.5\\n\\tHistogram of rain and area………………………………………………………...\\n\\t30\\n\\t5.6\\n\\tDescriptive Statistics of Montesinho Natural Dataset sample…………………….\\n\\t30\\n\\t5.7\\n\\tCorrelation Analysis of Montesinho Natural Park Dataset………………………..\\n\\t31\\n\\t5.8\\n\\tAlgeriam Forest Fires Dataset Sample…………………………………………….\\n\\t32\\n\\t5.9\\n\\tInformation of Algerian Forest Fires Dataset Sample…………………………….\\n\\t33\\n\\t5.10\\n\\tHistogram of Algerian Forest Fires Dataset………………………………………\\n\\t34\\n\\t5.11\\n\\tDescriptive Statistics of Algerian Forest Fires Dataset……………………………\\n\\t35\\n\\t5.12\\n\\tCorrelation Analysis of Algerian Forest Fires Dataset…………………………….\\n\\t36\\n\\t5.13\\n\\tElbow Method for Montesinho Natural Park dataset Sample……………………..\\n\\t37\\n\\t5.14\\n\\tComparison of data point cluster between 2 and 9………………………………..\\n\\t38\\n\\t5.15\\n\\tClustered Dataset of Montesinho Natural Park dataset……………………………\\n\\t38\\n\\t5.16\\n\\tFeature Scaling of Montesinho Natural Park Dataset……………………………..\\n\\t39\\n\\t5.17\\n\\tFeature Scaling of Algerian Forest Fires Dataset………………………………….\\n\\t39\\n\\t5.18\\n\\tAccuracy Table using Logistic Regression for montesinho natural park dataset….\\n\\t40\\n\\t5.19\\n\\tAccuracy Table using Logistic Regression for Algerian Forest Fires dataset…….\\n\\t40\\n\\t5.20\\n\\tAccuracy Table using KNN for montesinho natural park dataset…………………\\n\\t41\\n\\t5.21\\n\\tAccuracy Table using KNN for Algerian Forest Fires dataset…………………….\\n\\t41\\n\\t5.22\\n\\tAccuracy Table using SVM for montesinho natural park dataset…………………\\n\\t41\\n\\t5.23\\n\\tAccuracy Table using SVM for Algerian Forest Fires dataset…………………….\\n\\t42\\n\\t5.24\\n\\tAccuracy curve of Montesinho Natural Park Dataset……………………………..\\n\\t43\\n\\t5.25\\n\\tBatch Size Accuracy curves for the Montesinho Natural Park Dataset…………...\\n\\t44\\n\\t5.26\\n\\tAccuracy curve of the Algerian Forest Fires Dataset……………………………...\\n\\t45\\n\\t5.27\\n\\tBatch Size Accuracy curves for the Algerian Forest Fires Dataset……………….\\n\\t46\\n\\t\\n\\nLIST OF EQUATIONS\\n\\n\\n4.1\\n\\tDistance to each centroid and data points………………………………………..\\n\\t16\\n\\t4.2\\n\\tCentre of the data points assigned to each centroid………………………………\\n\\t16\\n\\t4.3\\n\\tWithin cluster sum of squares……………………………………………………\\n\\t17\\n\\t4.4\\n\\tSilhouette Score………………………………………………………………….\\n\\t17\\n\\t4.5\\n\\tStandardization equation…………………………………………………………\\n\\t18\\n\\t4.6\\n\\tMin Max Scaling Equation………………………………………………………\\n\\t18\\n\\t4.7\\n\\tMax-Absolute Scaling Equation…………………………………………………\\n\\t19\\n\\t4.8\\n\\tRobust Scaling Equation…………………………………………………………\\n\\t19\\n\\t4.9\\n\\tBox-Cox Transformation…………………………………………………………\\n\\t19\\n\\t4.10\\n\\tYeo-Johnson Transformation for x>=0…………………………………………..\\n\\t20\\n\\t4.11\\n\\tYeo-Johnson Transformation for x<0……………………………………………\\n\\t20\\n\\t4.12\\n\\tRoot Mean Squared Error………………………………………………………..\\n\\t20\\n\\t4.13\\n\\tChi Square Test Formula…………………………………………………………\\n\\t22\\n\\t4.14\\n\\tPearson Correlation Formula…………………………………………………….\\n\\t23\\n\\t4.15\\n\\tWeighted sum of the inputs………………………………………………………\\n\\t24\\n\\t\\n\\nABBREVIATIONS\\n\\n\\n\\n\\nANN\\n\\tArtificial Neural Network\\n\\tBUI\\n\\tBuildup Index\\n\\tCNN\\n\\tConvolution Neural Network\\n\\tDC\\n\\tDrought Code\\n\\tDMC\\n\\tDuff Moisture Code\\n\\tFFMC\\n\\tFine Fuel Moisture Code\\n\\tFWI\\n\\tFire Weather Index\\n\\tISI\\n\\tInitial Speed Index\\n\\tKNN\\n\\tK Nearest Linea Neighbors\\n\\tReLU\\n\\tRectified Linear Unit\\n\\tRMSE\\n\\tRoot Mean Square Error\\n\\tSSD\\n\\tSum of the Squared Distances\\n\\tSVM\\n\\tSupport Vector Machine\\n\\tWCSS\\n\\tWithin-cluster Sum of Squares\\n\\t\\n\\nCHAPTER 1 INTRODUCTION\\n   1. Overview\\nForest fires have recently risen to the top of the list of natural disasters that have been known to wipe off hectares of forest. They have a huge influence on the environment, ecological systems, and the ecology of a region by endangering not just the forest materials but also the entire ecosystem. When it fails to rain for several months in the summer, it is full of dwellings that are prone to catching fire from a single spark and dry, senescent leaven. The main contributing elements for fire in the forest is the fact that global warming is the main cause of high temperature in Earth\\'s surface. Some additional reasons also include rain, flooding, and ignorance of normal citizens.\\nFire-related deforestation can have a variety of bad effects on human civilization. In order to effectively combat forest fires, early discovery is crucial. No later than six minutes after fire begins, the forest fire department would be alerted to the danger., according to several characteristics of forest fires, in order to put out the fire without permanently harming the forest.Prediction of fire in forest takes into account of atmospheric conditions, environmental variables, the atmosphere, the dryness of flameproof materials, the forms of flammable materials, and ignition sources when assessing and forecasting the combustion hazards of combustible objects in the wood.\\n   2. Objective\\nRecent years have seen an increase in worry about the rise of fire in forest, which leads to catastrophic repercussions affect both human beings and nature. For forest fires to be prevented and their effects reduced, forecasting is essential. Forest fires may be accurately and quickly predicted, which can assist authorities in taking the right actions to prevent their spread and guarantee public and environmental safety. Hence, as our goal is to develop a deep learning project for precise fire prediction, clustering can be utilized to gather related data points, which can aid in discovering patterns and trends that can help with fire prediction. In order to ensure that various features are given equal weight during analysis and prediction, feature scaling is crucial. The most important features for forest fire prediction can be found with the aid of feature selection, which can increase accuracy and reduce noise. Accuracy of predicting forest fires can be increased by using deep learning, which can assist in recognising complicated patterns and correlations between various information.\\nAs a result, the accuracy of forest fire prediction can be greatly increased by combining clustering, feature scaling, feature selection, and deep learning approaches. This can decrease the\\nharm that forest fires cause to the environment and to people. Authorities can take the required actions to stop the spread of forest fires and safeguard both the environment and the populace by properly forecasting their occurrence.\\n   3. Contribution\\nIn this research, we propose a approach to predicting forest fires using a neural network model that incorporates effective feature scaling and threshold-based feature selection. Our work contributes in the following aspects:\\n\\n\\nFirstly, we introduced a clustered approach to forest fire prediction, which enables us to better capture the spatial distribution of fire incidents within a forest. By clustering the forest into different zones, we can train our neural network model to learn the unique features and patterns associated with each zone. This improves the accuracy of our predictions, as our model is better equipped to handle the variability and heterogeneity present in different parts of the forest.\\n\\n\\nSecondly, we applied effective feature scaling and threshold-based feature selection techniques to our model, which improved its performance significantly. By normalizing our input features and selecting only the most relevant ones based on a predetermined threshold, we reduced noise in our data. We also improved our model\\'s accuracy and robustness. Overall, our proposed approach represents a significant contribution to forest fire prediction, as it leverages neural networks and advanced data pre-processing techniques to achieve more accurate and reliable predictions.\\n\\n\\n   4. Hardware Requirements\\n\\n\\nSpecification\\n\\tValue\\n\\tDevice Required\\n\\tLaptop/Desktop\\n\\tOperating System\\n\\tWindows/Mac/Linux/Any\\n\\tMemory(RAM)\\n\\t4GB or more\\n\\tCPU\\n\\tGreater than 1.5GHz\\n\\tInternet\\n\\tLAN Connection, Wireless Connection(Wi-Fi)\\n\\tTable 1.1 Hardware Requirements\\n   5. Software Requirements\\n\\n\\nSPECIFICACATION\\n\\tPACKAGE\\n\\tEditor\\n\\tGoogle Collab/Jupyter Notebook/Visual Studio\\n\\tFramework\\n\\tData-driven Testing\\n\\tDataset\\n\\tKaggle\\n\\tTable 1.2 Software Requirements\\nCHAPTER 2 LITERATURE REVIEW\\nThe literature review is a crucial section of the report since it directs the path of the investigation. Setting a purpose for your investigation and generating a problem statement can be helpful. Additionally, it involves a methodical and exhaustive investigation of all published literature and other sources. Here are a few studies that are linked to this one:\\n      * Jang Hyun Sunga, Young Ryub, and Kee-Won Seong created a DBN model with thhe help of Deep Learning which would help in Prediction of Occurrence of fire taking into account the climatic Condition and Drought Phase across the region to be able to forecast the likelihood of fire in a certain location. They chose DBN model because traditional regression models had limits in forecasting the characteristics of forest fires since They believed there to be a linear relationship between weather conditions and forest fires, but the DBN model can account for its nonlinearity. Meteorological parameters like drought indices were created to enhance DBN performance since the OFs in the spring have been increasing. SPI and SPEI were calculated during periods of 3, 6, and 9 months, and the relationship amongst both of these dryness parameters and OF. The efficacy of the model\\'s prediction utilising the drought indicators was evaluated, and this relationship was tested using DBN.\\nBlock-stacked was created with the help of an unsupervised network known as Restricted Boltzmann Machine, which makes up the DBN.A particular class of stochastic model called the RBM can be trained to learn a probability across its input data. The DBN training used the greedy level- wise training technique to address the gradient disappearance that most deep learning has experienced. Hidden levels of the RBM contain no connections other than those between layers, while the first two levels\\' interconnections are free of direction.They have benefits like initializing the network and preventing overfitting issues and poor local optima through unsupervised pre-training. As a result, the DBN model trains the weight in the lowest layer, which is close to the input layer.\\nThe correlations with relevant variables were examined so as to understand the relationship among meteorological factors, the dryness phase, the ignition frequency of occurrence of fire in forest. Hence, it’s established that a significant link exists between OF & climate-related variables. Among the climate factors, the association between the OF and RH, WS was particularly strong because low humidity speeds up the burning of forest fires and causes vegetation to dry up. As the wind picks up, a forest fire spreads more quickly.\\n      * Pranati Rakhit, Sreshta Sarkar,Sambit Khan,PritamSaha,Sonali Bhatacharya,Sardar Islam,Nilarpan De, Sovik Pal in Forest Fire Prediction Using Machine Learning Algorithms estimates\\nthe level of threat of fire in a particular forest. The goal of the particular method is to anticipate the threat of fire in a particular forest by categorizing a given area as most susceptible, medium susceptible, low susceptible, or not susceptible to fire at all.To predict the risk zone as well as the medium, low, and no risk zones, they applied various machine learning classifiers. A comparison study has also been conducted to see which algorithm provides the highest level of accuracy for predicting forest fires. Using the algorithm, area under the curve was found out and integrated with confusion matrix to find out accuracy of each classifier so as to decide accordingly.\\nThis study analyzed four different classification model types, calculated each one\\'s accuracy, and came to a point that Decision Tree is the most suitable method for their dataset since it properly classifies more data than the other classifiers. The decision tree has a good accuracy rate as well. The test dataset demonstrates that the decision tree achieves a higher categorization rate. The decision tree is the most effective classification for this prediction, according to this study\\'s evaluation of four distinct types of classification models and computation of each one\\'s accuracy. This conclusion was reached as a consequence of the study\\'s findings and implications. Here, they made a prediction on the likelihood of a forest fire based on its severity under particular climatic circumstances in a particular place.\\n      * Priti T, Dr. Suvarna Kankaradi, Aiswarya Belagi, S Malagi, Aiswarya Sudii using Machine Learning methods focuses on various regression techniques, from there they took out the best one to predict fire. Random forest, Decision tree, and SVR are the regression techniques used for prediction. The Proposed System explains how they used the meteorological dataset to perform an exploratory analysis, pre-processing in which they attempted to eliminate outliers and convert classification data to numeric based data to make the dataset easier to interpret. Using those machine learning models to forecast likelihood of occurrence of fire and notify closest station. The steps in this study are as follows: Data collecting is completed, and then data pre-processing occurs so that a dataset may be created in a standard format. After data preparation, a suitable model from a range of regression techniques should be chosen based on the dataset. Model evaluation follows model implementation. Finally, predicting the data and each model\\'s precision.\\nThe gathered information is utilized to train the system and make predictions. They examined the relationship between temperature, humidity, precipitation, wind speed, and other factors to forecast forest fires. For models using random forest, svr, and decision tree, accuracy and MAE have been calculated. A regression curve\\'s mean square error indicates how closely it resembles a set of points. This is accomplished by squaring length given between the points and the curve of regression. The squaring is necessary to eliminate any unfavorable indicators. It also provides many weights for\\nlarger variations.\\n\\n\\nThe results of experiments are used to set a variable number of training examples and evaluation instances for forest fire prediction. In this study, factors influencing frequency of fire are examined. It takes into account elements like temperature, RH, and speed of wind. High temperatures, low humidity, and strong winds all dramatically increase the risk of burning. Also, it has been discovered that there are more forest fires than in other types of surface areas. Data mining techniques should be utilized to predict forest fires because of the forest\\'s elevated risk of such occurrences.\\n      * Ditipiya Sinha,Vikram Raj,Debasish Dey and A K Das in forest fire detection using energy efficient data forwarding and localization methods in WSN focuses on to consider the connectivity of different types of sensor nodes like GPS linked nodes and nodes which doesn’t have GPS connections, in order to build a model that is cost-effective. Use an SVM-based localization algorithm to locate nodes that are not connected to the GPS. Compared to current localization procedures, it lowers the localization error. The forest can be divided into three zones: High Activity, Mid Activity, and Low Activity. Status of each zone\\'s forest fire is determined using the planned Integrated Rule- based forecasting model. Comparing the suggested model to other traditional models already in use, zone prediction accuracy is greater in the proposed model. It results in quick forest fire detection and improves energy efficiency.\\nIn order to lower cost of sensor deployment, this research suggests a novel localization strategy. In this strategy, unknown node locations are approximated using Support Vector Machine model with lower error than any methods currently in use. Data collection in the forest can be carried out using a variety of sensor motes, including the HNODE and WISMOTE small. Several grids are used to divide up the entire forest area. Along with its eight nearby neighbors, each grid forms a zone.\\nThe following are the fundamental presumptions of given model: The main station\\'s location is fixed. There is equal energy in each sensor node, and Grids are used to divide the network. The suggested method is broken down into the following three sub-modules: data forwarding, zone prediction using a semi-supervised classification model, and localization of not known nodes using SVM. Unknown nodes positions are calculated using SVM classification method.\\nThe suggested work takes into account energy constraints during passing of data; which helps in dividing the network into several grids and to locate the suspicious nodes using the SVM approach. This cutting-edge method helps forest sensors interact with one another even when they are not connected to GPS. To determine the grid\\'s status, the suggested approach uses a semi-supervised classification technique. Whereas the Medium Active zone only transmits data infrequently, the High\\nActive zone continuously transmits data to the ground station. The packet is not sent to the main station in the least active zone. As a result, the network uses less energy and lasts longer thanks to the unique technique that has been suggested.\\n      * Ansori, Farhana Mari ,Mukamad Wildan Alaudin,Wayan Firdaus Mahmmudy in Predicting Fire in Forest using Neural Network have chosen Extreme Learning Machines as their proposed model to predict fire. In the area of predictions, neural networks have been successfully used by delivering good perecentage of accuracy in comparison with other algorithms. Extreme Learning Machines is one of the many architectures used by neural networks in their learning processes, was used in this study and produced acceptable results. The ELM approach will be employed in this study to estimate the total amount of land that would be impacted by forest fires based on atmospheric variables, namely wind speed, temperature, moisture and wetness. This method has the greatest accuracy when compared to other methods used in prior studies. By doing this, forest fire catastrophes will be reduced.\\n\\n\\nOnly a single secret layer is used in the ELM\\'s operating system, and its parameters are generated arbitrary. It can give a higher consistency of accuracy in a comparatively shorter period of time than traditional feed forward networks. The training and testing phases of the learning process are separated into two in the ELM technique. The performance and accuracy of the regression model are evaluated using MSE.\\n      * Sudhakar Tripathi, Rina Kumari and Ditipriya Sinha in Semi supervised Clustering Approach based on classification in Wireless Sensor Network for Forst Fire Detecton in 2019. In this research, a unique method is presented that quickly transfers all the detected data to the ground station through non wire communication after identifying the high activity zone in the forest. In this study, a semi- supervised rule-based model is recommended to deciding if a forest zone belongs to a cluster that is highly, moderately, or weakly active. The work is divided into three groups.\\n\\n\\nThe model predicts the area status with 96% predictability when only single elements is shared by the detectors in those area. The proposed model extends the network\\'s lifetime. In contrast, only the sensor nodes in High and Medium Active zones sends data to the ground station. Base station receives fire information from the cluster head in an emergency. Therefore, continual transmission of data is required in the HA zone.\\n\\n\\nIn 2018, Simon Jones, Dieu Tien Bui , Mahyat Shafapour Tehrany, Francisco Martnez-Llvarez, and Farzin Shabani, developed a unique modelling method employing the Logit-Boost machine\\nlearning classifier and multi-source geospatial data for this geographic prediction of tropical potential for areas that caught fire in the forest .\\nThis paper evaluates the required of Logit-Boost ensemble-based decision trees (LEDT) for forest fire potential mapping in the Lao Cai region of Vietnam. The authors conducted a review to confirm that the method they proposed had not been previously applied to forest fires. They compared their proposed artificial machine learning approach, which integrates LEDT, with three benchmark models in terms of performance on both training and validation datasets. The Logit-Boost ensemble- based decision trees methodology showed the good performance, achieving 92% prediction capability.\\n\\n\\nThe research aimed to develop a new tool for forest fire potential mapping, using research of Lao Cai Province in the country of Vietnam at the Northwestern region. The authors used a GIS database with 256 fire areas and ten and more forest conditioning factors to train and validate their model. The Logit-Boost ensemble-based decision trees was formulated as a findings recognition model that predicts the likelihood of forest fire occurrence in the area. This research introduced an innovative and proper tool for forest fire potential plotting that has not been explored in the current literature.\\n      * Kajol R Singh, K.P. Neethu, K Madhurekaa, A Harita, and Pushpa Mohan authors of Parallel Support Vector Machine model for forest fire prediction in 2021. A parallel machine (SVM) model has been developed effectively train weather data and predict fires of the forest. By leveraging parallel processing, this model reduces the technical time and data required for analysis. In this model incorporates the FWI and other weather parameters to predict forest fires. Its efficiency has been evaluated using data from different countries. The model achieved an RMSE of 63.45 for the Portugal data collection, while the SVM method had an RMSE of 63.5.\\nWhen datasets are too large, linear SVM may have lower accuracy and become less reliable. In contrast, Parallel SVM can filter the simplest support vectors and generate a more supportable process for predicting forest fires. Therefore, warnings created are decided on the Parallel Support Vector Machine method.\\nLinear SVM computes the linear relationship between features in the source and output, requiring less computational power. In contrast, Parallel SVM performs a non-linear relationship analysis based on the RBF and has a higher computational cost compared to linear SVM. As the size of the source increases, the number of support vectors also increases, resulting in longer computational times. However, changing the algorithm and utilizing parallel computing can improve\\nperformance and reduce calculation time.\\n\\n\\nRaj Vikram and Ditipriya Sinha authors of Fog-Fire: fog assisted IoT enabled forest fire management in 2021. The given forest fire detection model consists of three layers. Forest zones are divided into different zones. According to prediction analysis, the proposed Fog-Fire classification model achieves a prediction accuracy of 94%, which is higher than other methods and approaches.\\n      * Minggang Peng and Ying Xie are authors of Forest fire forecasting using the ensemble learning approaches in 2017. This study employed the forest fire dataset from the California University, repository named as the Irvine machine learning, obtained from the north-eastern of Portugal. It was used to predict burned areas and large-scale forest fires.\\n\\n\\nThe patterns demonstrated that ensemble learning techniques has enormous potential for use in more autonomous systems for forest fire prevention and detection. Additionally, the EGB\\'s overall prediction accuracy reached 72.3%, and the DT, SVM, EGB, and DL models all had correct prediction rates higher than 70%. Regarding the EGB3 model\\'s ability to accurately predict large- scale fires, it performed better than the other models. These findings indicate that the EGB technique has a significant deal of potential for predicting the occurrence of big-scale forest fires in other parts of the earth.\\n\\n\\nIn this method, it also generated in the weighted versions of the data. They can provide significant techniques for forest fire-fighting decision-making. These techniques can ultimately improve forest fire management efficiency worldwide.\\n\\n\\n      * Rajesh Kumar Singh, Rakesh Arya, Firoz Ahmad, and Abdul Qayum, are the authors of predictive modeling of the forest fire using geospatal tools and the strategic allocation of data: eForest-Fire in 2020. This research aimed to develop a Geographic Information system it is an integrated mapping system that uses direct and indirect factors to predict settlements and villages at high risk of forest fires. By integrating socio-economic, geographic, and climatic factors with differential weightage, the system identified 560 out of 5258 settlements at high fire risk.\\nThe system was improved by linking and downloading the application into the mobile and web portal, which allowed citizens to send a message to the fire incidents in circumstances of the areas which caught fire and directly communicate with authorities. The study demonstrated the potential of spatial technology in environmental monitoring and resource allocation for forest fire management, and promoted people\\'s participation in fire mitigation efforts.\\nThe e_Forest-Fire app served as an early warning system, promoting e-governance and\\nenhancing the existing machinery of the Forests Department. Technology was demonstrated in the study to be effective in addressing environmental, forest, and wildlife-related issues, as well as in bringing people closer to government functionaries as a result of it.\\n      * Ditipriya Sinha and Raj Vikram are the authors of A multimodal framework for Forest fire detection and also monitoring in 2022. The framework is designed to help identify fire-prone areas in a forest and take actions to pretend the spread of forest fires. It is based on a combination of 2 types of detectors are deployed in different zones in the fires forest, which sense the temperature, relative humidity, and drought conditions.\\nThe method is a unique and innovative approach to protecting forests from fire. It integrates a Neuro-fuzzy classification-based on Sensor model and a CNN-based Image model to provide a more accurate prediction of the fire status in each zone. The performance analysis of the model has shown that its accuracy is 83%, which will be higher compared to single Sensor.\\nThe given framework has significant potential for use in forest fire prevention and mitigation efforts. By accurately predicting the fire status of each zone, the ground station can take precautions and also actions to prevent the spread of forest fires, such as deploying firefighting resources or evacuating nearby settlements. The framework also has the potential to be applied to larger forest fire datasets in the future, allowing for more widespread use in forest management and protection.\\n\\n\\nOverall, the given model of the Multimodal Forest fire detection method is a valuable tool for protecting forests from fire and can help to lessen the harm that forest fires cause. Its integration of different sensor types and models makes it a unique and effective approach to forest fire detection and prevention.\\nCHAPTER3\\nSYSTEM ARCHITECTURE AND MODULES\\n   1. Architecture Diagram\\n\\n\\n\\n\\n arch.drawio \\n\\nFig 3.1 Architectural Design of Forest Fire Prediction\\n\\n\\n\\n\\nIn the figure 3.1, it shows the architectural design of the Clustered forest fire prediction using neural network with effective feature scaling and threshold based feature selection, the dataset having necessary features should be picked up. Then for that particular dataset pre-processing would be done to remove noise and outliers from that particular dataset. In our case, clustering would be done first to group those data present in the dataset. After that Feature scaling would be done so as to make the data uniform in the range of their values. The following feature scaling techniques are available: minimum to maximum, maximum to absolute, robust, quantile transformer, and power transformer. From that the best scaling method is used for further pre-processing of data. Finally Feature selection would be done to remove the unnecessary features which are of less or zero significance for our purpose. Methods of feature selection include Extra Tree Classifier, Forward Selection, Chi Square and Pearson correlation. The best method from these feature selection methods is considered. Finally, we would be left out with necessary and useful uniform scaled features. After pre-processing, the model would be trained on algorithms such as Logistic Regression, SVM & KNN and how it’s performing against each algorithm would take into account. After model training, using neural network forest fire prediction would happen and finally the quantitative and qualitative analysis would be done from those prediction.\\n   2. Data Flow Diagram\\n\\n\\n  \\n\\nFig-3.2 - Data Flow Diagram\\n\\n\\nIn the figure 3.2, the data flow diagram for the clustered forest fire prediction system using a neural network with effective feature scaling and threshold-based feature selection illustrates the flow of data from the input dataset through the preprocessing stages to the trained neural network model, emphasizing the significance of feature selection and scaling.\\nInput Data Sources: In this phase ,data from various  such as weather stations and ground sensors are collected and integrated dataset through the preprocessing stages to the trained neural network model, emphasizing the significance of feature selection and scaling.\\n\\n\\n      K-Means Clustering: It is a machine-learning approach that uses unsupervised learning to cluster \\nFeature Scaling: It is a initial step in this machine learning used to standardization or normalize the range of values of various elements or features in a dataset.\\nFeature Selection: It is used to make the module more accurate. It increases the prediction and accuracy power of the algorithms. It selects the most important variables and eliminates the irreleones.\\nNeural Network: It is one of the deep learning based on the machine learning models based on the human brain\\'s structure and functioning. It consists of interconnecting layers, or circuits, that process and transform input data to produce output predictions. The neural networks showed high accuracy for a wide range of applications.\\n   3. Modules\\n\\n\\n      1. K-Means Clustering\\n\\n\\nThis method uses unsupervised machine learning to classify unlabelled data into groups based on their similarity. It is a simple and effective algorithm widely used in various fields such as computer vision, pattern recognition, and marketing.\\n\\n\\nElbow Method\\n\\n\\nThe elbow method is a method for discovering the finest number of clusters K in K-means as clusters. The basic plan is to determine the SSD between each data point and its given cluster centroid, as a function of the count of clusters K.\\nSilhouette score\\nIt is a metric used to evaluate the fine of clustering quality. And the value of 1 means that the data point is very close to its own group and far from other groups, while a value of -1 means that the data point is closer to other groups than to itself.\\n      2. Feature Scaling\\nFeature scaling is a preprocessing step in deep learning used to standardize or normalization the scale of values of various features in a dataset. It is crucial because many machine learning algorithms depend on the input variables scales to be consistent for best results. For feature scaling, normalization and standardization are the two most popular techniques.\\nStandardisation:\\nTo standardise, remove the feature\\'s mean from each value and divide the result by the feature\\'s standard deviation.\\nNormalization:\\nThe minimum value of the feature is subtracted from each value before being divided by the feature\\'s range to achieve normalisation.\\n         1. Min-Max Scaling\\n         2. Max-Absolute scaling\\n         3. Robust Scaling\\n         4. Quantile Transformer Scaling\\n         5. Power transformer Scaling\\n            * Box-Cox transformation\\n            * Yeo-Johnson transformation\\nRoot Mean Squared Error (RMSE):\\nIn training, cross-validation, and monitoring, a single number measures how well a model performs. It calculates the transformation in the middle of values predicted by a module and actual values.\\n      3. Feature Selection\\nThe variables in the dataset that cannot be used to construct a deep learning model are either redundant or irrelevant. If all of these redundant and meaningless pieces of information are included in the dataset, the general efficacy and precision of the model may deteriorate. Therefore, it is essential to identify and select the most suitable features from the data, as well as to remove any extraneous or unimportant features. Deep learning uses feature selection to do this. The process of selecting features for a model is called feature selection. A feature is a quality that has an impact on or contributes to the resolution of a problem. Feature engineering, which essentially consists of the two processes of feature extraction and selection, is necessary for each phase of the deep learning process. Although feature selection and extraction methods may have the same end in mind, they are very different from one another. The main difference between the two is that feature extraction adds new features whereas feature selection just chooses a portion of the initial feature set. By using only pertinent data, feature selection can limit the input variable for the model and lessen overfitting. It helps to increase the procedure\\' accuracy. It improves the algorithms\\' capacity for prediction. It chooses the most important variables and gets rid of the unnecessary or unimportant ones. Using this has the advantages of reducing overfitting, increasing accuracy, and cutting training time. The numerous strategies that have been employed in this specific project are:\\n* Extra Tree Classifier\\n* Forward Selection\\n* Chi Square\\n* Personal Correlation\\n\\n\\n      4. Neural Network:\\nIt is a powerful tool in deep learning that can be used for predicting forest fires using numerical datasets. It is one of the best machine-learning models inspired by the structure and the function of the human brain. In this system, information is processed and transmitted through multiple interconnected layers of artificial neurons.\\nArtificial Neural Network\\nThis are a class of neural networks that may be used for regression tasks and classifications, both of which are pertinent to forest fire prediction.\\nThe Sequential Method\\nThe Sequential method is used to build a linear stack of layers in an ANN. This means that each layer in the model is connected to the previous layer, and the data flows through the layers sequentially. The Sequential method is simple to use, making it a popular choice for building ANNs. Dense Layers\\nDense layers include those which are fully connected, meaning that every node in one layer is linked to every other node in the layer above it. This are employed to execute data transformations and to bring nonlinearity into the model.\\nCHAPTER 4 METHODOLOGY\\n   1. Data Preprocessing\\n\\n\\nData pre-processing is a set of techniques and processes applied to raw data with the aim of preparing it for analysis by a neural network model for predicting clustered forest fires. This includes cleaning and integrating the data, creating updated features through feature engineering, and scaling the data to ensure each feature is on the same scale. Data pre-processing ensures that the data is consistent, accurate, and prepared for neural network model analysis.\\n\\n\\n      1. K-Means Clustering\\n\\n\\nIt is a powerful unsupervised machine learning algorithm that is commonly used to cluster similar data points based on their characteristics. It is one of the most important and simple algorithms that has been used in a variety of fields including finding recognition, computer vision, and marketing. It can be used to identify alike forest fires based on weather and topography data in the context of Clustered Forest Fire Prediction Using Neural Network with Effective Feature Scaling and Threshold Based Feature Selection. The resulting clusters can be used to predict a neural network to predict the likelihood of forest fires occurring in each cluster by performing appropriate data pre-processing steps like feature scaling and selecting the finest number of clusters.\\n\\n\\nHere are how the algorithm functions:\\nInitialization: Select the number of clusters, k, and randomly initialize k points called centroids in the data set.\\nAssigning Data Points to Clusters: For each data point, calculate the distance to each centroid and assign the data point to the nearest centroid c using the equ. (4.1).\\n\\n\\n\\n\\n(4.1) {\"mathml\":\"<math style=\\\\\"font-family:stix;font-size:16px;\\\\\" xmlns=\\\\\"http://www.w3.org/1998/Math/MathML\\\\\"><mstyle mathsize=\\\\\"16px\\\\\"><mi>d</mi><mi>i</mi><mi>s</mi><mi>t</mi><mfenced><mrow><mi>x</mi><mo>,</mo><mi>c</mi></mrow></mfenced><mo>&#xA0;</mo><mo>=</mo><mo>&#xA0;</mo><msqrt><mfenced><mrow><msup><mfenced><mrow><msub><mi>x</mi><mn>1</mn></msub><mo>&#xA0;</mo><mo>-</mo><mo>&#xA0;</mo><msub><mi>c</mi><mn>1</mn></msub></mrow></mfenced><mn>2</mn></msup><mo>&#xA0;</mo><mo>+</mo><mo>&#xA0;</mo><msup><mfenced><mrow><msub><mi>x</mi><mn>2</mn></msub><mo>&#xA0;</mo><mo>-</mo><mo>&#xA0;</mo><msub><mi>c</mi><mn>2</mn></msub></mrow></mfenced><mn>2</mn></msup></mrow></mfenced></msqrt></mstyle></math>\"}  d i s t open parentheses x comma c close parentheses space equals space square root of open parentheses open parentheses x subscript 1 space minus space c subscript 1 close parentheses squared space plus space open parentheses x subscript 2 space minus space c subscript 2 close parentheses squared close parentheses end root \\n\\n\\nwhere xi and ci stand for the values of the ith feature of x and c, respectively.\\nMoving Centroids: Calculate the centre of the data points assigned to each centroid, and move the centroid to the intermediate position using the equ (4.2).\\n\\n\\n(4.2) {\"mathml\":\"<math style=\\\\\"font-family:stix;font-size:16px;\\\\\" xmlns=\\\\\"http://www.w3.org/1998/Math/MathML\\\\\"><mstyle mathsize=\\\\\"16px\\\\\"><mi>m</mi><mi>e</mi><mi>a</mi><mi>n</mi><mfenced><mrow><mi>c</mi><mi>l</mi><mi>u</mi><mi>s</mi><mi>t</mi><mi>e</mi><mi>r</mi></mrow></mfenced><mo>&#xA0;</mo><mo>=</mo><mo>&#xA0;</mo><mfenced><mfrac><mn>1</mn><mi>N</mi></mfrac></mfenced><mo>&#xA0;</mo><mo>&#xD7;</mo><mo>&#xA0;</mo><mstyle displaystyle=\\\\\"false\\\\\"><munder><mo>&#x2211;</mo><mrow/></munder></mstyle><mfenced><mrow><msub><mi>x</mi><mn>1</mn></msub><mo>&#xA0;</mo><mo>,</mo><mo>&#xA0;</mo><msub><mi>x</mi><mn>2</mn></msub><mo>&#xA0;</mo><mo>,</mo><mo>&#xA0;</mo><mo>.</mo><mo>.</mo><mo>.</mo><mo>.</mo><mo>,</mo><msub><mi>x</mi><mi>n</mi></msub></mrow></mfenced></mstyle></math>\"}  m e a n open parentheses c l u s t e r close parentheses space equals space open parentheses 1 over N close parentheses space cross times space sum for blank of open parentheses x subscript 1 space comma space x subscript 2 space comma space.... comma x subscript n close parentheses \\n\\n\\nxi stands for the value of the ith feature of each data point in the cluster, and N denotes the total number of data points in the cluster.\\nOutput: The final positions of the centroids represent the k clusters, and each data point is assigned to the nearest centroid.\\nIt, also known as the WCSS, seeks to minimize the sum of the squared distances between the data sets and their specified centres using the equ. (4.3).\\n\\n\\n(4.3) {\"mathml\":\"<math style=\\\\\"font-family:stix;font-size:16px;\\\\\" xmlns=\\\\\"http://www.w3.org/1998/Math/MathML\\\\\"><mstyle mathsize=\\\\\"16px\\\\\"><mi>W</mi><mi>C</mi><mi>S</mi><mi>S</mi><mo>&#xA0;</mo><mo>=</mo><mo>&#xA0;</mo><munder><mo>&#x2211;</mo><mrow/></munder><mo>&#xA0;</mo><msub><mi>P</mi><mrow><mi>i</mi><mo>&#xA0;</mo><mi>i</mi><mi>n</mi><mo>&#xA0;</mo><mi>c</mi><mi>l</mi><mi>u</mi><mi>s</mi><mi>t</mi><mi>e</mi><mi>r</mi><mo>&#xA0;</mo><mn>1</mn></mrow></msub><mo>&#xA0;</mo><mi>d</mi><mi>i</mi><mi>s</mi><mi>t</mi><msup><mfenced><mrow><msub><mi>P</mi><mi>i</mi></msub><mo>,</mo><msub><mi>C</mi><mn>1</mn></msub></mrow></mfenced><mn>2</mn></msup><mo>&#xA0;</mo><mo>+</mo><mo>&#xA0;</mo><mstyle displaystyle=\\\\\"false\\\\\"><munder><mrow><mo>&#x2211;</mo><mo>&#xA0;</mo><msub><mi>P</mi><mrow><mi>i</mi><mo>&#xA0;</mo><mi>i</mi><mi>n</mi><mo>&#xA0;</mo><mi>c</mi><mi>l</mi><mi>u</mi><mi>s</mi><mi>t</mi><mi>e</mi><mi>r</mi><mo>&#xA0;</mo><mn>2</mn><mo>&#xA0;</mo></mrow></msub><mo>&#xA0;</mo><mi>d</mi><mi>i</mi><mi>s</mi><mi>t</mi><msup><mfenced><mrow><msub><mi>P</mi><mi>i</mi></msub><mo>,</mo><mo>&#xA0;</mo><msub><mi>C</mi><mn>2</mn></msub></mrow></mfenced><mn>2</mn></msup><mo>&#xA0;</mo></mrow><mrow/></munder></mstyle></mstyle></math>\"}  W C S S space equals space sum for blank of space P subscript i space i n space c l u s t e r space 1 end subscript space d i s t open parentheses P subscript i comma C subscript 1 close parentheses squared space plus space stack sum space P subscript i space i n space c l u s t e r space 2 space end subscript space d i s t open parentheses P subscript i comma space C subscript 2 close parentheses squared space with blank below \\n\\n\\n {\"mathml\":\"<math style=\\\\\"font-family:stix;font-size:16px;\\\\\" xmlns=\\\\\"http://www.w3.org/1998/Math/MathML\\\\\"><mstyle mathsize=\\\\\"16px\\\\\"><munder><mrow><mo>&#x2211;</mo><msub><mi>P</mi><mrow><mi>i</mi><mo>&#xA0;</mo><mi>i</mi><mi>n</mi><mo>&#xA0;</mo><mi>c</mi><mi>l</mi><mi>u</mi><mi>s</mi><mi>t</mi><mi>e</mi><mi>r</mi><mo>&#xA0;</mo><mn>1</mn></mrow></msub><mo>&#xA0;</mo><mi>d</mi><mi>i</mi><mi>s</mi><mi>t</mi><msup><mfenced><mrow><msub><mi>P</mi><mi>i</mi></msub><mo>,</mo><mo>&#xA0;</mo><msub><mi>C</mi><mn>1</mn></msub></mrow></mfenced><mn>2</mn></msup></mrow><mrow/></munder></mstyle></math>\"}  stack sum P subscript i space i n space c l u s t e r space 1 end subscript space d i s t open parentheses P subscript i comma space C subscript 1 close parentheses squared with blank below \\n : It is the sum of the squares of the distances between each data point and its centroid within a cluster1, and the same is true for the other terms.\\nElbow Method:\\nIt is a method for determining the finest number of clusters K in K-means as clusters. The basic idea is to determine the sum of the squared distances (SSD) in the middle of the each data point and its given node centroid, as a method of the number of clusters K.\\nThe algorithm for the elbow method is as follows:\\n* Choose different types of K values to check (we used 1 to 20).\\n* For each value of K, perform K-means [23] clustering on the dataset. Calculate the SSD between each data point and its assigned centroid.\\n* Plot the SSD against the number of clusters K.\\n* Look for the \"elbow\" point on the curve - this is the point where the SSD starts to drop, indicating that additional clusters do not significantly improve cluster performance.\\n* Select K as the number of clusters that the dataset requires, as this is the correct number.\\nSilhouette score:\\nIt is a metric used to evaluate the classification results. A value of 1 means that the data point is very near to its own group and far from other groups, while a value of -1 means that the data point is closer to other groups than to itself calculating using the equ. (4.4). It is an effective tool for evaluating the quality of team results and can help identify areas for improvement. {\"mathml\":\"<math style=\\\\\"font-family:stix;font-size:16px;\\\\\" xmlns=\\\\\"http://www.w3.org/1998/Math/MathML\\\\\"><mstyle mathsize=\\\\\"16px\\\\\"><mi>S</mi><mi>c</mi><mi>o</mi><mi>r</mi><mi>e</mi><mo>&#xA0;</mo><mo>=</mo><mo>&#xA0;</mo><mfrac><mfenced><mrow><mi>b</mi><mo>-</mo><mi>a</mi></mrow></mfenced><mrow><mi>m</mi><mi>a</mi><mi>x</mi><mfenced><mrow><mi>a</mi><mo>,</mo><mi>b</mi></mrow></mfenced></mrow></mfrac></mstyle></math>\"}  S c o r e space equals space fraction numerator open parentheses b minus a close parentheses over denominator m a x open parentheses a comma b close parentheses end fraction \\n(4.4)\\na is the Mean distance between each point within a cluster,\\n\\n\\nb is the mean of the inter-cluster distance\\n\\n\\n      2. Feature Scaling:\\n\\n\\nFeature scaling plays a significant role in data pre-processing for Clustered Forest Fire Prediction using Neural Network with Effective Feature Scaling on Threshold Based Feature Selection. By transforming forest fire features to the same scale, feature scaling improves the performance of the neural network.\\n\\n\\nThere are various methods of feature scaling that can be used for this task, including normalization and standardization. In this context, it is also critical to ensure that the scaling method is applied consistently across all the features. By doing so, biases can be avoided in neural network training.\\n\\n\\nOverall, effective feature scaling and selection can greatly improve the neural network performance for Clustered Forest Fire Prediction.\\n\\n\\nStandardization\\nIt is done by subtracting the feature mean from each value and dividing it by the standard deviation. This scale\\'s feature has a mean of 0 and a standard deviation of 1. {\"mathml\":\"<math style=\\\\\"font-family:stix;font-size:16px;\\\\\" xmlns=\\\\\"http://www.w3.org/1998/Math/MathML\\\\\"><mstyle mathsize=\\\\\"16px\\\\\"><mover><mi>x</mi><mo>~</mo></mover><mo>&#xA0;</mo><mo>&#xA0;</mo><mo>=</mo><mo>&#xA0;</mo><mfrac><mfenced><mrow><mi>x</mi><mo>&#xA0;</mo><mo>-</mo><mo>&#xA0;</mo><mi>m</mi><mi>e</mi><mi>a</mi><mi>n</mi><mfenced><mi>x</mi></mfenced></mrow></mfenced><msqrt><mi>v</mi><mi>a</mi><mi>r</mi><mfenced><mi>x</mi></mfenced></msqrt></mfrac></mstyle></math>\"}  x with tilde on top space space equals space fraction numerator open parentheses x space minus space m e a n open parentheses x close parentheses close parentheses over denominator square root of v a r open parentheses x close parentheses end root end fraction \\n\\n\\n\\n\\n(4.5)\\n\\n\\nNormalization\\nNormalization is done by subtracting the least value of the feature from each value and then dividing by the feature scale. It is useful when feature distribution is not known or when there are outliers in the data. A feature\\'s values are scaled between 0 and 1.\\n1. Min-Max Scaling\\n\\n\\nIt is the method subtracts the least value of the feature and divides it by the difference between the highest and least values. In this method scales the data to a fixed scale, usually between 0 and 1.\\n\\n\\n\\n\\n(4.6) {\"mathml\":\"<math style=\\\\\"font-family:stix;font-size:16px;\\\\\" xmlns=\\\\\"http://www.w3.org/1998/Math/MathML\\\\\"><mstyle mathsize=\\\\\"16px\\\\\"><mover><mi>x</mi><mo>~</mo></mover><mo>&#xA0;</mo><mo>&#xA0;</mo><mo>=</mo><mo>&#xA0;</mo><mfrac><mfenced><mrow><mi>x</mi><mo>&#xA0;</mo><mo>-</mo><mo>&#xA0;</mo><mi>m</mi><mi>i</mi><mi>n</mi><mfenced><mi>x</mi></mfenced></mrow></mfenced><mrow><mi>m</mi><mi>a</mi><mi>x</mi><mfenced><mi>x</mi></mfenced><mo>&#xA0;</mo><mo>-</mo><mo>&#xA0;</mo><mi>m</mi><mi>i</mi><mi>n</mi><mfenced><mi>x</mi></mfenced></mrow></mfrac></mstyle></math>\"}  x with tilde on top space space equals space fraction numerator open parentheses x space minus space m i n open parentheses x close parentheses close parentheses over denominator m a x open parentheses x close parentheses space minus space m i n open parentheses x close parentheses end fraction \\n2. Max-Absolute scaling\\n\\n\\nMax-Absolute Scaling method works by dividing each feature value by the maximum absolute value of that feature using the equ.(4.7). This ensures that the absolute value of each feature is no greater than 1, and that the relative values of the features are preserved. In this method ranges the data to a fixed scale, usually between -1 and 1. This method uses the maximum and hence it is also sensitive to outliers like MinMaxScaler.\\n\\n\\n(4.7) {\"mathml\":\"<math style=\\\\\"font-family:stix;font-size:16px;\\\\\" xmlns=\\\\\"http://www.w3.org/1998/Math/MathML\\\\\"><mstyle mathsize=\\\\\"16px\\\\\"><msup><msub><mi>X</mi><mi>i</mi></msub><mo>\\'</mo></msup><mo>=</mo><mo>&#xA0;</mo><mfrac><msub><mi>X</mi><mi>i</mi></msub><mrow><mi>a</mi><mi>b</mi><mi>s</mi><mfenced><msub><mi>X</mi><mrow><mi>m</mi><mi>a</mi><mi>x</mi></mrow></msub></mfenced></mrow></mfrac></mstyle></math>\"}  X subscript i to the power of apostrophe equals space fraction numerator X subscript i over denominator a b s open parentheses X subscript m a x end subscript close parentheses end fraction \\n\\n\\n3. Robust Scaling\\n\\n\\nRobust scaling is designed to scale the data while minimizing the effect of outliers. It works by subtracting the median of each feature from each data point, and then dividing by the interquartile range (IQR) of the feature. The IQR is the subtracting the 75th percentile and the 25th percentile of the data using the equ.(4.8). So that it is less affected by outliers.\\n\\n\\n(4.8) {\"mathml\":\"<math style=\\\\\"font-family:stix;font-size:16px;\\\\\" xmlns=\\\\\"http://www.w3.org/1998/Math/MathML\\\\\"><mstyle mathsize=\\\\\"16px\\\\\"><msup><msub><mi>X</mi><mi>i</mi></msub><mo>\\'</mo></msup><mo>&#xA0;</mo><mo>&#xA0;</mo><mo>=</mo><mo>&#xA0;</mo><mfrac><mrow><mfenced><mrow><msub><mi>x</mi><mi>i</mi></msub><mo>&#xA0;</mo><mo>-</mo><mo>&#xA0;</mo><msub><mi>x</mi><mrow><mi>m</mi><mi>e</mi><mi>d</mi></mrow></msub></mrow></mfenced><mo>&#xA0;</mo></mrow><mrow><mi>I</mi><mi>Q</mi><mi>R</mi></mrow></mfrac><mo>&#xA0;</mo><mo>=</mo><mo>&#xA0;</mo><mfrac><mfenced><mrow><msub><mi>x</mi><mi>i</mi></msub><mo>&#xA0;</mo><mo>-</mo><mo>&#xA0;</mo><msub><mi>x</mi><mrow><mi>m</mi><mi>e</mi><mi>d</mi></mrow></msub></mrow></mfenced><mrow><msub><mi>x</mi><mrow><mn>75</mn><mo>&#xA0;</mo></mrow></msub><mo>&#xA0;</mo><mo>-</mo><mo>&#xA0;</mo><msub><mi>x</mi><mn>25</mn></msub><mo>&#xA0;</mo></mrow></mfrac></mstyle></math>\"}  X subscript i to the power of apostrophe space space equals space fraction numerator open parentheses x subscript i space minus space x subscript m e d end subscript close parentheses space over denominator I Q R end fraction space equals space fraction numerator open parentheses x subscript i space minus space x subscript m e d end subscript close parentheses over denominator x subscript 75 space end subscript space minus space x subscript 25 space end fraction \\n\\n\\n\\n\\n4. Quantile Transformer Scaling\\n\\n\\nQuantile Transformer Scaling is designed to transform the data to follow a uniform or normal distribution. It is an inverse function of the cumulative Distribution Function (CDF). It is used to remove outliers or fit the Normal Distribution. This is defined on the unit interval (0, 1)\\n5. Power transformer Scaling\\n\\n\\nPower transformer Scaling is designed to transform the data to follow a normal or Gaussian distribution. This method works by applying a power transformation to the data that can adjust the skewness of the data and make it more symmetric. The power function is defined on the unit interval (0, 1). In this scaling method uses two types of power transformations: the the Yeo-Johnson transformation and the Box-Cox transformation.\\nBox-Cox transformation:\\n\\n\\n(4.9) {\"mathml\":\"<math style=\\\\\"font-family:stix;font-size:16px;\\\\\" xmlns=\\\\\"http://www.w3.org/1998/Math/MathML\\\\\"><mstyle mathsize=\\\\\"16px\\\\\"><mi>y</mi><mo>&#xA0;</mo><mo>=</mo><mo>&#xA0;</mo><mfrac><mfenced><mrow><msup><mi>x</mi><mi>&#x3BB;</mi></msup><mo>&#xA0;</mo><mo>-</mo><mo>&#xA0;</mo><mn>1</mn></mrow></mfenced><mi>&#x3BB;</mi></mfrac></mstyle></math>\"}  y space equals space fraction numerator open parentheses x to the power of lambda space minus space 1 close parentheses over denominator lambda end fraction \\nwhere lambda is the power parameter.\\nThe power parameter is estimated using maximum likelihood estimation, which searches for the optimal value of lambda that makes the data as close to normal distribution as possible using equ.(4.9).\\n\\n\\nYeo-Johnson transformation:\\n\\n\\n(4.10) {\"mathml\":\"<math style=\\\\\"font-family:stix;font-size:16px;\\\\\" xmlns=\\\\\"http://www.w3.org/1998/Math/MathML\\\\\"><mstyle mathsize=\\\\\"16px\\\\\"><mi>y</mi><mo>&#xA0;</mo><mo>=</mo><mo>&#xA0;</mo><mfrac><mfenced open=\\\\\"[\\\\\" close=\\\\\"]\\\\\"><mrow><msup><mfenced><mrow><mi>x</mi><mo>+</mo><mn>1</mn></mrow></mfenced><mi>&#x3BB;</mi></msup><mo>&#xA0;</mo><mo>-</mo><mn>1</mn></mrow></mfenced><mi>&#x3BB;</mi></mfrac></mstyle></math>\"}  y space equals space fraction numerator open square brackets open parentheses x plus 1 close parentheses to the power of lambda space minus 1 close square brackets over denominator lambda end fraction \\n\\n\\nfor x >= 0\\n\\n\\n(4.11) {\"mathml\":\"<math style=\\\\\"font-family:stix;font-size:16px;\\\\\" xmlns=\\\\\"http://www.w3.org/1998/Math/MathML\\\\\"><mstyle mathsize=\\\\\"16px\\\\\"><mi>y</mi><mo>&#xA0;</mo><mo>=</mo><mo>&#xA0;</mo><mo>-</mo><mo>&#xA0;</mo><mfrac><mfenced open=\\\\\"[\\\\\" close=\\\\\"]\\\\\"><mrow><msup><mfenced><mrow><mo>|</mo><mi>x</mi><mo>|</mo><mo>&#xA0;</mo><mo>+</mo><mo>&#xA0;</mo><mn>1</mn></mrow></mfenced><mi>&#x3BB;</mi></msup><mo>&#xA0;</mo><mo>-</mo><mo>&#xA0;</mo><mn>1</mn></mrow></mfenced><mi>&#x3BB;</mi></mfrac></mstyle></math>\"}  y space equals space minus space fraction numerator open square brackets open parentheses vertical line x vertical line space plus space 1 close parentheses to the power of lambda space minus space 1 close square brackets over denominator lambda end fraction \\n\\n\\nfor x < 0\\nabs(x) is the absolute value of x\\n\\n\\nRoot Mean Squared Error (RMSE)\\n\\n\\nIt can be used as an evaluation metric for evaluating the accuracy of a Clustered Forest Fire Prediction using Neural Network with effective Feature Scaling and Threshold Based Feature Selection. To use RMSE, divide the dataset into training and testing datasets. Perform feature scaling on the training dataset to ensure that all features are on the same scale. In addition, threshold-based feature selection should be utilized to find the most important and suitable features for predicting forest fires. During training, the RMSE should be used as an evaluation metric to monitor the model\\'s performance using the equ. (4.12). After training, the model\\'s performance on the testing dataset can be evaluated using the RMSE value, with a lower RMSE value indicating better performance. You can use RMSE to ensure that your Clustered Forest Fire Prediction model predicts forest fires as accurately as possible. {\"mathml\":\"<math style=\\\\\"font-family:stix;font-size:16px;\\\\\" xmlns=\\\\\"http://www.w3.org/1998/Math/MathML\\\\\"><mstyle mathsize=\\\\\"16px\\\\\"><mi>R</mi><mi>M</mi><mi>S</mi><mi>E</mi><mo>&#xA0;</mo><mo>=</mo><mo>&#xA0;</mo><msqrt><mfrac><mfenced><mrow><munderover><mo>&#x2211;</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi>N</mi></munderover><mo>&#xA0;</mo><mo>|</mo><mo>|</mo><mo>&#xA0;</mo><mi>y</mi><mfenced><mi>i</mi></mfenced><mo>&#xA0;</mo><mo>-</mo><mo>&#xA0;</mo><mstyle displaystyle=\\\\\"true\\\\\"><mover><mi>y</mi><mo>&#x2227;</mo></mover></mstyle><mo>&#xA0;</mo><mfenced><mi>i</mi></mfenced><mo>|</mo><msup><mo>|</mo><mn>2</mn></msup></mrow></mfenced><mi>N</mi></mfrac></msqrt></mstyle></math>\"}  R M S E space equals space square root of fraction numerator open parentheses sum from i equals 1 to N of space vertical line vertical line space y open parentheses i close parentheses space minus space y with logical and on top space open parentheses i close parentheses vertical line vertical line squared close parentheses over denominator N end fraction end root \\n\\n\\n\\n\\n(4.12)\\n\\n\\n\\n\\n* where N is the data points count,\\n* y(i) is the i-th measurement, and\\n* y ̂(i) is its corresponding prediction.\\n      1. Data Exploration:\\n\\n\\nData exploration is an critical step in any deep learning project, however Clustered Forest Fire Prediction Using Neural Network with Effective Feature Scaling and Threshold Based Feature Selection. To begin, it is critical to understand the data source and collection methods in order to identify any potential biases or limitations in the dataset. Once the data source has been identified, the various features available in the dataset must be identified.\\n\\n\\nIn this case, the target variable is forest fires. Checking for data quality issues, such as values in the dataset are missing or outliers, is also critical, as these can reduce the prediction model\\'s accuracy. Data visualisation techniques such as plots and histograms can be used to identify plots and correlations in data. These patterns and correlations can be used to determine which features are most significant for predicting forest fires and also how they relate to each another. It is a critical factor in increasing the model\\'s accuracy and creating a more effective prediction system.\\n\\n\\nData source and collection\\n\\n\\nUnderstanding the data source and collection methods is critical for identifying any biases or limitations in the dataset. This can be done by reviewing documentation or referring to information sources.\\nData quality\\n\\n\\nCheck for missing values, outliers, and any other issues with data quality that might impact the prediction model\\'s accuracy. These issues can be addressed by imputing missing values, removing outliers, or employing statistical methods.\\nData visualization\\n\\n\\nTo identify patterns and correlations in data by visualizing it with plots, histograms, and other visualization tools. This can help you understand which features are most relevant for predicting forest fires and how they are related to one another.\\nDescriptive statistics\\n\\n\\nCalculate descriptive statistics, such as average, median, and standard deviation, to better understand the distribution of features in the dataset.\\nCorrelation analysis\\n\\n\\nConduct correlation analysis to identify the relationship between features and the target variable.\\n\\n\\n   2. Feature Selection\\n\\n\\nExtra Tree Classifier\\n\\n\\nFor choosing the features, a tree-based supervised model is used. It uses a filter-based approach. This approach builds several tree models at random from the training dataset. Next, rather than dividing the data into locally optimal values using Gini or entropy, it sorts the characteristics that have received the most votes. The choice of a splitting value at random for a feature is the distinguishing feature. It reduces variance and lessens the likelihood of overfitting the model.\\n\\n\\nChi Squared\\n\\n\\nIn statistical models, a chi-square test is employed to examine the dependency of features. The model calculates difference between anticipated and original value. Correlation between variables is stronger when the Chi-square value is higher and lower when the value is higher using the equ.(4.13). The initial assumption that the qualities are independent creates the null hypothesis.\\nChi Square Test:\\nxc2 =\\n________________\\nΣ(Oi − Ei )2\\nEi  \\n________________\\n\\n\\n\\n\\n(4.13)\\n\\n\\nO = Observed Value, E = Expected Value, and c = Degrees of Freedom\\nForward Selection\\nThis model is a wrapper type. It assesses the combined predictive ability of the features. The best-performing features are returned as a set. The processes in this procedure include training n models using n features each separately, then evaluating the results. then decide on the variable that performs the best. Repeat the procedure, introducing each variable separately. The variable that results in the greatest improvement is kept. Continue doing this until the model\\'s performance doesn\\'t noticeably improve.\\nPearson Correlation\\nThe linear relationship between two properties is measured by creating a correlation matrix. How closely related the two traits are to one another is indicated by a value between -1 and 1 using for fire prediction, a robust machine learning pipeline can incorporate Chi-Squared feature selection, clustering, and a Random Forest classifier to enhance model accuracy and interpretability. Initially, relevant environmental and meteorological data—such as temperature, humidity, wind speed, and vegetation indices—are collected and preprocessed. Chi-Squared feature selection is then applied to identify and retain the most statistically significant features related to the target variable, typically a binary indicator of fire occurrence. This step reduces dimensionality and helps the model focus on the most relevant inputs. To uncover underlying spatial or feature-based groupings in the data, clustering techniques like K-Means or DBSCAN can be employed; the resulting cluster labels may serve as an additional feature to enrich the model\\'s contextual understanding. Finally, a Random Forest algorithm is used for prediction, leveraging its ability to handle non-linear relationships and feature interactions. This integrated approach—combining statistical filtering, unsupervised learning, and ensemble methods—can significantly improve the reliability of fire risk prediction models. The Chi-Squared test ensures that only the features with the strongest correlation to fire events are considered, which enhances model performance and reduces overfitting. Clustering can help in recognizing patterns such as high-risk zones or seasonal fire-prone areas, which might not be evident through raw features alone. The inclusion of cluster information adds a spatial or contextual layer to the dataset, making the model more sensitive to regional variations. The Random Forest classifier, known for its robustness and resistance to noise, can handle both numerical and categorical features efficiently. Its ensemble structure, made of multiple decision trees, ensures better generalization on unseen data. Feature importance scores provided by the Random Forest also offer valuable insights into which environmental factors contribute most to fire occurrence. When combined, these techniques support not only accurate prediction but also interpretable and actionable insights for early warning systems. This pipeline can be further improved by incorporating real-time data streams and satellite imagery for continuous learning. Ultimately, such models can aid authorities in resource allocation and timely interventions to mitigate wildfire risks. Moreover, periodic retraining of the model with updated data can help maintain its accuracy in changing environmental conditions. Integrating Geographic Information System (GIS) data can further enhance the spatial resolution of predictions. Additionally, visualization tools can be developed to display fire risk maps based on the model\\'s output, aiding decision-makers and emergency responders. This comprehensive approach not only supports proactive fire management but also contributes to environmental protection and public safety.\\nthe equ.(4.14). It determines the correlation between each feature and the desired outcome. A correlation between the features is shown by a value of 1, -1, or 0, depending on how they are related. Only metric variables should be used for Pearson correlations. It is sometimes indicated as \"correlation\" or the \"product moment correlation coefficient\" (PMCC). The formula for the correlation is:\\n\\n\\n\\n\\nhere,\\n________________\\n𝑟 =        𝑁𝛴𝑥𝑦−(𝛴𝑥)(𝛴𝑦)\\n√[𝑁𝛴𝑥2−(𝛴𝑥)2][𝑁𝛴𝑦2−(𝛴𝑦)2]  \\n________________\\n(4.14)\\nN is the quantity of score pairs.\\nxy is equal to the product of the matched scores. x = total of all x scores\\ny= the total of the y scores\\nΣx2 is the squared sum of the x scores Σy2 is the squared sum of the y scores\\n\\n\\n   3. Deep Neural Network\\n\\n\\nNeural networks are a powerful tool in deep learning that can be used for predicting forest fires using numerical datasets. In the context of forest fire prediction, a neural network can be trained on numerical datasets that include various features such as temperature, relative humidity, wind speed, and moisture content of the vegetation. These features can be used to predict the likelihood and intensity of a forest fire. It consists of different interconnected layers of hybrid neurons that process and transmit information\\nOne of the advantages of using this algorithm for forest fire prediction is their ability to learn and adapt to new information. They can also handle complex nonlinear correlation between the features and the target variable, which can be useful for identifying patterns and trends in the data.\\nOverall, neural networks are a promising approach for predicting forest fires using numerical datasets. They have the potential to improve our ability to prevent and manage wildfires, which can have a important impact on both human life and the environment.\\nArtificial Neural Network\\nIt is a type of deep learning learning algorithm that can be used for regression tasks and classification, which are both applicable to forest fire prediction.\\nIn forest fire prediction using ANNs, historical data on weather and environmental factors such as temperature, relative humidity, wind and speed, and FWI can be used to train the network to predict\\nthe occurrence of forest fires. The network can be designed as a feedforward neural network, with an input layer consisting of the weather and environmental factors, one or more hidden layers, and an output layer representing the prediction of forest fire occurrence.\\nDuring training, the network adjusts its weights and biases to minimize the error between the predicted output and the actual output. After the classsification, It can be used to make predictions on new resources, allowing for early warning of potential forest fires.\\nThe Sequential Method\\nThe Sequential method is used to build a linear stack of layers in an ANN. This means that each layer in the model is connected to the previous layer, and the data flows through the layers sequentially. The Sequential method is finest and easy to use, making it a popular choice for building ANNs.\\nDense Layers\\nDense layers are those that are fully interconnected, meaning that every neuron in one layer is linked to every other neuron in the layer previously. using the equ. (4.15). Dense layers are used to introduce nonlinearity into the model and to perform transformations on the data.\\n\\n\\nThe combination of the Sequential method and Dense layers is a simple yet powerful way to build ANNs for various tasks such as image classification, NLP, and time series prediction.\\nIn the context of forest fire prediction, ANNs can be built using the Sequential method and Dense layers to analyse historical weather and environmental data and predict the occurrence of forest fires. The model would take input data such as temperature, relative humidity, FWI, wind speed, and precipitation, and use Dense layers with activation functions to perform transformations on the data before outputting a prediction\\n(4.15) {\"mathml\":\"<math style=\\\\\"font-family:stix;font-size:16px;\\\\\" xmlns=\\\\\"http://www.w3.org/1998/Math/MathML\\\\\"><mstyle mathsize=\\\\\"16px\\\\\"><mi>z</mi><mo>&#xA0;</mo><mo>=</mo><mo>&#xA0;</mo><mi>W</mi><mi>x</mi><mo>&#xA0;</mo><mo>+</mo><mo>&#xA0;</mo><mi>b</mi><mspace linebreak=\\\\\"newline\\\\\"/><mi>a</mi><mo>&#xA0;</mo><mo>=</mo><mo>&#xA0;</mo><mi>f</mi><mfenced><mi>z</mi></mfenced></mstyle></math>\"}  z space equals space W x space plus space b a space equals space f open parentheses z close parentheses \\n\\n\\n\\n\\nWhere:\\n* z is the weighted sum of the inputs.\\n* W is the weight matrix.\\n* x is the input vector.\\n* b is the bias vector.\\n* f() is the activation function.\\n* a is the output of the layer\\n  \\n\\n\\n\\nFig 4.1 Architecture Diagram of the neural network\\n\\n\\nThe inner layer receives the input data and passes it to the first dense/fully connected layer. Each dense/fully connected layer is followed by an activation function, which applies a non-linear transformation to the layer output. The non-linear activation function is necessary to enable the network to learn non-linear relationships between the input features and the target variable.\\n\\n\\nReLU (Rectified Linear Unit): The ReLU function returns the input if it is positive, and returns\\n0. ReLU is a popular choice for hidden layers in deep neural networks, as it is computationally efficient and performs well in practice.\\n\\n\\nThe number of dense/fully connected layers, the number of nodes in each layer, and the choice of activation functions are all hyperparameters that need to be chosen based on the specific task and dataset. The output layer typically has one neuron for binary classification tasks or multiple neurons for multi-class classification or regression tasks.\\nCHAPTER 5 RESULTS AND DISCUSSION\\n   1. Experimental Setup\\nWe first collected data related to forest fires, such as weather conditions, fuel moisture, topography, and other environmental factors typically associated with forest fires. This data was collected from various sources, such as Kaggle. We then pre-processed the data by performing various data-cleaning techniques, such as removing missing values and outliers. Next, we applied feature scaling techniques such as min-max scaling. We then applied threshold-based feature selection to identify the most important features that have a critical impact on forest fire prediction. This helped us reduce the dataset\\'s dimensionality and improve the model\\'s accuracy.\\n\\n\\nUpon pre-processing, the data were divided into training and testing sets using an 80/20 split, with 80% of the data being used to train the ANN and 20% being set aside for evaluating the model\\'s accuracy. The ANN used a sequential model architecture and was trained by the Adam optimizer. This is a popular optimization algorithm used in deep learning that combines the benefits of the momentum optimizer.\\n\\n\\nIn summary, our experimental setup involved collecting and pre-processing data, applying feature scaling and threshold-based feature selection techniques, training a neural network model, and evaluating the model\\'s performance.\\n   2. Dataset And Exploration\\n\\n\\nDataset Information:\\nMontesinho natural park Dataset\\nDataset Link - https://www.kaggle.com/code/elikplim/predict-the-burned-area-of-forest-fires/data\\nRelevant Paper - https://doi.org/10.1007/s11277-019-06697-0 Region - Montesinho natural park, northeast region of Portugal\\n  \\n\\n\\n\\nFig 5.1 - Fire Weather Observations\\nThe FFMC is an important indicator of fire danger, as dry surface fuels can ignite easily and spread quickly, especially under hot and windy conditions The DMC measures the moisture content of the forest floor, while the DC measures the dryness of deeper soil layers. Both codes are important components of the Forest Fire Weather Index System, as they help to estimate the likelihood and severity of forest fires under different weather conditions.\\nThe ISI measures the rate of fire spread that can be expected in the first few hours of a fire, taking into account wind speed and FFMC. It ranges from 0 (no spread) to 28 (extremely fast spread). The FWI combines the first three components (FFMC, DMC, and DC) to give an overall picture of fire danger in the forest. It ranges from 0 (low fire danger) to more than 100 (extreme fire danger).\\n\\n\\n\\n\\n\\n\\n  \\n\\nFig 5.2 Montesinho Natural Park dataset Sample\\nComponents of Montesinho Natural Park dataset:\\n  \\n\\n\\n\\nDataset types, check null values and memory usage:\\n  \\n\\nFig 5.3 Information of Montesinho Natural Park dataset Sample\\n\\n\\nThe figure 5.3 shows that there are 14 columns in the dataset. The dataset contains 517 non- null entries for each column, indicating no missing values. There are three columns with the data type \"object\": \"month\", \"day\", and \"size_category\". These likely represent categorical variables, such as\\nthe month and day of the week the fire occurred. In addition, whether the fire resulted in a small or large burned area.\\nThere are three columns with data type \"int64\": \"X\", \"Y\", and \"RH\". These likely represent integer values, such as the spatial coordinates of the fires and the relative humidity at the time of the fire. There are eight columns with data type \"float64\": \"FFMC\", \"DMC\", \"DC\", \"ISI\", \"temp\", \"wind\", \"rain\", and \"area\". These likely represent continuous numerical values, such as ratings related to the moisture content of forest fuels, weather conditions, and the size of the burned area. The dataset\\'s memory usage is 56.7+ KB, indicating that the dataset is relatively small and can be easily loaded into memory for analysis.\\n\\n\\nData Visualization\\n\\n\\n\\n\\n  \\n\\n\\t\\n\\n  \\n\\n\\t\\n\\nFig 5.4 Histogram of Montesinho Natural Park dataset.\\nIt can be seen that the majority of the distribution resembles the normal distribution as shown in the fig 5.4. However, X and Y have no distribution patterns, which makes sense given that we don\\'t know the natural park\\'s geography. Because of this, the distribution of trees for fires is not normal because we do not know where they are. We can\\'t do anything about it except drop the features, so we\\'ll keep them for now and see if they become a problem in the future. As the distribution of rain and area variables is skewed and contains outliers, both variables will be scaled. The scaling results for the rain variable appear insufficient, but for the variable, the distribution of area increases significantly.  \\n\\n\\nFig-5.5 Histogram of rain and area\\n\\n\\nThe distribution of rain and area variables shown in fig 5.5 is too skewed and has outliers. The scaling of the two variables will be done using natural logarithm scaling.The scaling results of the rain variable look not good enough, but for the variable the distribution area increases quite high.\\nDescriptive Statistics:\\n  \\n\\nFig-5.6 Descriptive Statistics of Montesinho Natural Park dataset Sample\\nThe variables FFMC, DMC, DC, and ISI shown in the fig 5.6, it represents ratings or indices related to the moisture content of forest fuels and the expected rate of fire spread. According to the mean values for these variables, moisture content was moderate to high. The expected rate of fire spread was low to moderate. However, it\\'s important to remember that these are just averages, and there could be a lot of variation in these ratings across different fires.\\n\\n\\nTemperature, RH, wind speed, and rainfall: These variables represent the weather conditions at time of the fires. The average temperature was 18.9 degrees Celsius, and the relative humidity was moderate at 44.3%. Wind speeds were also moderate, averaging 4.0 km/h. Rainfall was generally very light, averaging only 0.02 mm/m2. These conditions indicate that forest fires occurred during dry and mild weather.\\n\\n\\nArea: The size of the burned area for each fire, measured in hectares. In this variable, there is a mean value of 12.8 ha, but the standard deviation is large at 63.7 ha, indicating a great deal of variability. Furthermore, the minimum value for this variable is 0.0 ha, indicating that some fires did not result in any burned areas.\\n\\n\\nCorrelation Analysis\\n  \\n\\nFig 5.7 Correlation Analysis of Montesinho Natural Park dataset\\n\\n\\nIn the figure 5.7, the heatmap shows strong correlations between the indices FFMC, DMC, DC, ISI, and temperature because they are all major predictors of forest fire risk.\\nThe fuel moisture content, or FFMC, of fine fuels like grasses is influenced by weather conditions such as temperature, humidity, and precipitation. While DC reflects the dryness of the deeper layers of dense organic material, such as decomposed leaves and branches, DMC measures the moisture content in the deeper organic layers of the forest floor. Using the FFMC and wind speed, ISI calculates the potential rate of fire spread in the initial minutes following a fire.\\n\\n\\nOn the other hand, temperature directly influences forest fire circumstances. Fires can start and spread faster when the weather is hot and dry because there is more fuel available to burn. As a result of their connections and shared environmental influences, all of these indices are closely related to one another.\\n\\n\\nAlgerian Forest Fires Dataset\\n\\n\\nThe Algerian Forest fires dataset provides information on forest fires in 2 regions of Algeria - Bejaia and Sidi Bel-abbes. The dataset is available at this LINK and the relevant paper is available at this LINK.\\n  \\n\\n\\n\\nFig 5.8 Algerian Forest Fires Dataset Sample\\n\\n\\nAs the sample is shown in fig. 5.8, the dataset consists of 244 instances of data collected from two regions in Algeria. There are 11 attributes in all, with class serving as one of the output attributes. The data was gathered between June 2012 and September 2012.\\n\\n\\nThe class attribute indicates whether an instance belongs to the fire class (138 instances) or not fire class (106 instances). The dataset is evenly split between the two regions, with 122 instances for each region. The dataset can be used for classification tasks related to detecting fires in these regions during the specified time period.\\nComponents of the Algerian Forest Fires Dataset:\\n\\n\\n  \\n\\nDataset types, check null values and memory usage:\\n\\n\\n  \\n\\nFig 5.9 Information of Algerian Forest Fires Dataset Sample\\n\\n\\nIn this figure 5.9, the data contains 244 entries, meaning 244 observations. There are 13 columns in the dataset, and their names and types are given in the summary.\\nThe dataset appears to include information related to weather conditions and fire weather indices, as well as a column for class labels. The class labels column contains integers, and the rest of the columns contain integers or floats. The \"Non-Null Count\" column in the dataset shows that there are no values that are missing. The dataset\\'s memory usage is relatively small at 24.9 KB, indicating that the dataset is not very large.\\n\\n\\nData Visualization\\n\\n\\nYou can determine from a histogram whether your data is normally distributed, skewed, or has multiple peaks. Here are some general guidelines for histogram interpretation:\\n* The normal distribution is shown by a bell-shaped histogram.\\n* A histogram is skewed if one of the peaks is on the left or right side.\\n* A histogram with multiple peaks might suggest subpopulations.\\n* If your histogram shows no clear trend, your data might be random.\\n\\n\\n\\n\\n  \\n\\n\\t\\n\\n  \\n\\n\\tFig 5.10 Histogram of Algerian Forest Fires Dataset.\\nIn this fig 5.10 on this histogram of temperature and relative humidity in forest fire prediction are often bell-shaped, which suggests that they are normally distributed. This means that the majority\\nof the temperature and relative humidity measurements will cluster around a central value, with fewer measurements occurring further away from this value. Understanding the normal distribution of temperature and relative humidity is important for predicting forest fires because these factors play a significant role in determining the likelihood and severity of a fire.\\nHowever, other factors such as Rain, BUI (Buildup Index), FWI (Fire Weather Index), ISI (Initial Spread Index), DC , and DMC may exhibit skewed histograms, which means that the data is not evenly distributed. For example, a skewed histogram with a peak on the left side suggests that the data is concentrated on higher values. A peak on the right indicates that the data is concentrated at lower values.\\nUnderstanding the skewness of these factors is also crucial for predicting forest fires. For instance, the build-up index (BUI) measures the amount of fuel available for a fire to burn and is often positively skewed, meaning that most values are concentrated on the lower end of the scale. A right- skewed histogram suggests that most features are concentrated on the lower end of the scale, indicating a lower likelihood of forest fires.\\nDescriptive Statistics:\\n  \\n\\n\\n\\nFig 5.11 Descriptive Statistics of Algerian Forest Fires Dataset\\nIn the fig 5.11, the average temperature was 32.17 degrees Celsius, with a SD of 3.63 degrees Celsius. Temperatures ranged from 22 to 42 degrees Celsius, with fifty percent of the data falling between 30 and 35 degrees Celsius. With a standard deviation of 14.88%, the average relative humidity was 61.94%. The relative humidity ranged from 21% to 90%, with half of the data falling between 52% and 73.25%. The wind speed ranged from 6 to 29 kph, with an average of 15.50 kph.\\n\\n\\nDC, ISI, BUI, FFMC, DMC, and FWI are forest fire danger rating indices. The above indices had average values of 49.29, 4.76, 16.67, 77.89, 14.67, and 7.05, respectively. These indices\\' standard deviations ranged from 4.15 to 47.62. It is worth noting that DMC and BUI are relatively low compared to the other indices. In contrast, FFMC and FWI have higher average values, indicating a higher fire danger rating.\\n\\n\\nThe mean value for the target variable (classes) was 0.57, indicating that 57% of the days in the dataset had a forest fire danger rating of 1. The target variable\\'s minimum and maximum values were 0 and 1, respectively.\\n\\n\\nCorrelation Graph:\\n  \\n\\n\\n\\nFig 5.12 Correlation Analysis of Algerian Forest Fires Dataset\\n\\n\\nCorrelation analysis in the fig 5.12 looks at how these variables are related. The correlation map shows the close relationships between FFMC, DMC, DC, ISI, BUI, and FWI. This shows that they are connected and essential to preventing and controlling forest fires. For illustration, the FFMC provides information on the moisture content of litter and other fine fuels. Because the fuels are dry\\nwhen the FFMC is high, a fire may occur more frequently and with greater intensity. The deeper soil layers\\' moisture content, which might affect how quickly flames spread, is measured by the DC and DMC.\\n\\n\\nThe fuel moisture content, wind speed, and other weather variables that affect forest fire occurrence and spread are also connected the ISI, BUI, and FWI. Therefore, by understanding the inter linkages between these factors, forest fire prediction models can be developed to mitigate their impacts.\\n\\n\\n   3. Experiment Discussion\\n      1. Experiment-1 K Means Clustering:\\n  \\n\\nFig 5.13 Elbow Method for Montesinho Natural Park dataset Sample\\n\\n\\nThe Montesinho Natural Park dataset was subjected to k-means clustering, which produced two clusters of data points shown in the fig 5.13. We obtained the necessary labels for this dataset shown in the figure using elbow method. In the figure doesn\\'t show any negative outcomes. A metric for assessing classification results\\' efficacy is the silhouette score. By comparing the 2 data point cluster to the 9 data point cluster, we can see that the 2 data point cluster is correctly evaluated and the 9 data point cluster gives a negative graph shown in the figure 5.14. The clustering results show that the two clusters have different characteristics concerning the weather conditions and other factors related to forest fires. This is represented by the features labelled 0 and 1. It is important to remember the accuracy and meaning of the results can be impacted by the number of groups chosen, which features are used, and the completeness and reliability of the data.\\n\\n\\n\\n\\n  \\n\\n\\t\\n\\n  \\n\\n\\t\\n\\nFig 5.14 Comparison of data point cluster between 2 and 9.\\n\\n\\nClustered dataset of Montesinho Natural Park dataset using K means clustering:\\n\\n\\n  \\n\\n\\n\\n\\n\\nFig 5.15 Clustered Dataset of Montesinho Natural Park dataset.\\n\\n\\nForest fire datasets often encode relevant variables as integers to facilitate data processing and analysis. This includes the month variable and day variable, which are commonly encoded as an integer to allow mathematical and statistical models to predict future forest fires. So, in this figure 5.15, the Montesinho Natural Park dataset is encoded to integers. These labels generated using K- means clustering can help to better understand the patterns and causes of forest fires.\\n      2. Experiment-2 Feature Scaling:\\n\\n\\n  \\n\\nFig 5.16 Feature scaling of Montesinho Natural Park dataset.\\nIn this fig5.16, feature scaling process is performed on the Montesinho Natural Park dataset. The RMSE values to all of the scaling techniques appear to be very minimal, indicating that they are all successful in lowering variability and enhancing the accuracy of the data. The Max Absolute and Min Max scaling methods produced the lowest errors. Overall, Min Max and Max Absolute scaling methods may be good choices for further exploration for the Montesinho natural park dataset.\\n  \\n\\nFig 5.12 Feature scaling of Algerian Forest Fires Dataset.\\n\\n\\nIn this fig 5.17, A feature scaling process is performed on the Algerian Forest Fires Dataset. For each scaling method, there are some differences in the rmse values\\' magnitude. Quantile Transformer and Max Absolute scaling methods came in second and third, respectively, in terms of lowest error. The performance of standard scaling was also good, with a marginally higher but still incredibly low error value. Overall, Min Max scaling method is a good choice for further exploration for the Algerian Forest Fires Dataset.\\n      3. Experiment-3 Feature Selection:\\n\\n\\nBy effectively applying min max scaling on both datasets due to its low RMSE value among all options present, feature selection using various techniques are applied on both datasets and each method accuracy is recorded. The accuracy of Logistic Regression, KNN & SVM techniques containing four threshold, three threshold, two threshold and all features are taken into consideration to find out the best model.\\n IMG_256 \\n\\n\\n\\n\\n\\nFig 5.18 Accuracy Table using Logistic Regression for Montesinho natural park dataset\\n\\n\\nFig 5.18 gives a brief idea of how logistic regression performs for the selected features.While selecting Two threshold features for the above mentioned algorithm it gave the highest accuracy as 98.07% and it gave least accuracy of 86.5% for four threshold features. For all threshold features it gave accuracy around 96.15% and for three threshold features it gave 97.11%.\\n IMG_256 \\n\\n\\n\\n\\n\\nFig 5.19 Accuracy Table using Logistic Regression for Algerian Forest Fires Dataset\\n\\n\\nFig 5.19 gives a brief idea of how logistic regression performs for the selected features for Algerian Forest Fires Dataset. While selecting Two threshold features for the above-mentioned algorithm it gave the highest accuracy as 95.91% and it gave least accuracy of 89.7% for four threshold features. For all threshold features it gave accuracy around 91.83% and for three threshold features it also gave same accuracy as 91.83%\\n  \\n\\n\\n\\nFig 5.20 Accuracy Table using KNN for Montesinho natural park dataset\\n\\n\\nFig 5.20 gives a brief idea of how KNN performs for the selected features.While selecting Three threshold features for the above mentioned algorithm it gave the highest accuracy as 98.07% and it gave least accuracy of 92.3% for four threshold features. For all threshold features it gave accuracy around 96.15% and for two threshold features it gave 97.11%.\\n  \\n\\n\\n\\nFig 5.21 Accuracy Table using KNN for Algerian Forest Fires Dataset\\nFig 5.21 gives a brief idea of how logistic regression performs for the selected features for Algerian Forest Fires Dataset. While selecting Two threshold features for the above-mentioned algorithm it gave the highest accuracy as 97.95% and it gave least accuracy of 91.83% for three threshold features. For all threshold features it gave accuracy around 93.87% and for four threshold features it also gave accuracy around 95.91%\\n  \\n\\n\\n\\n\\n\\nFig 5.22 Accuracy Table using SVM for Montesinho natural park dataset\\nFig 5.22 gives a brief idea of how SVM performs for the selected features. While selecting Two threshold features and all threshold features for the above-mentioned algorithm it gave the highest accuracy as 98.07% and it gave least accuracy of 87.5% for four threshold features. For three threshold features it gave an accuracy of around 97.11%.\\n  \\n\\n\\n\\n\\n\\nFig 5.23 Accuracy Table using SVM for Algerian Forest Fires Dataset\\n\\n\\nFig 5.23 gives a brief idea of how logistic regression performs for the selected features for Algerian Forest Fires Dataset. While selecting Two threshold features for the above-mentioned algorithm it gave the highest accuracy as 95.91% and it gave least accuracy of 89.7% for four threshold features. For all threshold features it gave accuracy around 91.83% and for three threshold features it accuracy as 93.87%\\nOverall Results for Montesinho natural park dataset:\\n\\n\\nOverall considering all methods, Two Threshold was giving a relatively high accuracy compared to the other. The highest accuracy it gave was 98.08%. The features that are selected in two threshold are as follows:-\\'X\\', \\'Y\\',\\'FFMC\\', \\'DMC\\', \\'DC\\', \\'ISI\\', \\'RH\\',\\'temp\\', \\'wind\\',\\'area\\',\\'day_encoded\\'. Basically, these features combined would give a better predictive accuracy for forest fire. So, these features can only be taken up for the next step i.e., neural network process while other features can be ignored as it doesn’t hold that much significance for predicting forest fire. The highest accuracy for two threshold i.e., 98.07% is shown by both logistic regression and SVM while for KNN it showed 97.11%. Accuracy was almost consistent so features present in two threshold are taken for further analysis.\\nOverall Results for Algerian Forest Fires Dataset:\\n\\n\\nOverall considering all methods, Two threshold for 2nd dataset also was giving a relatively high accuracy compared to the other. The highest accuracy it gave was 97.95%.The features that are selected in two threshold are:-\\'year\\',\\'Temperature\\',\\'Ws\\',\\'Rain\\',\\'FFMC\\', \\'FWI\\',\\'DMC\\', \\'DC\\', \\'ISI\\',\\'BUI\\'.So these features can only be taken up for the next step i.e., neural network process while\\nother features can be ignored . The highest accuracy is shown by KNN i.e., 97.95% while logistic regression and Support Vector Machine gave accuracy as 95.91% for two threshold. The accuracy was almost consistent in all three methods so features present in two threshold are taken for further analysis.\\nThe results/features found out from the feature selection processes will be carry forwarded to the final process i.e., using neural network for accurate prediction of forest fire. This process made the dataset much more compact and cleaner for fast,easier and accurate processing now.\\n      4. Experiment-4 Classification\\nOur proposed model\\'s accuracy of 98% . This indicates that our approach, which incorporates Min-Max feature scaling and a selected set of features, has significantly improved the performance of the neural network model compared to the existing method. The selection of features in our study, which includes \\'X\\', \\'Y\\', \\'DMC\\', \\'DC\\', \\'ISI\\', \\'temp\\', \\'wind\\', \\'area\\', \\'RH\\', \\'FFMC\\', and \\'day_encoded\\', has played a important role in achieving the high accuracy of the model. So the accuracy curves has shown in the figure which have epoch of 100 and batch size 10. The train and valid curves are align to each other it shows the accurate.\\n  \\n\\nFig 5.24 Accuracy curve of Montesinho Natural Park dataset.\\n\\n\\nAs shown in the figure 5.24, the accuracy curves show how a neural network model performs during training and validation. The fact that the train and valid curves align indicates that the model generalizes well. This means that it is not overfitting or underfitting the data.\\n\\n\\nIn neural network training, an \"epoch\" refers to one complete iteration through the entire training dataset. In this case, the model a batch size of 10 was trained with 100 epochs.\\nThe fact that the accuracy curves for both validation and training data show an upward trend and align with each other suggests that the model is learning from the data and improving its predictions over time. Moreover, the fact that accuracy is relatively high indicates that the model performs well on both training and validation data.\\n\\n\\nIn conclusion, our model presents a novel approach for clustered forest fire prediction using a neural network model with Min-Max feature scaling and a carefully selected set of features. The results demonstrate that our proposed system achieved an impressive accuracy of 98%, surpassing the accuracy of the existing method reported in the literature.\\n  \\n\\n\\n\\n\\n\\nFig 5.25 Batch size Accuracy curves for the Montesinho Natural Park dataset.\\nIn the fig 5.25, the accuracy curves illustrate the performance of a neural network model developed using different batch sizes for the Montesinho Natural Park dataset. The trainings were designed to determine a suitable batch size that impacts a balance between memory use, training time, and generalization performance.\\n\\n\\nThe train alignment and valid curves for batches 4, 6, and 10 show that the model generalizes well. The alignment of the train and valid curves for these batch sizes indicates that the model is not over- or under-fitting the data, but rather striking an appropriate balance between bias and variance.\\n\\n\\nOur model uses a neural network with Min-Max feature scaling and a selected set of features, including \\'Temp\\', \\'ISI\\', \\'FFMC\\', \\'DMC\\', \\'DC\\', \\'BUI\\', and \\'FWI\\'. The accuracy of our proposed method proved to be 97.5% for the Algerian Forest Fires Dataset. We achieved 97.5% accuracy with our proposed method compared to 84% accuracy with the existing \"Predicting Forest Fire in Algeria Using Data Mining Techniques\" method.\\nThis indicates that our approach, which incorporates Min-Max feature scaling and a different set of features, has improved the predictive execution of the neural network model comparation to the existing method. The selection of features in our study, including \\'Temperature\\', \\'ISI\\', \\'FFMC\\', \\'DMC\\', \\'DC\\', \\'BUI\\', and \\'FWI\\', has played a important role in achieving high accuracy of the model. By selecting only, the most informative features, we reduced noise and improved the model\\'s accuracy by allowing it to focus on the most significant variables.\\n  \\n\\nFig 5.26 Accuracy curve of the Algerian Forest Fires Dataset.\\n\\n\\nAs shown in the fig 5.26, the accuracy curves show how a neural network model performs during training and validation. The fact that the train and valid curves align indicates that the model generalizes well. This means that it is not overfitting or underfitting the data.\\nIn neural network training, an \"epoch\" refers to one complete iteration through the entire training dataset. The batch size refers to the number of training examples used in each iteration. In this case, the model was trained for 100 epochs with a batch size of 10.\\n\\n\\nThe fact that the accuracy curves for both training and validation data show an upward trend and align with each other suggests that the model is learning from the data and improving its predictions over time. Moreover, the fact that accuracy is relatively high indicates that the model performs well on both training and validation data.\\nIn conclusion, our study presents a novel approach to clustered forest fire prediction using a neural network model with Min-Max feature scaling and a selected set of features. Based on our results, we showed that our proposed system was more accurate than existing methods, achieving 97.5% accuracy.\\n  \\n\\n\\n\\nFig 5.27 Batch size Accuracy curves for the Algerian Forest Fires Dataset.\\nIn the figure 5.27, the accuracy curves illustrate the performance of a neural network model developed using different batch sizes for the Algerian Forest Fires Dataset. The experiments were designed to determine a suitable batch size that impacts a balance between memory use, training time, and generalization performance.\\nThe train alignment and valid curves for batches 4, 6, 10 and 32 show that the model generalizes well. The alignment of the train and valid curves for these batch sizes indicates that the model is not over- or under-fitting the data, but rather striking an appropriate balance between bias and variance.\\nOverall Results of Neural Network for Montesinho natural park dataset:\\n\\n\\nOur proposed model\\'s accuracy of 98% outperforms the existing Semisupervised Classification Based Clustering Approach in the Wireless Sensor Network for Forest Fire Detection method which achieved an accuracy of 96%. This indicates that our approach, which incorporates Min-Max feature scaling and a selected set of features, has significantly improved the performance of the neural network model compared to the existing method. The selection of features in our study, which includes \\'X\\', \\'Y\\', \\'FFMC\\', \\'DMC\\', \\'DC\\', \\'ISI\\', \\'RH\\', \\'temp\\', \\'wind\\', \\'area\\', and \\'day_encoded\\', has played a crucial role in achieving the high accuracy of the model.\\nIn conclusion, our model presents a novel approach for clustered forest fire prediction using a neural network model with Min-Max feature scaling and a carefully selected set of features. The results demonstrate that our proposed system achieved an impressive accuracy of 98%, surpassing the accuracy of the existing method reported in the literature. The findings highlight the importance of feature scaling and feature selection in improving the model\\'s performance for forest fire prediction.\\nOverall Results of Neural Network for Algerian Forest Fires Dataset:\\n\\n\\nOur model uses a neural network with Min-Max feature scaling and a selected set of features, including \\'Temperature\\', \\'ISI\\', \\'FFMC\\', \\'DMC\\', \\'DC\\', \\'BUI\\', and \\'FWI\\'. The accuracy of our proposed method proved to be 96%. We achieved 96% accuracy with our proposed method compared to 84% accuracy with the existing \"Predicting Forest Fire in Algeria Using Data Mining Techniques\" method. This indicates that our approach, which incorporates Min-Max feature scaling and a different set of features, has improved the predictive performance of the neural network model compared to the existing method. The selection of features in our study, including \\'Temperature\\', \\'ISI\\', \\'FFMC\\', \\'DMC\\', \\'DC\\', \\'BUI\\', and \\'FWI\\', has played a crucial role in achieving the high accuracy of the model. By\\nselecting only the most informative features, we reduced noise and improved the model\\'s accuracy by allowing it to focus on the most significant variables.\\nIn conclusion, our study presents a novel approach to clustered forest fire prediction using a neural network model with Min-Max feature scaling and a selected set of features. Based on our results, we showed that our proposed system was more accurate than existing methods, achieving 96% accuracy.\\n5.5 Comparative Study\\n\\n\\nMontesinho natural park Dataset\\n\\n\\nS.No\\n\\tAlgorithm\\n\\tBest Accuracy %\\n\\t1.\\n\\tKNN\\n\\t98.07%\\n\\t2.\\n\\tLogistic Regression\\n\\t98.07%\\n\\t3.\\n\\tSupport Vector Machine\\n\\t98.07%\\n\\t4.\\n\\tNeural Network\\n\\t98.07%\\n\\tTable 5.1 Results of Montesinho natural park Dataset\\n\\n\\n\\n\\nTable 5.1 depicts the overall analysis of Montesinho natural park dataset taking into consideration of all algorithms. All the algorithm for feature selection method used in our project gives the best possible accuracy as 98.07%.\\nCarry forwarding those features which gave the best possible accuracy for forest fire prediction using the technique neural network also gave accuracy as 98.07% for prediction. So out detailed analysis on feature selection and from that predicting forest fire was a successful try.\\nAlgerian Forest Fires Dataset\\n\\n\\nS.No\\n\\tAlgorithm\\n\\tBest Accuracy %\\n\\t1.\\n\\tKNN\\n\\t97.9%\\n\\t2.\\n\\tLogistic Regression\\n\\t95.9%\\n\\t3.\\n\\tSupport Vector Machine\\n\\t95.9%\\n\\t4.\\n\\tNeural Network\\n\\t97.9%\\n\\tTable 5.2 Results of Algerian Forest Fires Dataset\\nTable 5.2 depicts the overall analysis of Algerian forest fires dataset taking into consideration of all algorithms. The KNN algorithm for feature selection method used in our project gives the best possible accuracy as 97.9%. While Logistic regression and support vector machine method gave best possible accuracy as 95.9%.\\nCarry forwarding those features which gave the best possible accuracy for forest fire prediction using the technique neural network also gave accuracy as 97.9% for prediction. So out detailed analysis on feature selection and from that predicting forest fire was a successful try to implement the proposed method.\\nCHAPTER 7\\nCONCLUSION AND FUTURE ENHANCEMENT\\n\\n\\nForest fire prediction is crucial for preventing and minimizing their devastating impact on the environment and human life. Clustering, feature scaling, feature selection, and deep learning techniques can be combined to improve the accuracy of forest fire prediction. Clustering can help identify patterns and trends, while feature scaling ensures equal importance is given to different features. Feature selection helps in identifying relevant features, and deep learning can identify complex patterns and relationships between features. By accurately predicting forest fires, authorities can take necessary measures to prevent their spread and protect the environment and population.\\nUsing neural networks with effective feature scaling and threshold-based feature selection for numerical data is a promising approach for improving the accuracy of forest fire prediction models. This technique can help identify the most important variables that contribute to the prediction of forest fires, while also reducing the dimensionality of the input data, which can make the model more efficient and faster to train.\\nFor clustered forest fire prediction using neural networks with effective feature scaling and threshold-based feature selection, there are a number of possible future improvements and areas for further research. These include developing hybrid models that combine multiple machine learning techniques, investigating ensemble methods to combine multiple models for improved prediction accuracy, researching advanced deep learning techniques like CNNs or RNNs to capture more complex patterns in the data, incorporating additional spatial and temporal information for context- aware predictions, developing real-time prediction capabilities for proactive fire management, and improving model. Studying these areas may advance the discipline and result in more accurate and efficient prediction models for forest fires.\\nREFERENCES\\n\\n\\n1. Kee-Won Seong, Jang Hyun Sunga, and Young Ryub.Deep Learning-Based Prediction of Fire Occurrence in South Korea in 2021 in Relation to Hydroclimatic Conditions and Drought Phase. https://doi.org/10.3390/su14095494 \\n2. Pranati Rakshit, Srestha Sarkar, Sambit Khan, Pritam Saha, Sonali Bhattacharyya, Nilarpan Dey, Sardar M.N. Islam, and Souvik Pal are among the participants. 1-6, 9719887 Prediction of Forest Fire Using Machine Learning Algorithms: The Search for the Better Algorithm, 2021.DOI:10.1109/CITISIA53721.2021\\n3. Aishwarya Beelagi, Preeti T., Dr. Suvarna Kanakaraddi, Sumalata Malagi, and Aishwarya Sudi.Machine learning techniques for predicting forest fires..,2021. Reference: 10.1109/CONIT51480.2021.9498448\\n4. Raj Vikram,Ayan Kumar Das, Ditipriya Sinha, and Debashis De. 2020. Energy-efficient data forwarding for forest fire detection utilising localization algorithm via wireless sensor network.DOI: 10.1007/s11276-020-02393-1\\n5. Wayan Firdaus Mahmudy, Mochammad Anshori, Farhanna Mar\\'i, and Muhammad Wildan Alauddin.Extreme Learning Machines (ELM)-based neural network for forest fire prediction. 2020. DOI: 10.1109/SIET48054.2019.8986106\\n6. Sinha, D., Tripathi, S. & Kumari R Semisupervised Classification Based Clustering Approach in WSN for Forest Fire Detection. Wireless Pers Commun 109, 2561–2605 (2019). https://doi.org/10.1007/s11277-019-06697-0\\n7. Tehrany, M.S., Jones, S., Shabani, F. et al. A novel ensemble modeling approach for the spatial prediction of tropical forest fire susceptibility using LogitBoost machine learning classifier and multi-source geospatial data. Theor Appl Climatol 137, 637–653 (2019). https://doi.org/10.1007/s00704-018-2628-9\\n8. Singh, K.R., Neethu, K.P., Madhurekaa, K., Harita, A., & Mohan, P. (2021). Parallel SVM model for forest fire prediction. Soft Computing Letters, 3, 100014. https://doi.org/10.1016/j.socl.2021.100014\\n9. Vikram, R., Sinha, D. FogFire: fog assisted IoT enabled forest fire management. Evol. Intel. 16, 329–350 (2023). https://doi.org/10.1007/s12065-021-00666-y\\n10. Xie, Y., Peng, M. Forest fire forecasting using ensemble learning approaches. Neural Compute & Application 31, 4541–4550 (2019). https://doi.org/10.1007/s00521-018-3515-0\\n11. Qayum, A., Ahmad, F., Arya, R. et al. Predictive modeling of forest fire using geospatial tools and strategic allocation of resources: eForestFire. Stoch Environ Res Risk Assess 34, 2259– 2275 (2020).  https://doi.org/10.1007/s00477-020-01872-3\\n\\n\\n12. Vikram, R., Sinha, D. A multimodal framework for Forest fire detection and monitoring. Multimedia Tools Appl 82, 9819–9842 (2023). https://doi.org/10.1007/s11042-022-13043-3\\n13. R. Thakkar, V. Abhyankar, P. D. Reddy and S. Prakash, \"Environmental Fire Hazard Detection and Prediction using Random Forest Algorithm,\" 2022 International Conference for Advancement in Technology (ICONAT), Goa, India, 2022, pp. 1-4, doi: 10.1109/ICONAT53423.2022.9726029\\n14. T. Bhatt and A. Kaur, \"Automated Forest Fire Prediction Systems: A Comprehensive Review,\" 2021 9th International Conference on Reliability, Infocom Technologies and Optimization (Trends and Future Directions) (ICRITO), Noida, India, 2021, pp. 1-5, doi: 10.1109/ICRITO51393.2021.9596528\\n15. D. C. J. Kani and S. Saudia, \"Analysis on the Performance of Machine Learning Models for Forest Fire Prediction,\" doi: 10.1109/ICSSIT55814.2023.10060870\\n16. N. Hidayanto, A. H. Saputro and D. E. Nuryanto, \"Peatland Data Fusion for Forest Fire Susceptibility Prediction Using Machine Learning,\" 2021 4th International Seminar on Research of Information Technology and Intelligent Systems (ISRITI), Yogyakarta, Indonesia, 2021, pp. 544- 549, doi: 10.1109/ISRITI54043.2021.9702762\\n17. M. W. Alauddin, M. Anshori, A. S. Wicaksono and F. Utaminingrum, \"Comparative Study based on Error Calculation in Multiple Linear Regression Coefficient for Forest Fires Prediction,\" 2018 International Conference on Sustainable Information Engineering and Technology (SIET), Malang, Indonesia, 2018, pp. 115-120, doi: 10.1109/SIET.2018.8693226\\n18. R. S. Aakash, M. Nishanth, R. Rajageethan, R. Rao and R. Ezhilarasie, \"Data Mining Approach to Predict Forest Fire Using Fog Computing,\" 2018 Second International Conference on Intelligent Computing and Control Systems (ICICCS), Madurai, India, 2018, pp. 1582-1587, doi: 10.1109/ICCONS.2018.8663160\\n19. S. Pawar, K. Pandit, R. Prabhu, R. Samaga and Geethalaxmi, \"A Machine Learning Approach to Forest Fire Prediction Through Environment Parameters,\" 2022 International Conference on Artificial Intelligence and Data Engineering (AIDE), Karkala, India, 2022, pp. 1-7, doi: 10.1109/AIDE57180.2022.10059873  \\n20. I. S. Sineva and M. D. Molovtsev, \"An Integrated Approach to the Regression Problem in Forest Fires Detection,\" 2020 doi: 10.1109/IEEECONF48371.2020.9078645  \\n21. N. Omar, A. Al-zebari and A. Sengur, \"Deep Learning Approach to Predict Forest Fires Using Meteorological Measurements,\" doi: 10.1109/IISEC54230.2021.9672446\\n22. Mimboro, Prasetyo & Yanuargi, Bayu & Surimba, Romdhi & Kusrini, Kusrini & Khusnawi, Khusnawi. (2022). Forest Fire Prediction Using K-Mean Clustering and Random Forest Classifier. CSRID (Computer Science Research and Its Development Journal). 14. 157. 10.22303/csrid.14.2.2022.157-165. http://dx.doi.org/10.22303/csrid.14.2.2022.157-165\\n23. Khairani, Nabila & Sutoyo, Edi. (2020). Application of K-Means Clustering Algorithm for Determination of Fire-Prone Areas Utilizing Hotspots in West Kalimantan Province. International Journal of Advances in Data and Information Systems. 1. 9-16. http://dx.doi.org/10.25008/ijadis.v1i1.13\\n24. P. Mimboro, K. Kusrini and A. D. Laksito, \"Spatial Hotspot Data and Weather for Forest Fire Data Clustering,\" 2022 5th International Conference on Information and Communications Technology (ICOIACT), Yogyakarta, Indonesia, 2022, pp. 160-165, doi: 10.1109/ICOIACT55506.2022.9971884.\\n25. Li, Yuanwei & Sikui, Zhang & Fu, Guoyi. (2022). Forest fire modeling and analysis based on K-means clustering algorithm and time series forecasting. 310-316. 10.1145/3523286.3524560.\\n26. Huang, C. W., Cheng, K. Y., Chou, Y. H., & Liu, T. Y. (2019). The impacts of feature scaling on wildfire prediction: A comparison of machine learning models. Journal of Environmental Management, 248, 109291. doi: 10.1016/j.jenvman.2019.109291\\n27. Ahsan, Md Manjurul & Mahmud, M. & Saha, Pritom & Gupta, Kishor Datta & Siddique, Zahed. (2021). Effect of Data Scaling Methods on Machine Learning Algorithms and Model Performance. Technologies. 9. 52. 10.3390/technologies9030052.\\n28. Koutsaftakis, I., Kanellopoulos, I., & Soulios, G. (2016). Data preprocessing techniques for the optimization of forest fire prediction models. Ecological Modelling, 320, 138-150. doi: 10.1016/j.ecolmodel.2015.09.003\\n29. Shreya, M., Rai, R., Shukla, S. (2023). Forest Fire Prediction Using Machine Learning and Deep Learning Techniques. In: Smys, S., Lafata, P., Palanisamy, R., Kamel, K.A. (eds) Computer Networks and Inventive Communication Technologies. Lecture Notes on Data Engineering and Communications Technologies, vol 141. Springer, Singapore. \\nhttps://doi.org/10.1007/978-981-19-3035-5_51\\n\\n\\n   30. Al-Shourbaji, I., Alhameed, M., Katrawi, A., Jeribi, F., Alim, S. (2022). A Comparative Study for Predicting Burned Areas of a Forest Fire Using Soft Computing Techniques. In: Kumar, A., Senatore, S., Gunjan, V.K. (eds) ICDSMLA 2020. Lecture Notes in Electrical Engineering, vol 783. Springer, Singapore. https://doi.org/10.1007/978-981-16-3690-5_22\\nAPPENDIX\\nThis section contains the code for developing the project\\n\\n\\nCode for K Means Clustering:\\nImporting necessary libraries and packages\\nimport numpy as np\\nimport matplotlib.pyplot as plt import seaborn as sns\\nimport pandas as pd\\nEncoding the month and days\\nfrom sklearn.preprocessing import LabelEncoder labelencoder = LabelEncoder()\\ndf[\\'month_encoded\\'] = labelencoder.fit_transform(df[\\'month\\']) df[[\\'month\\', \\'month_encoded\\']].sample(20)\\ndf[\\'day_encoded\\'] = labelencoder.fit_transform(df[\\'day\\']) df[[\\'day\\', \\'day_encoded\\']].sample(20)\\nStandard Scaling\\nfrom sklearn.preprocessing import StandardScaler sc = StandardScaler()\\ndfsc = sc.fit_transform(df)\\ndfsc = pd.DataFrame(dfsc,columns=df.columns) dfsc.head()\\nApplying K Means clustering to the dataset\\nfrom sklearn.cluster import KMeans err = []\\nfor i in range(1,20):\\nkm = KMeans(n_clusters=i) km.fit(dfsc) err.append(km.inertia_)\\nApplying elbow method plt.plot(range(1,20),err,marker=\\'o\\') plt.xlabel(\"No. of Clusters\") plt.ylabel(\"Error\")\\nplt.title(\"Elbow Method\") plt.axvline(2,color=\"r\") plt.show()\\nkm1 = KMeans(n_clusters=2) km1.fit(dfsc)\\nkm1.labels_\\nApplying silhouette score\\nfrom sklearn.metrics import silhouette_score for i in range(2,10):\\nkm = KMeans(n_clusters=i) km.fit(dfsc)\\nsil_score = silhouette_score(dfsc,km.labels_) print(sil_score)\\nfrom yellowbrick.cluster import SilhouetteVisualizer km5 = KMeans(n_clusters=2)\\nsil_vi = SilhouetteVisualizer(km5) sil_vi.fit(dfsc)\\nplt.show()\\nkm9 = KMeans(n_clusters=9) sil_vi = SilhouetteVisualizer(km9) sil_vi.fit(dfsc)\\nplt.show(\\nDownloading the clustered dataset\\ndf_forestfires.to_csv(\\'clustered.csv\\', encoding=\\'utf-8\\')\\nfiles.download(\\'clustered.csv\\') Code for Feature Scaling Standard Scaling\\nfrom sklearn.preprocessing import StandardScaler sc = StandardScaler()\\nsc.fit(df)\\nscdf = sc.transform(df)\\nx_train,x_test,y_train,y_test = train_test_split(scdf,y,test_size=0.2) lr = LinearRegression()\\nlr.fit(x_train,y_train) pred = lr.predict(x_test)\\nrmsestd = np.sqrt(metrics.mean_squared_error(y_test,pred)) rmsestd\\nMin-Max Scaling\\nfrom sklearn.preprocessing import MinMaxScaler mms = MinMaxScaler()\\nmms.fit(df)\\nmmsdf = mms.transform(df)\\nx_train,x_test,y_train,y_test = train_test_split(mmsdf,y,test_size=0.2) lr = LinearRegression()\\nlr.fit(x_train,y_train) pred = lr.predict(x_test)\\nrmsemms = np.sqrt(metrics.mean_squared_error(y_test,pred)) rmsemms\\nRobust Scaling\\nfrom sklearn.preprocessing import RobustScaler rs = RobustScaler()\\nrs.fit(df)\\nrsdf = rs.transform(df)\\nx_train,x_test,y_train,y_test = train_test_split(rsdf,y,test_size=0.2) lr = LinearRegression()\\nlr.fit(x_train,y_train) pred = lr.predict(x_test)\\nrmserbs = np.sqrt(metrics.mean_squared_error(y_test,pred)) rmserbs\\nMax-Abs Scaling\\nfrom sklearn.preprocessing import MaxAbsScaler rss = MaxAbsScaler()\\nrss.fit(df)\\nrssdf = rss.transform(df)\\nx_train,x_test,y_train,y_test = train_test_split(rssdf,y,test_size=0.2) lr = LinearRegression()\\nlr.fit(x_train,y_train) pred = lr.predict(x_test)\\nrmsemas = np.sqrt(metrics.mean_squared_error(y_test,pred)) rmsemas\\nQuantile Transformer\\nfrom sklearn.preprocessing import QuantileTransformer qt = QuantileTransformer()\\nqt.fit(df)\\nqtdf = qt.transform(df)\\nx_train,x_test,y_train,y_test = train_test_split(qtdf,y,test_size=0.2) lr = LinearRegression()\\nlr.fit(x_train,y_train) pred = lr.predict(x_test)\\nrmseqts = np.sqrt(metrics.mean_squared_error(y_test,pred))\\nrmseqts\\nPower Transformer\\nfrom sklearn.preprocessing import PowerTransformer pt = PowerTransformer(method = \\'yeo-johnson\\') pt.fit(df)\\nptdf = qt.transform(df)\\nx_train,x_test,y_train,y_test = train_test_split(ptdf,y,test_size=0.2) lr = LinearRegression()\\nlr.fit(x_train,y_train) pred = lr.predict(x_test)\\nrmsepts = np.sqrt(metrics.mean_squared_error(y_test,pred)) rmsepts\\n\\n\\nCode for Feature Selection\\nExtra Tree Classifier Feature Selection Method\\netc = ExtraTreesClassifier() etc.fit(df, y) print(etc.feature_importances_)\\nfeat_importances = pd.Series(etc.feature_importances_, index=df.columns) feat_importances.nlargest(13).plot.bar()\\nplt.show() list1=feat_importances.keys().to_list()\\n  \\n\\n\\n\\n\\n\\nFeature Importance graph for Montesinho natural park dataset\\n\\n\\n\\n\\n\\n\\n  \\n\\n\\n\\n\\n\\nFeature Importance graph for Algerian Forest Fires dataset\\n\\n\\nForward Selection Feature Selection Method\\nforward_feature_selector = SequentialFeatureSelector(RandomForestClassifier(n_jobs=-1), k_features=6,\\nforward=True, verbose=2, scoring=\\'roc_auc\\', cv=4)\\nfselector = forward_feature_selector.fit(df,y)\\n  \\n\\nForward Selection Feature for Montesinho natural park dataset\\n  \\n\\n\\n\\nForward Selection Feature for Algerian Forest Fires dataset\\n\\n\\nTo Display Features obtained from Forward Selection Feature Selection Method\\nfward=fselector.k_feature_names_ list2=list(fward)\\nlist2\\n  \\n\\nForward Selection Feature from Montesinho natural park dataset\\n\\n\\n  \\n\\n\\n\\n\\n\\nForward Selection Feature from Algerian Forest Fires dataset\\n\\n\\nChi Square Feature Selection Method\\nchi2_features = SelectKBest(chi2, k = 6) X_kbest_features = chi2_features.fit_transform(df, y) mask=chi2_features.get_support()\\nnew_feature=[]\\nfor bool,feature in zip(mask,df.columns):\\nif bool:\\nnew_feature.append(feature) list3=new_feature\\nlist3\\n  \\n\\n\\n\\nFeature selected on chi square from Montesinho natural park dataset\\n  \\n\\n\\n\\nFeature selected on chi square from Algerian Forest Fires dataset\\n\\n\\nPearson Correlation Feature Selection Method\\ncor_target = abs(corr[\"area\"])\\n\\n\\nrelevant_features = cor_target[cor_target>0.05] list4=relevant_features.keys().to_list()\\nlist4\\n  \\n\\n\\n\\nFeature selected on pearson correlation from Montesinho natural park dataset\\n  \\n\\nFeature selected on pearson correlation from Algerian Forest Fires dataset\\n\\n\\nTo Display Features obtained from above Feature Selection Method\\nfinal_list=[list1,list2,list3,list4] name=[\\'Extra_Tree_Classifier\\',\\'Forward_Selection\\',\\'CHI-square\\',\\'Pearson Correlation\\'] dflist=pd.DataFrame(list(zip(name,final_list)),columns=[\\'Name\\',\\'Features\\'])\\ndflist\\n  \\n\\n\\n\\nSelected features table from Montesinho natural park dataset\\n  \\n\\n\\n\\nSelected features table from Algerian Forest Fires dataset\\n\\n\\nTo find common features present in all Feature Selection Method\\nfeat=[]\\n\\n\\nfeat= [value for value in list1 if value in list2 and value in list3 and value in list4 ] feat\\n  \\n\\n\\n\\n\\n\\nCommon features selected from Montesinho natural park dataset\\n  \\n\\n\\n\\nCommon features selected from Algerian Forest Fires dataset\\n\\n\\nTo Train the model from results of above Feature Selection Method\\ndffs = df[[\\'temp\\']]\\ndffsf = df[[\\'DMC\\', \\'DC\\', \\'temp\\',\\'wind\\',\\'area\\']]\\n\\n\\ndffsff = df[[\\'X\\', \\'Y\\',\\'FFMC\\', \\'DMC\\', \\'DC\\', \\'ISI\\', \\'RH\\',\\'temp\\', \\'wind\\',\\'area\\',\\'day_encoded\\']] x_train,x_test,y_train,y_test = train_test_split(df,y,test_size=0.2) x_trainfs,x_testfs,y_trainfs,y_testfs = train_test_split(dffs,y,test_size=0.2) x_trainfsf,x_testfsf,y_trainfsf,y_testfsf = train_test_split(dffsf,y,test_size=0.2) x_trainfsff,x_testfsff,y_trainfsff,y_testfsff = train_test_split(dffsff,y,test_size=0.2) trainX = [x_train, x_trainfs, x_trainfsf,x_trainfsff]\\ntestX = [x_test, x_testfs, x_testfsf, x_testfsff] trainY = [y_train, y_trainfs, y_trainfsf, y_trainfsff] testY = [y_test, y_testfs, y_testfsf, y_testfsff] logreg = LogisticRegression()\\nknn = KNeighborsClassifier(n_neighbors=5) svm = SVC(kernel=\\'linear\\')\\nTo obtain accuracy of trained model using Logistic Regression\\naccu=[]\\n\\n\\nfor i in range(len(trainX)): logreg.fit(trainX[i],trainY[i]) pred = logreg.predict(testX[i])\\naccu.append(metrics.accuracy_score(testY[i],pred))\\n\\n\\ndf_lr = pd.DataFrame({\\'Accuracy \\':accu},index=[\\'All\\',\\'Four Threshold\\',\\'Three Threshold\\',\\'Two Threshold\\'])\\ndf_lr\\n\\n\\nTo obtain accuracy of trained model using KNN\\naccu=[]\\n\\n\\nfor i in range(len(trainX)): knn.fit(trainX[i],trainY[i])\\npred = knn.predict(testX[i]) accu.append(metrics.accuracy_score(testY[i],pred))\\ndf_knn = pd.DataFrame({\\'Accuracy \\':accu},index=[\\'All\\',\\'Four Threshold\\',\\'Three Threshold\\',\\'Two Threshold\\'])\\ndf_knn\\n\\n\\nTo obtain accuracy of trained model using SVM\\naccu=[]\\n\\n\\nfor i in range(len(trainX)): svm.fit(trainX[i],trainY[i]) pred = svm.predict(testX[i])\\naccu.append(metrics.accuracy_score(testY[i],pred)) esult\\ndf_svmfc = pd.DataFrame({\\'Accuracy \\':accu},index=[\\'All\\',\\'Four Threshold\\',\\'Three Threshold\\',\\'Two Threshold\\'])\\ndf_svmfc\\n\\n\\nCode for Neural Network\\nImporting necessary libraries and packages:\\nimport numpy as np import pandas as pd\\nimport matplotlib.pyplot as plt plt.style.use(\\'seaborn\\')\\nimport seaborn as sns\\nfrom sklearn.preprocessing import LabelEncoder, StandardScaler, MinMaxScaler from sklearn.model_selection import train_test_split\\nfrom sklearn.metrics import r2_score import tensorflow as tensorflow from keras.models import Sequential\\nfrom keras.layers import Dense, Dropout\\nfrom tensorflow import keras\\nfrom tensorflow.keras import layers\\nfrom tensorflow.keras.optimizers import SGD from tensorflow.keras.utils import to_categorical from keras.callbacks import EarlyStopping\\nfrom keras.callbacks import ModelCheckpoint from keras.utils.vis_utils import plot_model Loading the Dataset:\\nfrom google.colab import files uploaded = files.upload() import io\\ndf = pd.read_csv(io.BytesIO(uploaded[\\'forestfires (3).csv\\'])) df.head(10)\\nData exploration:\\ndf.info()\\nChecking the Null Values in the dataset:\\ndf.isnull().sum() df.describe().transpose() Correlation Graph:\\nfig, ax = plt.subplots(figsize=(16,10)) dataplot = sns.heatmap(df.corr(), cmap=\"YlGnBu\", annot=True) plt.show()\\nData Preprocessing Month sns.countplot(df[\\'month\\']) plt.title(\\'Count plot of months\\')\\nseason_dict = {\\'dec\\' : \\'winter\\', \\'jan\\' : \\'winter\\', \\'feb\\' : \\'winter\\',\\n\\'mar\\' : \\'spring\\', \\'apr\\' : \\'spring\\', \\'may\\' : \\'spring\\',\\n\\'jun\\' : \\'summer\\', \\'jul\\' : \\'summer\\', \\'aug\\' : \\'summer\\', \\'sep\\' : \\'summer\\', \\'oct\\' : \\'autumn\\', \\'nov\\' : \\'autumn\\'}\\ndf = df.replace({\\'month\\' : season_dict})\\ndf = df.rename(columns = {\\'month\\' : \\'season\\'})\\nsns.countplot(df[\\'season\\']) plt.title(\\'Count plot of seasons\\') Data Preprocessing Days sns.countplot(df[\\'day\\']) plt.title(\\'Count plot of days\\')\\ndf[\\'day\\'] = ((df[\\'day\\'] \\'sun\\') | (df[\\'day\\'] \\'sat\\'))\\ndf = df.rename(columns = {\\'day\\' : \\'is_weekend\\'}) sns.countplot(df[\\'is_weekend\\'])\\nplt.title(\\'Count plot of weekend vs weekda\\nHistogram\\nfig, ax = plt.subplots(11, figsize = (7,25)) index = 0\\nunused_column = [\\'X\\', \\'Y\\', \\'FFMC\\', \\'DMC\\', \\'DC\\', \\'ISI\\', \\'temp\\', \\'RH\\', \\'wind\\', \\'rain\\', \\'area\\'] for column in unused_column:\\nif column != \\'is_summer\\': ax[index].hist(df[column]) ax[index].title.set_text(\\'histogram of \\' + column) index += 1\\nplt.tight_layout()\\nExperiment 1: Base Model feature selection features_coba = df.drop([\\'labels\\',\\'rain\\',\\'day_encoded\\'], axis = 1) labels_coba = df[\\'labels\\'].values.reshape(-1, 1)\\nX_train_coba, X_test_coba, y_train_coba, y_test_coba = train_test_split(features_coba,\\nlabels_coba, test_size = 0.2,\\nrandom_state = 42) from sklearn.preprocessing import MinMaxScaler\\nsc_features = MinMaxScaler()\\nX_test = sc_features.fit_transform(X_test) X_train = sc_features.transform(X_train)\\nX_test_coba = pd.DataFrame(X_test_coba, columns = features_coba.columns) X_train_coba = pd.DataFrame(X_train_coba, columns = features_coba.columns) y_test_coba = pd.DataFrame(y_test_coba, columns = [\\'labels\\'])\\ny_train_coba = pd.DataFrame(y_train_coba, columns = [\\'labels\\']) model_coba = Sequential()\\ninput layer + 1st hidden layer\\nmodel_coba.add(Dense(6, input_dim=11, activation=\\'relu\\'))\\n2nd hidden layer model_coba.add(Dense(6, activation=\\'relu\\')) output layer\\nmodel_coba.add(Dense(6, activation=\\'sigmoid\\')) model_coba.add(Dropout(0.2)) model_coba.add(Dense(1, activation = \\'relu\\'))\\nplot_model(model_coba, to_file=\\'model_plot.png\\', show_shapes=True, show_layer_names=True)\\nCompile Model\\nmodel_coba.compile(optimizer = \\'adam\\', metrics=[\\'accuracy\\'], loss = \\'binary_crossentropy\\')\\nTrain Model\\nhistory_coba = model_coba.fit(X_train_coba, y_train_coba, validation_data = (X_test_coba, y_test_coba), batch_size = 10, epochs = 100)\\n, trainacc_coba = model_coba.evaluate(X_train_coba, y_train_coba, verbose=0)\\n, validacc_coba = model_coba.evaluate(X_test_coba, y_test_coba, verbose=0) print(\\'Train: %.3f, Valid: %.3f\\' % (train_acc_coba, valid_acc_coba))\\n# Accuracy Curves\\nplt.figure(figsize=[8,5]) plt.plot(history_coba.history[\\'accuracy\\'], label=\\'Train\\') plt.plot(history_coba.history[\\'val_accuracy\\'], label=\\'Valid\\') plt.legend()\\nplt.xlabel(\\'Epochs\\', fontsize=16) plt.ylabel(\\'Accuracy\\', fontsize=16)\\nplt.title(\\'Accuracy Curves Epoch 100 Batch Size 10\\', fontsize=16) plt.show()\\nExperiment -2 Model with Batch sizes [4, 6, 10, 16, 32, 64, 128, 260]\\ndef fit_model(X_train, y_train, X_test, y_test, n_batch):\\nmodel = Sequential()\\nmodel.add(Dense(6, input_dim=13, activation=\\'relu\\')) model.add(Dense(6, activation=\\'relu\\')) model.add(Dense(6, activation=\\'sigmoid\\')) model.add(Dropout(0.2))\\nmodel.add(Dense(1, activation = \\'relu\\'))\\nmodel.compile(optimizer = \\'adam\\',metrics=[\\'accuracy\\'], loss = \\'binary_crossentropy\\')\\nhistory = model.fit(X_train, y_train, validation_data=(X_test, y_test), epochs=100, verbose=0,batch_size=n_batch)\\nplt.plot(history.history[\\'accuracy\\'], label=\\'train\\') plt.plot(history.history[\\'val_accuracy\\'], label=\\'test\\') plt.title(\\'batch=\\'+str(n_batch))\\nplt.legend()\\nbatch_sizes = [4, 6, 10, 16, 32, 64, 128, 260]\\nplt.figure(figsize=(10,15))\\nfor i in range(len(batch_sizes)):\\nplot_no = 420 + (i+1) plt.subplot(plot_no)\\nfit_model(X_train, y_train, X_test, y_test, batch_sizes[i]) plt.show()'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "95fbf37a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#close"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "72d12f48",
   "metadata": {},
   "outputs": [],
   "source": [
    "x.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "25852b46",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\ufeffCLUSTERED FOREST FIRE PREDICTION USING DEEP LEARNING WITH EFFECTIVE SCALING AND THRESHOLD BASED FEATURE SELECTION\\n'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.readline()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "2f98f351",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['\\ufeffCLUSTERED FOREST FIRE PREDICTION USING DEEP LEARNING WITH EFFECTIVE SCALING AND THRESHOLD BASED FEATURE SELECTION\\n',\n",
       " 'A PROJECT REPORT\\n',\n",
       " 'Submitted by\\n',\n",
       " 'RAMANNAGARI CHAKRAVARDHAN [RA2111003010767]\\n',\n",
       " '       PERVATHANENI LIKITH CHOWDARY [RA2111003010789]\\n',\n",
       " 'Under the Guidance of\\n',\n",
       " 'Dr. M. SENTHIL RAJA\\n',\n",
       " 'Assistant Professor, Department of Computing Technologies\\n',\n",
       " '\\n',\n",
       " '\\n',\n",
       " 'in partial fulfillment of the requirements for the degree of\\n',\n",
       " '\\n',\n",
       " '\\n',\n",
       " 'BACHELOR OF TECHNOLOGY\\n',\n",
       " 'in\\n',\n",
       " 'COMPUTER SCIENCE AND ENGINEERING\\n',\n",
       " '\\n',\n",
       " '\\n',\n",
       " '\\n',\n",
       " '\\n',\n",
       " ' Logo, company name  Description automatically generated \\n',\n",
       " '\\n',\n",
       " '\\n',\n",
       " '\\n',\n",
       " 'DEPARTMENT OF COMPUTING TECHNOLOGIES COLLEGE OF ENGINEERING AND TECHNOLOGY SRM INSTITUTE OF SCIENCE AND TECHNOLOGY KATTANKULATHUR– 603 203\\n',\n",
       " 'MAY 2025\\n',\n",
       " ' Logo \\n',\n",
       " '\\n',\n",
       " '\\n',\n",
       " '\\n',\n",
       " 'SRM INSTITUTE OF SCIENCE AND TECHNOLOGY KATTANKULATHUR–603 203\\n',\n",
       " 'BONAFIDE CERTIFICATE\\n',\n",
       " '\\n',\n",
       " '\\n',\n",
       " 'Certified that 18CSP109L / I8CSP111L project report titled “CLUSTERED FOREST FIRE PREDICTION USING DEEPLEARNING WITH EFFECTIVE THRESHOLD BASED FEATURE SELECTION” is the Bonafide work of RAMANNAGARI CHAKRAVARDHAN [RA2111003010767] and PERVATHANENI LIKITH CHOWDARY [RA2111003010767] who carried out the project work under my supervision. Certified further, that to the best of my knowledge the work reported here in does not form part of any other thesis or dissertation on the basis of which a degree or award was conferred on an earlier occasion for this or any other candidate.\\n',\n",
       " '\\n',\n",
       " '\\n',\n",
       " '\\n',\n",
       " '\\n',\n",
       " '\\n',\n",
       " '\\n',\n",
       " 'Dr. M.SENTHIL.RAJA\\n',\n",
       " 'SUPERVISOR\\n',\n",
       " 'Assistant Professor\\n',\n",
       " 'Department of Computing Technologies\\n',\n",
       " '________________\\n',\n",
       " 'Dr. B.SIVAKUMAR\\n',\n",
       " 'PANEL HEAD\\n',\n",
       " 'Associate Professor\\n',\n",
       " 'Department of Computing Technologies\\n',\n",
       " '\\n',\n",
       " '\\n',\n",
       " '\\n',\n",
       " '\\n',\n",
       " '\\n',\n",
       " '\\n',\n",
       " 'Dr. G.NIRANJANA\\n',\n",
       " 'HEAD OF THE DEPARTMENT\\n',\n",
       " 'Department of Computing Technologies\\n',\n",
       " '\\n',\n",
       " '\\n',\n",
       " '\\n',\n",
       " '\\n',\n",
       " '\\n',\n",
       " '\\n',\n",
       " 'INTERNAL EXAMINER        EXTERNAL EXAMINER\\n',\n",
       " '\\n',\n",
       " '\\n',\n",
       " 'Department of Computing Technologies Logo \\n',\n",
       " 'SRM Institute of Science and Technology Own Work Declaration Form\\n',\n",
       " 'Degree/Course        : B.Tech in Computer Science and Engineering Student Names        : CHAKRAVARDHAN,LIKITH CHOWDARY\\n',\n",
       " 'Registration Number: RA2111003010767,RA2111003010789\\n',\n",
       " '\\n',\n",
       " '\\n',\n",
       " 'Title of Work        : Clustered Forest Fire Prediction using deep learning with effective scaling and threshold-based feature selection.\\n',\n",
       " 'I/We here by certify that this assessment compiles with the University’s Rules and Regulations relating to Academic misconduct and plagiarism, as listed in the University Website, Regulations, and the Education Committee guidelines.\\n',\n",
       " 'I / We confirm that all the work contained in this assessment is our own except where indicated, and that we have met the following conditions:\\n',\n",
       " '* Clearly references / listed all sources as appropriate\\n',\n",
       " '* Referenced and put in inverted commas all quoted text(from books, web,etc.)\\n',\n",
       " '* Given the sources of all pictures, data etc that are not my own.\\n',\n",
       " '* Not made any use of the report(s) or essay(s) of any other student(s) either past or present\\n',\n",
       " '* Acknowledged in appropriate places any help that I have received from others(e.g fellow students, technicians, statisticians, external sources)\\n',\n",
       " '* Compiled with any other plagiarism criteria specified in the Course hand book / University website\\n',\n",
       " 'I understand that any false claim for this work will be penalized in accordance with the University policies and regulations.\\n',\n",
       " '\\n',\n",
       " '\\n',\n",
       " 'DECLARATION:\\n',\n",
       " '\\tI am aware of and understand the University’s policy on Academic misconduct and plagiarism and I certify that this assessment is my / our own work, except where indicated by referring, and that I have followed the good academic practices noted above.\\n',\n",
       " 'chakravardhan Signature: likith chowdary Signature: Date:\\n',\n",
       " '\\tIf you are working in a group, please write your registration numbers and sign with the date for every student in your group.\\n',\n",
       " '\\t\\n',\n",
       " '\\n',\n",
       " 'ACKNOWLEDGEMENT              \\n',\n",
       " 'We express our humble gratitude to Dr. C. Muthamizhchelvan, Vice-Chancellor, SRM Institute of Science and Technology, for the facilities extended for the project work and his continued support.\\n',\n",
       " 'We extend our sincere thanks to Dr.LEENUS JESU MARTIN, Dean-CET, SRM Institute of Science and Technology, for his invaluable support. \\n',\n",
       " ' We wish to thank Dr. Revathi Venkataraman, Professor and Chairperson, School of Computing, SRM Institute of Science and Technology, for her support throughout the project work. \\n',\n",
       " ' We encompass our sincere thanks to, Dr. M. Pushpalatha, Professor and Associate Chairperson - CS, School of Computing, for their invaluable support.\\n',\n",
       " ' We are incredibly grateful to our Head of the DEPARTMENT, Dr. G Niranjana, Professor & Head, SRM Institute of Science and Technology, for her suggestions and encouragement at all the stages of the project work. \\n',\n",
       " 'We want to convey our thanks to our Project Coordinators ,M.Senthil Raja, B.Sivakumar J Nithyashri DEPARTMENT of COMPUTING TECHNOLOGIES, SRM Institute of Science and Technology, for their inputs during the project reviews and support.\\n',\n",
       " 'Our inexpressible respect and thanks to our guide, Dr. M.Senthil Raja , DEPARTMENT of COMPUTING TECHNOLOGIES, SRM Institute of Science and Technology, for providing us with an opportunity to pursue our project under his / her mentorship. He / She provided us with the freedom and support to explore the research topics of our interest. His / Her passion for solving problems and making a difference in the world has always been inspiring. \\n',\n",
       " 'We sincerely thank all the staff members of COMPUTING TECHNOLOGIES, School of Computing, S.R.M Institute of Science and Technology, for their help during our project. Finally, we would like to thank our parents, family members, and friends for their unconditional love, constant support and encouragement.\\n',\n",
       " '\\n',\n",
       " '\\n',\n",
       " '                                         RAMANNAGARICHAKRAVARDHAN[RA2111003010767]\\n',\n",
       " '\\n',\n",
       " '\\n',\n",
       " '                                                   PERVATHANENILIKITHCHOWDARY[RA2111003010789]\\n',\n",
       " 'ABSTRACT\\n',\n",
       " '\\n',\n",
       " '\\n',\n",
       " \"Ecosystems that are prone to wildfires as well as those that are not have been devastated. We can anticipate a reorganization in many ecosystems around the world, possibly shifting towards a rise in federal species, as a result of the components of the fire regime changing due to global change (including the frequency, intensity, and timing of forest fires). Fire is one of the most common factors in deforestation and species extinction on a global scale. It's not always possible to contain a fire and get to the wooded region in time. As a result, there is frequently a lot of destruction. As a result, it is crucial to anticipate fires and act quickly when they occur. The natural vegetation and forest life suffer from a number of damaging repercussions as a result of forest fires. Everyone's lives and the environment are significantly impacted by forest fires. Several ecosystems, including grasslands and temperate forests, depend heavily on forest fires. Optimizing the situation will be made easier with the capacity to anticipate the potential forest fire's location. The major goal of this study is to demonstrate how a machine learning and deep learning algorithm may be used to estimate the risk of forest fires using weather data. The major goal is to foretell the likelihood of a forest fire and its intensity under particular climatic circumstances in a specific place. Assessing the likelihood of forest wildfires can help in forest wildfire prevention, management, and supervision because forest wildfires are a significant disturbance element in forest ecosystems.\\n\",\n",
       " 'TABLE OF CONTENTS\\n',\n",
       " '\\n',\n",
       " '\\n',\n",
       " '\\n',\n",
       " '\\n',\n",
       " 'ABSTRACT\\n',\n",
       " '\\tvi\\n',\n",
       " '\\t\\n',\n",
       " '\\n',\n",
       " '\\tLIST OF TABLES\\n',\n",
       " '\\tix\\n',\n",
       " '\\t\\n',\n",
       " '\\n',\n",
       " '\\tLIST OF FIGURES\\n',\n",
       " '\\tx\\n',\n",
       " '\\t\\n',\n",
       " '\\n',\n",
       " '\\tLIST OF EQUATIONS\\n',\n",
       " '\\txi\\n',\n",
       " '\\t\\n',\n",
       " '\\n',\n",
       " '\\tABBREVATIONS\\n',\n",
       " '\\txii\\n',\n",
       " '\\t1.\\n',\n",
       " '\\tINTRODUCTION\\n',\n",
       " '\\t1\\n',\n",
       " '\\t\\n',\n",
       " '\\n',\n",
       " '\\t1.1\\n',\n",
       " '\\tOverview\\n',\n",
       " '\\t1\\n',\n",
       " '\\t\\n',\n",
       " '\\n',\n",
       " '\\t1.2\\n',\n",
       " '\\tObjective\\n',\n",
       " '\\t1\\n',\n",
       " '\\t\\n',\n",
       " '\\n',\n",
       " '\\t1.3\\n',\n",
       " '\\tContribution\\n',\n",
       " '\\t2\\n',\n",
       " '\\t\\n',\n",
       " '\\n',\n",
       " '\\t1.4\\n',\n",
       " '\\tHardware Requirements\\n',\n",
       " '\\t2\\n',\n",
       " '\\t\\n',\n",
       " '\\n',\n",
       " '\\t1.5\\n',\n",
       " '\\tSoftware Requirements\\n',\n",
       " '\\t3\\n',\n",
       " '\\t2\\n',\n",
       " '\\tLITERATURE SURVEY\\n',\n",
       " '\\t4\\n',\n",
       " '\\t3\\n',\n",
       " '\\tSYSTEM ARCHITECTURE AND MODULES\\n',\n",
       " '\\t11\\n',\n",
       " '\\t\\n',\n",
       " '\\n',\n",
       " '\\t3.1\\n',\n",
       " '\\tArchitecture Diagram\\n',\n",
       " '\\t11\\n',\n",
       " '\\t\\n',\n",
       " '\\n',\n",
       " '\\t3.2\\n',\n",
       " '\\tData Flow Diagram\\n',\n",
       " '\\t12\\n',\n",
       " '\\t\\n',\n",
       " '\\n',\n",
       " '\\t3.3\\n',\n",
       " '\\tModules\\n',\n",
       " '\\t13\\n',\n",
       " '\\t\\n',\n",
       " '\\n',\n",
       " '\\t\\n',\n",
       " '\\n',\n",
       " '\\t3.3.1  Clustering\\n',\n",
       " '\\t13\\n',\n",
       " '\\t\\n',\n",
       " '\\n',\n",
       " '\\t\\n',\n",
       " '\\n',\n",
       " '\\t3.3.2        Feature Scaling\\n',\n",
       " '\\t13\\n',\n",
       " '\\t\\n',\n",
       " '\\n',\n",
       " '\\t\\n',\n",
       " '\\n',\n",
       " '\\t3.3.3        Feature Selection\\n',\n",
       " '\\t14\\n',\n",
       " '\\t\\n',\n",
       " '\\n',\n",
       " '\\t\\n',\n",
       " '\\n',\n",
       " '\\t3.3.4        Neural Network\\n',\n",
       " '\\t14\\n',\n",
       " '\\t4\\n',\n",
       " '\\tMETHODOLOGY\\n',\n",
       " '\\t16\\n',\n",
       " '\\t\\n',\n",
       " '\\n',\n",
       " '\\t4.1\\n',\n",
       " '\\tData Preprocessing\\n',\n",
       " '\\t16\\n',\n",
       " '\\t\\n',\n",
       " '\\n',\n",
       " '\\t\\n',\n",
       " '\\n',\n",
       " '\\t4.1.1  Clustering\\n',\n",
       " '\\t16\\n',\n",
       " '\\t\\n',\n",
       " '\\n',\n",
       " '\\t\\n',\n",
       " '\\n',\n",
       " '\\t4.1.2  Feature Scaling\\n',\n",
       " '\\t18\\n',\n",
       " '\\t\\n',\n",
       " '\\n',\n",
       " '\\t\\n',\n",
       " '\\n',\n",
       " '\\t4.1.3  Data Exploration\\n',\n",
       " '\\t21\\n',\n",
       " '\\t\\n',\n",
       " '\\n',\n",
       " '\\t4.2\\n',\n",
       " '\\tFeature Selection\\n',\n",
       " '\\t22\\n',\n",
       " '\\t\\n',\n",
       " '\\n',\n",
       " '\\t4.3\\n',\n",
       " '\\tDeep Neural Network\\n',\n",
       " '\\t23\\n',\n",
       " '\\t5\\n',\n",
       " '\\tRESULTS AND DISCUSSION\\n',\n",
       " '\\t26\\n',\n",
       " '\\t\\n',\n",
       " '\\n',\n",
       " '\\t5.1\\n',\n",
       " '\\tExperimental Setup\\n',\n",
       " '\\t26\\n',\n",
       " '\\t\\n',\n",
       " '\\n',\n",
       " '\\t5.2\\n',\n",
       " '\\tDataset and Exploration\\n',\n",
       " '\\t26\\n',\n",
       " '\\t\\n',\n",
       " '\\n',\n",
       " '\\t5.3\\n',\n",
       " '\\tExperimental Discussion\\n',\n",
       " '\\t38\\n',\n",
       " '\\t\\n',\n",
       " '\\n',\n",
       " '\\t\\n',\n",
       " '\\n',\n",
       " '\\t5.3.1  Experiment-1 Clustering\\n',\n",
       " '\\t38\\n',\n",
       " '\\t\\n',\n",
       " '\\n',\n",
       " '\\n',\n",
       " '\\n',\n",
       " '\\n',\n",
       " '\\n',\n",
       " '\\t\\n',\n",
       " '\\n',\n",
       " '\\t5.3.2  Experiment-2 Feature Scaling\\n',\n",
       " '\\t39\\n',\n",
       " '\\t\\n',\n",
       " '\\n',\n",
       " '\\t\\n',\n",
       " '\\n',\n",
       " '\\t5.3.3  Experiment-3 Feature Selection\\n',\n",
       " '\\t40\\n',\n",
       " '\\t\\n',\n",
       " '\\n',\n",
       " '\\t\\n',\n",
       " '\\n',\n",
       " '\\t5.3.4  Experiment-4 Classification\\n',\n",
       " '\\t43\\n',\n",
       " '\\t\\n',\n",
       " '\\n',\n",
       " '\\t5.5\\n',\n",
       " '\\tComparative Study\\n',\n",
       " '\\t49\\n',\n",
       " '\\t6\\n',\n",
       " '\\tCONCLUSION AND FUTURE SCOPE\\n',\n",
       " '\\t50\\n',\n",
       " '\\t\\n',\n",
       " '\\n',\n",
       " '\\tREFERENCES\\n',\n",
       " '\\t51\\n',\n",
       " '\\t\\n',\n",
       " '\\n',\n",
       " '\\tAPPENDIX\\n',\n",
       " '\\t55\\n',\n",
       " '\\t\\n',\n",
       " '\\n',\n",
       " '\\tPLAGIARISM REPORT\\n',\n",
       " '\\t\\n',\n",
       " '\\n',\n",
       " '\\t\\n',\n",
       " '\\n',\n",
       " '\\tJOURNAL PUBLICATION\\n',\n",
       " '\\t\\n',\n",
       " '\\n',\n",
       " '\\t\\n',\n",
       " '\\n',\n",
       " 'LIST OF TABLES\\n',\n",
       " '\\n',\n",
       " '\\n',\n",
       " '\\n',\n",
       " '\\n',\n",
       " '\\n',\n",
       " '\\n',\n",
       " '1.1\\n',\n",
       " '\\tHardware Requirements…………………………………….……………..\\n',\n",
       " '\\t5\\n',\n",
       " '\\t1.2\\n',\n",
       " '\\tSoftware Requirments……….……………………………….……………\\n',\n",
       " '\\t40\\n',\n",
       " '\\t5.1\\n',\n",
       " '\\tComparative Study of Montesinho Natural park dataset….....…………….\\n',\n",
       " '\\t48\\n',\n",
       " '\\t5.2\\n',\n",
       " '\\tComparative Study of Algerian forest fires dataset………..………………\\n',\n",
       " '\\t48\\n',\n",
       " '\\t\\n',\n",
       " '\\n',\n",
       " '\\n',\n",
       " '\\n',\n",
       " '\\n',\n",
       " '\\n',\n",
       " '\\n',\n",
       " '\\n',\n",
       " '\\n',\n",
       " '\\n',\n",
       " '\\n',\n",
       " '\\n',\n",
       " '\\n',\n",
       " '\\n',\n",
       " '\\n',\n",
       " '\\n',\n",
       " '\\n',\n",
       " '\\n',\n",
       " '\\n',\n",
       " '\\n',\n",
       " '\\n',\n",
       " '\\n',\n",
       " '\\n',\n",
       " '\\n',\n",
       " '\\n',\n",
       " '\\n',\n",
       " '\\n',\n",
       " '\\n',\n",
       " '\\n',\n",
       " '\\n',\n",
       " '\\n',\n",
       " '\\n',\n",
       " '\\n',\n",
       " '\\n',\n",
       " '\\n',\n",
       " '\\n',\n",
       " '\\n',\n",
       " '\\n',\n",
       " '\\n',\n",
       " '\\n',\n",
       " '\\n',\n",
       " '\\n',\n",
       " '\\n',\n",
       " '\\n',\n",
       " '\\n',\n",
       " '\\n',\n",
       " '\\n',\n",
       " '\\n',\n",
       " '\\n',\n",
       " '\\n',\n",
       " '\\n',\n",
       " '\\n',\n",
       " '\\n',\n",
       " '\\n',\n",
       " '\\n',\n",
       " '\\n',\n",
       " '\\n',\n",
       " '\\n',\n",
       " '\\n',\n",
       " '\\n',\n",
       " '\\n',\n",
       " '\\n',\n",
       " '\\n',\n",
       " '\\n',\n",
       " '\\n',\n",
       " '\\n',\n",
       " '\\n',\n",
       " '\\n',\n",
       " '‘\\n',\n",
       " 'LIST OF FIGURES\\n',\n",
       " '\\n',\n",
       " '\\n',\n",
       " '\\n',\n",
       " '\\n',\n",
       " '3.1\\n',\n",
       " '\\tArchitectural Design of Forest fire prediction……. ………………………………\\n',\n",
       " '\\t11\\n',\n",
       " '\\t3.2\\n',\n",
       " '\\tData Flow Diagram………………………..............................................................\\n',\n",
       " '\\t12\\n',\n",
       " '\\t4.1\\n',\n",
       " '\\tArchitecture Diagram of the Neural Network…..…………………………...…….\\n',\n",
       " '\\t25\\n',\n",
       " '\\t5.1\\n',\n",
       " '\\tFire Weather Observation….....................................................................................\\n',\n",
       " '\\t27\\n',\n",
       " '\\t5.2\\n',\n",
       " '\\tMontensinho Natural Park Dataset Sample..............................................................\\n',\n",
       " '\\t27\\n',\n",
       " '\\t5.3\\n',\n",
       " '\\tInformation of Montensinho Natural Park Dataset Sample……………………….\\n',\n",
       " '\\t28\\n',\n",
       " '\\t5.4\\n',\n",
       " '\\tHistogram of Montesinho Natural Park Dataset…………………………………..\\n',\n",
       " '\\t29\\n',\n",
       " '\\t5.5\\n',\n",
       " '\\tHistogram of rain and area………………………………………………………...\\n',\n",
       " '\\t30\\n',\n",
       " '\\t5.6\\n',\n",
       " '\\tDescriptive Statistics of Montesinho Natural Dataset sample…………………….\\n',\n",
       " '\\t30\\n',\n",
       " '\\t5.7\\n',\n",
       " '\\tCorrelation Analysis of Montesinho Natural Park Dataset………………………..\\n',\n",
       " '\\t31\\n',\n",
       " '\\t5.8\\n',\n",
       " '\\tAlgeriam Forest Fires Dataset Sample…………………………………………….\\n',\n",
       " '\\t32\\n',\n",
       " '\\t5.9\\n',\n",
       " '\\tInformation of Algerian Forest Fires Dataset Sample…………………………….\\n',\n",
       " '\\t33\\n',\n",
       " '\\t5.10\\n',\n",
       " '\\tHistogram of Algerian Forest Fires Dataset………………………………………\\n',\n",
       " '\\t34\\n',\n",
       " '\\t5.11\\n',\n",
       " '\\tDescriptive Statistics of Algerian Forest Fires Dataset……………………………\\n',\n",
       " '\\t35\\n',\n",
       " '\\t5.12\\n',\n",
       " '\\tCorrelation Analysis of Algerian Forest Fires Dataset…………………………….\\n',\n",
       " '\\t36\\n',\n",
       " '\\t5.13\\n',\n",
       " '\\tElbow Method for Montesinho Natural Park dataset Sample……………………..\\n',\n",
       " '\\t37\\n',\n",
       " '\\t5.14\\n',\n",
       " '\\tComparison of data point cluster between 2 and 9………………………………..\\n',\n",
       " '\\t38\\n',\n",
       " '\\t5.15\\n',\n",
       " '\\tClustered Dataset of Montesinho Natural Park dataset……………………………\\n',\n",
       " '\\t38\\n',\n",
       " '\\t5.16\\n',\n",
       " '\\tFeature Scaling of Montesinho Natural Park Dataset……………………………..\\n',\n",
       " '\\t39\\n',\n",
       " '\\t5.17\\n',\n",
       " '\\tFeature Scaling of Algerian Forest Fires Dataset………………………………….\\n',\n",
       " '\\t39\\n',\n",
       " '\\t5.18\\n',\n",
       " '\\tAccuracy Table using Logistic Regression for montesinho natural park dataset….\\n',\n",
       " '\\t40\\n',\n",
       " '\\t5.19\\n',\n",
       " '\\tAccuracy Table using Logistic Regression for Algerian Forest Fires dataset…….\\n',\n",
       " '\\t40\\n',\n",
       " '\\t5.20\\n',\n",
       " '\\tAccuracy Table using KNN for montesinho natural park dataset…………………\\n',\n",
       " '\\t41\\n',\n",
       " '\\t5.21\\n',\n",
       " '\\tAccuracy Table using KNN for Algerian Forest Fires dataset…………………….\\n',\n",
       " '\\t41\\n',\n",
       " '\\t5.22\\n',\n",
       " '\\tAccuracy Table using SVM for montesinho natural park dataset…………………\\n',\n",
       " '\\t41\\n',\n",
       " '\\t5.23\\n',\n",
       " '\\tAccuracy Table using SVM for Algerian Forest Fires dataset…………………….\\n',\n",
       " '\\t42\\n',\n",
       " '\\t5.24\\n',\n",
       " '\\tAccuracy curve of Montesinho Natural Park Dataset……………………………..\\n',\n",
       " '\\t43\\n',\n",
       " '\\t5.25\\n',\n",
       " '\\tBatch Size Accuracy curves for the Montesinho Natural Park Dataset…………...\\n',\n",
       " '\\t44\\n',\n",
       " '\\t5.26\\n',\n",
       " '\\tAccuracy curve of the Algerian Forest Fires Dataset……………………………...\\n',\n",
       " '\\t45\\n',\n",
       " '\\t5.27\\n',\n",
       " '\\tBatch Size Accuracy curves for the Algerian Forest Fires Dataset……………….\\n',\n",
       " '\\t46\\n',\n",
       " '\\t\\n',\n",
       " '\\n',\n",
       " 'LIST OF EQUATIONS\\n',\n",
       " '\\n',\n",
       " '\\n',\n",
       " '4.1\\n',\n",
       " '\\tDistance to each centroid and data points………………………………………..\\n',\n",
       " '\\t16\\n',\n",
       " '\\t4.2\\n',\n",
       " '\\tCentre of the data points assigned to each centroid………………………………\\n',\n",
       " '\\t16\\n',\n",
       " '\\t4.3\\n',\n",
       " '\\tWithin cluster sum of squares……………………………………………………\\n',\n",
       " '\\t17\\n',\n",
       " '\\t4.4\\n',\n",
       " '\\tSilhouette Score………………………………………………………………….\\n',\n",
       " '\\t17\\n',\n",
       " '\\t4.5\\n',\n",
       " '\\tStandardization equation…………………………………………………………\\n',\n",
       " '\\t18\\n',\n",
       " '\\t4.6\\n',\n",
       " '\\tMin Max Scaling Equation………………………………………………………\\n',\n",
       " '\\t18\\n',\n",
       " '\\t4.7\\n',\n",
       " '\\tMax-Absolute Scaling Equation…………………………………………………\\n',\n",
       " '\\t19\\n',\n",
       " '\\t4.8\\n',\n",
       " '\\tRobust Scaling Equation…………………………………………………………\\n',\n",
       " '\\t19\\n',\n",
       " '\\t4.9\\n',\n",
       " '\\tBox-Cox Transformation…………………………………………………………\\n',\n",
       " '\\t19\\n',\n",
       " '\\t4.10\\n',\n",
       " '\\tYeo-Johnson Transformation for x>=0…………………………………………..\\n',\n",
       " '\\t20\\n',\n",
       " '\\t4.11\\n',\n",
       " '\\tYeo-Johnson Transformation for x<0……………………………………………\\n',\n",
       " '\\t20\\n',\n",
       " '\\t4.12\\n',\n",
       " '\\tRoot Mean Squared Error………………………………………………………..\\n',\n",
       " '\\t20\\n',\n",
       " '\\t4.13\\n',\n",
       " '\\tChi Square Test Formula…………………………………………………………\\n',\n",
       " '\\t22\\n',\n",
       " '\\t4.14\\n',\n",
       " '\\tPearson Correlation Formula…………………………………………………….\\n',\n",
       " '\\t23\\n',\n",
       " '\\t4.15\\n',\n",
       " '\\tWeighted sum of the inputs………………………………………………………\\n',\n",
       " '\\t24\\n',\n",
       " '\\t\\n',\n",
       " '\\n',\n",
       " 'ABBREVIATIONS\\n',\n",
       " '\\n',\n",
       " '\\n',\n",
       " '\\n',\n",
       " '\\n',\n",
       " 'ANN\\n',\n",
       " '\\tArtificial Neural Network\\n',\n",
       " '\\tBUI\\n',\n",
       " '\\tBuildup Index\\n',\n",
       " '\\tCNN\\n',\n",
       " '\\tConvolution Neural Network\\n',\n",
       " '\\tDC\\n',\n",
       " '\\tDrought Code\\n',\n",
       " '\\tDMC\\n',\n",
       " '\\tDuff Moisture Code\\n',\n",
       " '\\tFFMC\\n',\n",
       " '\\tFine Fuel Moisture Code\\n',\n",
       " '\\tFWI\\n',\n",
       " '\\tFire Weather Index\\n',\n",
       " '\\tISI\\n',\n",
       " '\\tInitial Speed Index\\n',\n",
       " '\\tKNN\\n',\n",
       " '\\tK Nearest Linea Neighbors\\n',\n",
       " '\\tReLU\\n',\n",
       " '\\tRectified Linear Unit\\n',\n",
       " '\\tRMSE\\n',\n",
       " '\\tRoot Mean Square Error\\n',\n",
       " '\\tSSD\\n',\n",
       " '\\tSum of the Squared Distances\\n',\n",
       " '\\tSVM\\n',\n",
       " '\\tSupport Vector Machine\\n',\n",
       " '\\tWCSS\\n',\n",
       " '\\tWithin-cluster Sum of Squares\\n',\n",
       " '\\t\\n',\n",
       " '\\n',\n",
       " 'CHAPTER 1 INTRODUCTION\\n',\n",
       " '   1. Overview\\n',\n",
       " \"Forest fires have recently risen to the top of the list of natural disasters that have been known to wipe off hectares of forest. They have a huge influence on the environment, ecological systems, and the ecology of a region by endangering not just the forest materials but also the entire ecosystem. When it fails to rain for several months in the summer, it is full of dwellings that are prone to catching fire from a single spark and dry, senescent leaven. The main contributing elements for fire in the forest is the fact that global warming is the main cause of high temperature in Earth's surface. Some additional reasons also include rain, flooding, and ignorance of normal citizens.\\n\",\n",
       " 'Fire-related deforestation can have a variety of bad effects on human civilization. In order to effectively combat forest fires, early discovery is crucial. No later than six minutes after fire begins, the forest fire department would be alerted to the danger., according to several characteristics of forest fires, in order to put out the fire without permanently harming the forest.Prediction of fire in forest takes into account of atmospheric conditions, environmental variables, the atmosphere, the dryness of flameproof materials, the forms of flammable materials, and ignition sources when assessing and forecasting the combustion hazards of combustible objects in the wood.\\n',\n",
       " '   2. Objective\\n',\n",
       " 'Recent years have seen an increase in worry about the rise of fire in forest, which leads to catastrophic repercussions affect both human beings and nature. For forest fires to be prevented and their effects reduced, forecasting is essential. Forest fires may be accurately and quickly predicted, which can assist authorities in taking the right actions to prevent their spread and guarantee public and environmental safety. Hence, as our goal is to develop a deep learning project for precise fire prediction, clustering can be utilized to gather related data points, which can aid in discovering patterns and trends that can help with fire prediction. In order to ensure that various features are given equal weight during analysis and prediction, feature scaling is crucial. The most important features for forest fire prediction can be found with the aid of feature selection, which can increase accuracy and reduce noise. Accuracy of predicting forest fires can be increased by using deep learning, which can assist in recognising complicated patterns and correlations between various information.\\n',\n",
       " 'As a result, the accuracy of forest fire prediction can be greatly increased by combining clustering, feature scaling, feature selection, and deep learning approaches. This can decrease the\\n',\n",
       " 'harm that forest fires cause to the environment and to people. Authorities can take the required actions to stop the spread of forest fires and safeguard both the environment and the populace by properly forecasting their occurrence.\\n',\n",
       " '   3. Contribution\\n',\n",
       " 'In this research, we propose a approach to predicting forest fires using a neural network model that incorporates effective feature scaling and threshold-based feature selection. Our work contributes in the following aspects:\\n',\n",
       " '\\n',\n",
       " '\\n',\n",
       " 'Firstly, we introduced a clustered approach to forest fire prediction, which enables us to better capture the spatial distribution of fire incidents within a forest. By clustering the forest into different zones, we can train our neural network model to learn the unique features and patterns associated with each zone. This improves the accuracy of our predictions, as our model is better equipped to handle the variability and heterogeneity present in different parts of the forest.\\n',\n",
       " '\\n',\n",
       " '\\n',\n",
       " \"Secondly, we applied effective feature scaling and threshold-based feature selection techniques to our model, which improved its performance significantly. By normalizing our input features and selecting only the most relevant ones based on a predetermined threshold, we reduced noise in our data. We also improved our model's accuracy and robustness. Overall, our proposed approach represents a significant contribution to forest fire prediction, as it leverages neural networks and advanced data pre-processing techniques to achieve more accurate and reliable predictions.\\n\",\n",
       " '\\n',\n",
       " '\\n',\n",
       " '   4. Hardware Requirements\\n',\n",
       " '\\n',\n",
       " '\\n',\n",
       " 'Specification\\n',\n",
       " '\\tValue\\n',\n",
       " '\\tDevice Required\\n',\n",
       " '\\tLaptop/Desktop\\n',\n",
       " '\\tOperating System\\n',\n",
       " '\\tWindows/Mac/Linux/Any\\n',\n",
       " '\\tMemory(RAM)\\n',\n",
       " '\\t4GB or more\\n',\n",
       " '\\tCPU\\n',\n",
       " '\\tGreater than 1.5GHz\\n',\n",
       " '\\tInternet\\n',\n",
       " '\\tLAN Connection, Wireless Connection(Wi-Fi)\\n',\n",
       " '\\tTable 1.1 Hardware Requirements\\n',\n",
       " '   5. Software Requirements\\n',\n",
       " '\\n',\n",
       " '\\n',\n",
       " 'SPECIFICACATION\\n',\n",
       " '\\tPACKAGE\\n',\n",
       " '\\tEditor\\n',\n",
       " '\\tGoogle Collab/Jupyter Notebook/Visual Studio\\n',\n",
       " '\\tFramework\\n',\n",
       " '\\tData-driven Testing\\n',\n",
       " '\\tDataset\\n',\n",
       " '\\tKaggle\\n',\n",
       " '\\tTable 1.2 Software Requirements\\n',\n",
       " 'CHAPTER 2 LITERATURE REVIEW\\n',\n",
       " 'The literature review is a crucial section of the report since it directs the path of the investigation. Setting a purpose for your investigation and generating a problem statement can be helpful. Additionally, it involves a methodical and exhaustive investigation of all published literature and other sources. Here are a few studies that are linked to this one:\\n',\n",
       " \"      * Jang Hyun Sunga, Young Ryub, and Kee-Won Seong created a DBN model with thhe help of Deep Learning which would help in Prediction of Occurrence of fire taking into account the climatic Condition and Drought Phase across the region to be able to forecast the likelihood of fire in a certain location. They chose DBN model because traditional regression models had limits in forecasting the characteristics of forest fires since They believed there to be a linear relationship between weather conditions and forest fires, but the DBN model can account for its nonlinearity. Meteorological parameters like drought indices were created to enhance DBN performance since the OFs in the spring have been increasing. SPI and SPEI were calculated during periods of 3, 6, and 9 months, and the relationship amongst both of these dryness parameters and OF. The efficacy of the model's prediction utilising the drought indicators was evaluated, and this relationship was tested using DBN.\\n\",\n",
       " \"Block-stacked was created with the help of an unsupervised network known as Restricted Boltzmann Machine, which makes up the DBN.A particular class of stochastic model called the RBM can be trained to learn a probability across its input data. The DBN training used the greedy level- wise training technique to address the gradient disappearance that most deep learning has experienced. Hidden levels of the RBM contain no connections other than those between layers, while the first two levels' interconnections are free of direction.They have benefits like initializing the network and preventing overfitting issues and poor local optima through unsupervised pre-training. As a result, the DBN model trains the weight in the lowest layer, which is close to the input layer.\\n\",\n",
       " 'The correlations with relevant variables were examined so as to understand the relationship among meteorological factors, the dryness phase, the ignition frequency of occurrence of fire in forest. Hence, it’s established that a significant link exists between OF & climate-related variables. Among the climate factors, the association between the OF and RH, WS was particularly strong because low humidity speeds up the burning of forest fires and causes vegetation to dry up. As the wind picks up, a forest fire spreads more quickly.\\n',\n",
       " '      * Pranati Rakhit, Sreshta Sarkar,Sambit Khan,PritamSaha,Sonali Bhatacharya,Sardar Islam,Nilarpan De, Sovik Pal in Forest Fire Prediction Using Machine Learning Algorithms estimates\\n',\n",
       " 'the level of threat of fire in a particular forest. The goal of the particular method is to anticipate the threat of fire in a particular forest by categorizing a given area as most susceptible, medium susceptible, low susceptible, or not susceptible to fire at all.To predict the risk zone as well as the medium, low, and no risk zones, they applied various machine learning classifiers. A comparison study has also been conducted to see which algorithm provides the highest level of accuracy for predicting forest fires. Using the algorithm, area under the curve was found out and integrated with confusion matrix to find out accuracy of each classifier so as to decide accordingly.\\n',\n",
       " \"This study analyzed four different classification model types, calculated each one's accuracy, and came to a point that Decision Tree is the most suitable method for their dataset since it properly classifies more data than the other classifiers. The decision tree has a good accuracy rate as well. The test dataset demonstrates that the decision tree achieves a higher categorization rate. The decision tree is the most effective classification for this prediction, according to this study's evaluation of four distinct types of classification models and computation of each one's accuracy. This conclusion was reached as a consequence of the study's findings and implications. Here, they made a prediction on the likelihood of a forest fire based on its severity under particular climatic circumstances in a particular place.\\n\",\n",
       " \"      * Priti T, Dr. Suvarna Kankaradi, Aiswarya Belagi, S Malagi, Aiswarya Sudii using Machine Learning methods focuses on various regression techniques, from there they took out the best one to predict fire. Random forest, Decision tree, and SVR are the regression techniques used for prediction. The Proposed System explains how they used the meteorological dataset to perform an exploratory analysis, pre-processing in which they attempted to eliminate outliers and convert classification data to numeric based data to make the dataset easier to interpret. Using those machine learning models to forecast likelihood of occurrence of fire and notify closest station. The steps in this study are as follows: Data collecting is completed, and then data pre-processing occurs so that a dataset may be created in a standard format. After data preparation, a suitable model from a range of regression techniques should be chosen based on the dataset. Model evaluation follows model implementation. Finally, predicting the data and each model's precision.\\n\",\n",
       " \"The gathered information is utilized to train the system and make predictions. They examined the relationship between temperature, humidity, precipitation, wind speed, and other factors to forecast forest fires. For models using random forest, svr, and decision tree, accuracy and MAE have been calculated. A regression curve's mean square error indicates how closely it resembles a set of points. This is accomplished by squaring length given between the points and the curve of regression. The squaring is necessary to eliminate any unfavorable indicators. It also provides many weights for\\n\",\n",
       " 'larger variations.\\n',\n",
       " '\\n',\n",
       " '\\n',\n",
       " \"The results of experiments are used to set a variable number of training examples and evaluation instances for forest fire prediction. In this study, factors influencing frequency of fire are examined. It takes into account elements like temperature, RH, and speed of wind. High temperatures, low humidity, and strong winds all dramatically increase the risk of burning. Also, it has been discovered that there are more forest fires than in other types of surface areas. Data mining techniques should be utilized to predict forest fires because of the forest's elevated risk of such occurrences.\\n\",\n",
       " \"      * Ditipiya Sinha,Vikram Raj,Debasish Dey and A K Das in forest fire detection using energy efficient data forwarding and localization methods in WSN focuses on to consider the connectivity of different types of sensor nodes like GPS linked nodes and nodes which doesn’t have GPS connections, in order to build a model that is cost-effective. Use an SVM-based localization algorithm to locate nodes that are not connected to the GPS. Compared to current localization procedures, it lowers the localization error. The forest can be divided into three zones: High Activity, Mid Activity, and Low Activity. Status of each zone's forest fire is determined using the planned Integrated Rule- based forecasting model. Comparing the suggested model to other traditional models already in use, zone prediction accuracy is greater in the proposed model. It results in quick forest fire detection and improves energy efficiency.\\n\",\n",
       " 'In order to lower cost of sensor deployment, this research suggests a novel localization strategy. In this strategy, unknown node locations are approximated using Support Vector Machine model with lower error than any methods currently in use. Data collection in the forest can be carried out using a variety of sensor motes, including the HNODE and WISMOTE small. Several grids are used to divide up the entire forest area. Along with its eight nearby neighbors, each grid forms a zone.\\n',\n",
       " \"The following are the fundamental presumptions of given model: The main station's location is fixed. There is equal energy in each sensor node, and Grids are used to divide the network. The suggested method is broken down into the following three sub-modules: data forwarding, zone prediction using a semi-supervised classification model, and localization of not known nodes using SVM. Unknown nodes positions are calculated using SVM classification method.\\n\",\n",
       " \"The suggested work takes into account energy constraints during passing of data; which helps in dividing the network into several grids and to locate the suspicious nodes using the SVM approach. This cutting-edge method helps forest sensors interact with one another even when they are not connected to GPS. To determine the grid's status, the suggested approach uses a semi-supervised classification technique. Whereas the Medium Active zone only transmits data infrequently, the High\\n\",\n",
       " 'Active zone continuously transmits data to the ground station. The packet is not sent to the main station in the least active zone. As a result, the network uses less energy and lasts longer thanks to the unique technique that has been suggested.\\n',\n",
       " '      * Ansori, Farhana Mari ,Mukamad Wildan Alaudin,Wayan Firdaus Mahmmudy in Predicting Fire in Forest using Neural Network have chosen Extreme Learning Machines as their proposed model to predict fire. In the area of predictions, neural networks have been successfully used by delivering good perecentage of accuracy in comparison with other algorithms. Extreme Learning Machines is one of the many architectures used by neural networks in their learning processes, was used in this study and produced acceptable results. The ELM approach will be employed in this study to estimate the total amount of land that would be impacted by forest fires based on atmospheric variables, namely wind speed, temperature, moisture and wetness. This method has the greatest accuracy when compared to other methods used in prior studies. By doing this, forest fire catastrophes will be reduced.\\n',\n",
       " '\\n',\n",
       " '\\n',\n",
       " \"Only a single secret layer is used in the ELM's operating system, and its parameters are generated arbitrary. It can give a higher consistency of accuracy in a comparatively shorter period of time than traditional feed forward networks. The training and testing phases of the learning process are separated into two in the ELM technique. The performance and accuracy of the regression model are evaluated using MSE.\\n\",\n",
       " '      * Sudhakar Tripathi, Rina Kumari and Ditipriya Sinha in Semi supervised Clustering Approach based on classification in Wireless Sensor Network for Forst Fire Detecton in 2019. In this research, a unique method is presented that quickly transfers all the detected data to the ground station through non wire communication after identifying the high activity zone in the forest. In this study, a semi- supervised rule-based model is recommended to deciding if a forest zone belongs to a cluster that is highly, moderately, or weakly active. The work is divided into three groups.\\n',\n",
       " '\\n',\n",
       " '\\n',\n",
       " \"The model predicts the area status with 96% predictability when only single elements is shared by the detectors in those area. The proposed model extends the network's lifetime. In contrast, only the sensor nodes in High and Medium Active zones sends data to the ground station. Base station receives fire information from the cluster head in an emergency. Therefore, continual transmission of data is required in the HA zone.\\n\",\n",
       " '\\n',\n",
       " '\\n',\n",
       " 'In 2018, Simon Jones, Dieu Tien Bui , Mahyat Shafapour Tehrany, Francisco Martnez-Llvarez, and Farzin Shabani, developed a unique modelling method employing the Logit-Boost machine\\n',\n",
       " 'learning classifier and multi-source geospatial data for this geographic prediction of tropical potential for areas that caught fire in the forest .\\n',\n",
       " 'This paper evaluates the required of Logit-Boost ensemble-based decision trees (LEDT) for forest fire potential mapping in the Lao Cai region of Vietnam. The authors conducted a review to confirm that the method they proposed had not been previously applied to forest fires. They compared their proposed artificial machine learning approach, which integrates LEDT, with three benchmark models in terms of performance on both training and validation datasets. The Logit-Boost ensemble- based decision trees methodology showed the good performance, achieving 92% prediction capability.\\n',\n",
       " '\\n',\n",
       " '\\n',\n",
       " 'The research aimed to develop a new tool for forest fire potential mapping, using research of Lao Cai Province in the country of Vietnam at the Northwestern region. The authors used a GIS database with 256 fire areas and ten and more forest conditioning factors to train and validate their model. The Logit-Boost ensemble-based decision trees was formulated as a findings recognition model that predicts the likelihood of forest fire occurrence in the area. This research introduced an innovative and proper tool for forest fire potential plotting that has not been explored in the current literature.\\n',\n",
       " '      * Kajol R Singh, K.P. Neethu, K Madhurekaa, A Harita, and Pushpa Mohan authors of Parallel Support Vector Machine model for forest fire prediction in 2021. A parallel machine (SVM) model has been developed effectively train weather data and predict fires of the forest. By leveraging parallel processing, this model reduces the technical time and data required for analysis. In this model incorporates the FWI and other weather parameters to predict forest fires. Its efficiency has been evaluated using data from different countries. The model achieved an RMSE of 63.45 for the Portugal data collection, while the SVM method had an RMSE of 63.5.\\n',\n",
       " 'When datasets are too large, linear SVM may have lower accuracy and become less reliable. In contrast, Parallel SVM can filter the simplest support vectors and generate a more supportable process for predicting forest fires. Therefore, warnings created are decided on the Parallel Support Vector Machine method.\\n',\n",
       " 'Linear SVM computes the linear relationship between features in the source and output, requiring less computational power. In contrast, Parallel SVM performs a non-linear relationship analysis based on the RBF and has a higher computational cost compared to linear SVM. As the size of the source increases, the number of support vectors also increases, resulting in longer computational times. However, changing the algorithm and utilizing parallel computing can improve\\n',\n",
       " 'performance and reduce calculation time.\\n',\n",
       " '\\n',\n",
       " '\\n',\n",
       " 'Raj Vikram and Ditipriya Sinha authors of Fog-Fire: fog assisted IoT enabled forest fire management in 2021. The given forest fire detection model consists of three layers. Forest zones are divided into different zones. According to prediction analysis, the proposed Fog-Fire classification model achieves a prediction accuracy of 94%, which is higher than other methods and approaches.\\n',\n",
       " '      * Minggang Peng and Ying Xie are authors of Forest fire forecasting using the ensemble learning approaches in 2017. This study employed the forest fire dataset from the California University, repository named as the Irvine machine learning, obtained from the north-eastern of Portugal. It was used to predict burned areas and large-scale forest fires.\\n',\n",
       " '\\n',\n",
       " '\\n',\n",
       " \"The patterns demonstrated that ensemble learning techniques has enormous potential for use in more autonomous systems for forest fire prevention and detection. Additionally, the EGB's overall prediction accuracy reached 72.3%, and the DT, SVM, EGB, and DL models all had correct prediction rates higher than 70%. Regarding the EGB3 model's ability to accurately predict large- scale fires, it performed better than the other models. These findings indicate that the EGB technique has a significant deal of potential for predicting the occurrence of big-scale forest fires in other parts of the earth.\\n\",\n",
       " '\\n',\n",
       " '\\n',\n",
       " 'In this method, it also generated in the weighted versions of the data. They can provide significant techniques for forest fire-fighting decision-making. These techniques can ultimately improve forest fire management efficiency worldwide.\\n',\n",
       " '\\n',\n",
       " '\\n',\n",
       " '      * Rajesh Kumar Singh, Rakesh Arya, Firoz Ahmad, and Abdul Qayum, are the authors of predictive modeling of the forest fire using geospatal tools and the strategic allocation of data: eForest-Fire in 2020. This research aimed to develop a Geographic Information system it is an integrated mapping system that uses direct and indirect factors to predict settlements and villages at high risk of forest fires. By integrating socio-economic, geographic, and climatic factors with differential weightage, the system identified 560 out of 5258 settlements at high fire risk.\\n',\n",
       " \"The system was improved by linking and downloading the application into the mobile and web portal, which allowed citizens to send a message to the fire incidents in circumstances of the areas which caught fire and directly communicate with authorities. The study demonstrated the potential of spatial technology in environmental monitoring and resource allocation for forest fire management, and promoted people's participation in fire mitigation efforts.\\n\",\n",
       " 'The e_Forest-Fire app served as an early warning system, promoting e-governance and\\n',\n",
       " 'enhancing the existing machinery of the Forests Department. Technology was demonstrated in the study to be effective in addressing environmental, forest, and wildlife-related issues, as well as in bringing people closer to government functionaries as a result of it.\\n',\n",
       " '      * Ditipriya Sinha and Raj Vikram are the authors of A multimodal framework for Forest fire detection and also monitoring in 2022. The framework is designed to help identify fire-prone areas in a forest and take actions to pretend the spread of forest fires. It is based on a combination of 2 types of detectors are deployed in different zones in the fires forest, which sense the temperature, relative humidity, and drought conditions.\\n',\n",
       " 'The method is a unique and innovative approach to protecting forests from fire. It integrates a Neuro-fuzzy classification-based on Sensor model and a CNN-based Image model to provide a more accurate prediction of the fire status in each zone. The performance analysis of the model has shown that its accuracy is 83%, which will be higher compared to single Sensor.\\n',\n",
       " 'The given framework has significant potential for use in forest fire prevention and mitigation efforts. By accurately predicting the fire status of each zone, the ground station can take precautions and also actions to prevent the spread of forest fires, such as deploying firefighting resources or evacuating nearby settlements. The framework also has the potential to be applied to larger forest fire datasets in the future, allowing for more widespread use in forest management and protection.\\n',\n",
       " '\\n',\n",
       " '\\n',\n",
       " 'Overall, the given model of the Multimodal Forest fire detection method is a valuable tool for protecting forests from fire and can help to lessen the harm that forest fires cause. Its integration of different sensor types and models makes it a unique and effective approach to forest fire detection and prevention.\\n',\n",
       " 'CHAPTER3\\n',\n",
       " 'SYSTEM ARCHITECTURE AND MODULES\\n',\n",
       " '   1. Architecture Diagram\\n',\n",
       " '\\n',\n",
       " '\\n',\n",
       " '\\n',\n",
       " '\\n',\n",
       " ' arch.drawio \\n',\n",
       " '\\n',\n",
       " 'Fig 3.1 Architectural Design of Forest Fire Prediction\\n',\n",
       " '\\n',\n",
       " '\\n',\n",
       " '\\n',\n",
       " '\\n',\n",
       " 'In the figure 3.1, it shows the architectural design of the Clustered forest fire prediction using neural network with effective feature scaling and threshold based feature selection, the dataset having necessary features should be picked up. Then for that particular dataset pre-processing would be done to remove noise and outliers from that particular dataset. In our case, clustering would be done first to group those data present in the dataset. After that Feature scaling would be done so as to make the data uniform in the range of their values. The following feature scaling techniques are available: minimum to maximum, maximum to absolute, robust, quantile transformer, and power transformer. From that the best scaling method is used for further pre-processing of data. Finally Feature selection would be done to remove the unnecessary features which are of less or zero significance for our purpose. Methods of feature selection include Extra Tree Classifier, Forward Selection, Chi Square and Pearson correlation. The best method from these feature selection methods is considered. Finally, we would be left out with necessary and useful uniform scaled features. After pre-processing, the model would be trained on algorithms such as Logistic Regression, SVM & KNN and how it’s performing against each algorithm would take into account. After model training, using neural network forest fire prediction would happen and finally the quantitative and qualitative analysis would be done from those prediction.\\n',\n",
       " '   2. Data Flow Diagram\\n',\n",
       " '\\n',\n",
       " '\\n',\n",
       " '  \\n',\n",
       " '\\n',\n",
       " 'Fig-3.2 - Data Flow Diagram\\n',\n",
       " '\\n',\n",
       " '\\n',\n",
       " 'In the figure 3.2, the data flow diagram for the clustered forest fire prediction system using a neural network with effective feature scaling and threshold-based feature selection illustrates the flow of data from the input dataset through the preprocessing stages to the trained neural network model, emphasizing the significance of feature selection and scaling.\\n',\n",
       " 'Input Data Sources: In this phase ,data from various  such as weather stations and ground sensors are collected and integrated dataset through the preprocessing stages to the trained neural network model, emphasizing the significance of feature selection and scaling.\\n',\n",
       " '\\n',\n",
       " '\\n',\n",
       " '      K-Means Clustering: It is a machine-learning approach that uses unsupervised learning to cluster \\n',\n",
       " 'Feature Scaling: It is a initial step in this machine learning used to standardization or normalize the range of values of various elements or features in a dataset.\\n',\n",
       " 'Feature Selection: It is used to make the module more accurate. It increases the prediction and accuracy power of the algorithms. It selects the most important variables and eliminates the irreleones.\\n',\n",
       " \"Neural Network: It is one of the deep learning based on the machine learning models based on the human brain's structure and functioning. It consists of interconnecting layers, or circuits, that process and transform input data to produce output predictions. The neural networks showed high accuracy for a wide range of applications.\\n\",\n",
       " '   3. Modules\\n',\n",
       " '\\n',\n",
       " '\\n',\n",
       " '      1. K-Means Clustering\\n',\n",
       " '\\n',\n",
       " '\\n',\n",
       " 'This method uses unsupervised machine learning to classify unlabelled data into groups based on their similarity. It is a simple and effective algorithm widely used in various fields such as computer vision, pattern recognition, and marketing.\\n',\n",
       " '\\n',\n",
       " '\\n',\n",
       " 'Elbow Method\\n',\n",
       " '\\n',\n",
       " '\\n',\n",
       " 'The elbow method is a method for discovering the finest number of clusters K in K-means as clusters. The basic plan is to determine the SSD between each data point and its given cluster centroid, as a function of the count of clusters K.\\n',\n",
       " 'Silhouette score\\n',\n",
       " 'It is a metric used to evaluate the fine of clustering quality. And the value of 1 means that the data point is very close to its own group and far from other groups, while a value of -1 means that the data point is closer to other groups than to itself.\\n',\n",
       " '      2. Feature Scaling\\n',\n",
       " 'Feature scaling is a preprocessing step in deep learning used to standardize or normalization the scale of values of various features in a dataset. It is crucial because many machine learning algorithms depend on the input variables scales to be consistent for best results. For feature scaling, normalization and standardization are the two most popular techniques.\\n',\n",
       " 'Standardisation:\\n',\n",
       " \"To standardise, remove the feature's mean from each value and divide the result by the feature's standard deviation.\\n\",\n",
       " 'Normalization:\\n',\n",
       " \"The minimum value of the feature is subtracted from each value before being divided by the feature's range to achieve normalisation.\\n\",\n",
       " '         1. Min-Max Scaling\\n',\n",
       " '         2. Max-Absolute scaling\\n',\n",
       " '         3. Robust Scaling\\n',\n",
       " '         4. Quantile Transformer Scaling\\n',\n",
       " '         5. Power transformer Scaling\\n',\n",
       " '            * Box-Cox transformation\\n',\n",
       " '            * Yeo-Johnson transformation\\n',\n",
       " 'Root Mean Squared Error (RMSE):\\n',\n",
       " 'In training, cross-validation, and monitoring, a single number measures how well a model performs. It calculates the transformation in the middle of values predicted by a module and actual values.\\n',\n",
       " '      3. Feature Selection\\n',\n",
       " \"The variables in the dataset that cannot be used to construct a deep learning model are either redundant or irrelevant. If all of these redundant and meaningless pieces of information are included in the dataset, the general efficacy and precision of the model may deteriorate. Therefore, it is essential to identify and select the most suitable features from the data, as well as to remove any extraneous or unimportant features. Deep learning uses feature selection to do this. The process of selecting features for a model is called feature selection. A feature is a quality that has an impact on or contributes to the resolution of a problem. Feature engineering, which essentially consists of the two processes of feature extraction and selection, is necessary for each phase of the deep learning process. Although feature selection and extraction methods may have the same end in mind, they are very different from one another. The main difference between the two is that feature extraction adds new features whereas feature selection just chooses a portion of the initial feature set. By using only pertinent data, feature selection can limit the input variable for the model and lessen overfitting. It helps to increase the procedure' accuracy. It improves the algorithms' capacity for prediction. It chooses the most important variables and gets rid of the unnecessary or unimportant ones. Using this has the advantages of reducing overfitting, increasing accuracy, and cutting training time. The numerous strategies that have been employed in this specific project are:\\n\",\n",
       " '* Extra Tree Classifier\\n',\n",
       " '* Forward Selection\\n',\n",
       " '* Chi Square\\n',\n",
       " '* Personal Correlation\\n',\n",
       " '\\n',\n",
       " '\\n',\n",
       " '      4. Neural Network:\\n',\n",
       " 'It is a powerful tool in deep learning that can be used for predicting forest fires using numerical datasets. It is one of the best machine-learning models inspired by the structure and the function of the human brain. In this system, information is processed and transmitted through multiple interconnected layers of artificial neurons.\\n',\n",
       " 'Artificial Neural Network\\n',\n",
       " 'This are a class of neural networks that may be used for regression tasks and classifications, both of which are pertinent to forest fire prediction.\\n',\n",
       " 'The Sequential Method\\n',\n",
       " 'The Sequential method is used to build a linear stack of layers in an ANN. This means that each layer in the model is connected to the previous layer, and the data flows through the layers sequentially. The Sequential method is simple to use, making it a popular choice for building ANNs. Dense Layers\\n',\n",
       " 'Dense layers include those which are fully connected, meaning that every node in one layer is linked to every other node in the layer above it. This are employed to execute data transformations and to bring nonlinearity into the model.\\n',\n",
       " 'CHAPTER 4 METHODOLOGY\\n',\n",
       " '   1. Data Preprocessing\\n',\n",
       " '\\n',\n",
       " '\\n',\n",
       " 'Data pre-processing is a set of techniques and processes applied to raw data with the aim of preparing it for analysis by a neural network model for predicting clustered forest fires. This includes cleaning and integrating the data, creating updated features through feature engineering, and scaling the data to ensure each feature is on the same scale. Data pre-processing ensures that the data is consistent, accurate, and prepared for neural network model analysis.\\n',\n",
       " '\\n',\n",
       " '\\n',\n",
       " '      1. K-Means Clustering\\n',\n",
       " '\\n',\n",
       " '\\n',\n",
       " 'It is a powerful unsupervised machine learning algorithm that is commonly used to cluster similar data points based on their characteristics. It is one of the most important and simple algorithms that has been used in a variety of fields including finding recognition, computer vision, and marketing. It can be used to identify alike forest fires based on weather and topography data in the context of Clustered Forest Fire Prediction Using Neural Network with Effective Feature Scaling and Threshold Based Feature Selection. The resulting clusters can be used to predict a neural network to predict the likelihood of forest fires occurring in each cluster by performing appropriate data pre-processing steps like feature scaling and selecting the finest number of clusters.\\n',\n",
       " '\\n',\n",
       " '\\n',\n",
       " 'Here are how the algorithm functions:\\n',\n",
       " 'Initialization: Select the number of clusters, k, and randomly initialize k points called centroids in the data set.\\n',\n",
       " 'Assigning Data Points to Clusters: For each data point, calculate the distance to each centroid and assign the data point to the nearest centroid c using the equ. (4.1).\\n',\n",
       " '\\n',\n",
       " '\\n',\n",
       " '\\n',\n",
       " '\\n',\n",
       " '(4.1) {\"mathml\":\"<math style=\\\\\"font-family:stix;font-size:16px;\\\\\" xmlns=\\\\\"http://www.w3.org/1998/Math/MathML\\\\\"><mstyle mathsize=\\\\\"16px\\\\\"><mi>d</mi><mi>i</mi><mi>s</mi><mi>t</mi><mfenced><mrow><mi>x</mi><mo>,</mo><mi>c</mi></mrow></mfenced><mo>&#xA0;</mo><mo>=</mo><mo>&#xA0;</mo><msqrt><mfenced><mrow><msup><mfenced><mrow><msub><mi>x</mi><mn>1</mn></msub><mo>&#xA0;</mo><mo>-</mo><mo>&#xA0;</mo><msub><mi>c</mi><mn>1</mn></msub></mrow></mfenced><mn>2</mn></msup><mo>&#xA0;</mo><mo>+</mo><mo>&#xA0;</mo><msup><mfenced><mrow><msub><mi>x</mi><mn>2</mn></msub><mo>&#xA0;</mo><mo>-</mo><mo>&#xA0;</mo><msub><mi>c</mi><mn>2</mn></msub></mrow></mfenced><mn>2</mn></msup></mrow></mfenced></msqrt></mstyle></math>\"}  d i s t open parentheses x comma c close parentheses space equals space square root of open parentheses open parentheses x subscript 1 space minus space c subscript 1 close parentheses squared space plus space open parentheses x subscript 2 space minus space c subscript 2 close parentheses squared close parentheses end root \\n',\n",
       " '\\n',\n",
       " '\\n',\n",
       " 'where xi and ci stand for the values of the ith feature of x and c, respectively.\\n',\n",
       " 'Moving Centroids: Calculate the centre of the data points assigned to each centroid, and move the centroid to the intermediate position using the equ (4.2).\\n',\n",
       " '\\n',\n",
       " '\\n',\n",
       " '(4.2) {\"mathml\":\"<math style=\\\\\"font-family:stix;font-size:16px;\\\\\" xmlns=\\\\\"http://www.w3.org/1998/Math/MathML\\\\\"><mstyle mathsize=\\\\\"16px\\\\\"><mi>m</mi><mi>e</mi><mi>a</mi><mi>n</mi><mfenced><mrow><mi>c</mi><mi>l</mi><mi>u</mi><mi>s</mi><mi>t</mi><mi>e</mi><mi>r</mi></mrow></mfenced><mo>&#xA0;</mo><mo>=</mo><mo>&#xA0;</mo><mfenced><mfrac><mn>1</mn><mi>N</mi></mfrac></mfenced><mo>&#xA0;</mo><mo>&#xD7;</mo><mo>&#xA0;</mo><mstyle displaystyle=\\\\\"false\\\\\"><munder><mo>&#x2211;</mo><mrow/></munder></mstyle><mfenced><mrow><msub><mi>x</mi><mn>1</mn></msub><mo>&#xA0;</mo><mo>,</mo><mo>&#xA0;</mo><msub><mi>x</mi><mn>2</mn></msub><mo>&#xA0;</mo><mo>,</mo><mo>&#xA0;</mo><mo>.</mo><mo>.</mo><mo>.</mo><mo>.</mo><mo>,</mo><msub><mi>x</mi><mi>n</mi></msub></mrow></mfenced></mstyle></math>\"}  m e a n open parentheses c l u s t e r close parentheses space equals space open parentheses 1 over N close parentheses space cross times space sum for blank of open parentheses x subscript 1 space comma space x subscript 2 space comma space.... comma x subscript n close parentheses \\n',\n",
       " '\\n',\n",
       " '\\n',\n",
       " 'xi stands for the value of the ith feature of each data point in the cluster, and N denotes the total number of data points in the cluster.\\n',\n",
       " 'Output: The final positions of the centroids represent the k clusters, and each data point is assigned to the nearest centroid.\\n',\n",
       " 'It, also known as the WCSS, seeks to minimize the sum of the squared distances between the data sets and their specified centres using the equ. (4.3).\\n',\n",
       " '\\n',\n",
       " '\\n',\n",
       " '(4.3) {\"mathml\":\"<math style=\\\\\"font-family:stix;font-size:16px;\\\\\" xmlns=\\\\\"http://www.w3.org/1998/Math/MathML\\\\\"><mstyle mathsize=\\\\\"16px\\\\\"><mi>W</mi><mi>C</mi><mi>S</mi><mi>S</mi><mo>&#xA0;</mo><mo>=</mo><mo>&#xA0;</mo><munder><mo>&#x2211;</mo><mrow/></munder><mo>&#xA0;</mo><msub><mi>P</mi><mrow><mi>i</mi><mo>&#xA0;</mo><mi>i</mi><mi>n</mi><mo>&#xA0;</mo><mi>c</mi><mi>l</mi><mi>u</mi><mi>s</mi><mi>t</mi><mi>e</mi><mi>r</mi><mo>&#xA0;</mo><mn>1</mn></mrow></msub><mo>&#xA0;</mo><mi>d</mi><mi>i</mi><mi>s</mi><mi>t</mi><msup><mfenced><mrow><msub><mi>P</mi><mi>i</mi></msub><mo>,</mo><msub><mi>C</mi><mn>1</mn></msub></mrow></mfenced><mn>2</mn></msup><mo>&#xA0;</mo><mo>+</mo><mo>&#xA0;</mo><mstyle displaystyle=\\\\\"false\\\\\"><munder><mrow><mo>&#x2211;</mo><mo>&#xA0;</mo><msub><mi>P</mi><mrow><mi>i</mi><mo>&#xA0;</mo><mi>i</mi><mi>n</mi><mo>&#xA0;</mo><mi>c</mi><mi>l</mi><mi>u</mi><mi>s</mi><mi>t</mi><mi>e</mi><mi>r</mi><mo>&#xA0;</mo><mn>2</mn><mo>&#xA0;</mo></mrow></msub><mo>&#xA0;</mo><mi>d</mi><mi>i</mi><mi>s</mi><mi>t</mi><msup><mfenced><mrow><msub><mi>P</mi><mi>i</mi></msub><mo>,</mo><mo>&#xA0;</mo><msub><mi>C</mi><mn>2</mn></msub></mrow></mfenced><mn>2</mn></msup><mo>&#xA0;</mo></mrow><mrow/></munder></mstyle></mstyle></math>\"}  W C S S space equals space sum for blank of space P subscript i space i n space c l u s t e r space 1 end subscript space d i s t open parentheses P subscript i comma C subscript 1 close parentheses squared space plus space stack sum space P subscript i space i n space c l u s t e r space 2 space end subscript space d i s t open parentheses P subscript i comma space C subscript 2 close parentheses squared space with blank below \\n',\n",
       " '\\n',\n",
       " '\\n',\n",
       " ' {\"mathml\":\"<math style=\\\\\"font-family:stix;font-size:16px;\\\\\" xmlns=\\\\\"http://www.w3.org/1998/Math/MathML\\\\\"><mstyle mathsize=\\\\\"16px\\\\\"><munder><mrow><mo>&#x2211;</mo><msub><mi>P</mi><mrow><mi>i</mi><mo>&#xA0;</mo><mi>i</mi><mi>n</mi><mo>&#xA0;</mo><mi>c</mi><mi>l</mi><mi>u</mi><mi>s</mi><mi>t</mi><mi>e</mi><mi>r</mi><mo>&#xA0;</mo><mn>1</mn></mrow></msub><mo>&#xA0;</mo><mi>d</mi><mi>i</mi><mi>s</mi><mi>t</mi><msup><mfenced><mrow><msub><mi>P</mi><mi>i</mi></msub><mo>,</mo><mo>&#xA0;</mo><msub><mi>C</mi><mn>1</mn></msub></mrow></mfenced><mn>2</mn></msup></mrow><mrow/></munder></mstyle></math>\"}  stack sum P subscript i space i n space c l u s t e r space 1 end subscript space d i s t open parentheses P subscript i comma space C subscript 1 close parentheses squared with blank below \\n',\n",
       " ' : It is the sum of the squares of the distances between each data point and its centroid within a cluster1, and the same is true for the other terms.\\n',\n",
       " 'Elbow Method:\\n',\n",
       " 'It is a method for determining the finest number of clusters K in K-means as clusters. The basic idea is to determine the sum of the squared distances (SSD) in the middle of the each data point and its given node centroid, as a method of the number of clusters K.\\n',\n",
       " 'The algorithm for the elbow method is as follows:\\n',\n",
       " '* Choose different types of K values to check (we used 1 to 20).\\n',\n",
       " '* For each value of K, perform K-means [23] clustering on the dataset. Calculate the SSD between each data point and its assigned centroid.\\n',\n",
       " '* Plot the SSD against the number of clusters K.\\n',\n",
       " '* Look for the \"elbow\" point on the curve - this is the point where the SSD starts to drop, indicating that additional clusters do not significantly improve cluster performance.\\n',\n",
       " '* Select K as the number of clusters that the dataset requires, as this is the correct number.\\n',\n",
       " 'Silhouette score:\\n',\n",
       " 'It is a metric used to evaluate the classification results. A value of 1 means that the data point is very near to its own group and far from other groups, while a value of -1 means that the data point is closer to other groups than to itself calculating using the equ. (4.4). It is an effective tool for evaluating the quality of team results and can help identify areas for improvement. {\"mathml\":\"<math style=\\\\\"font-family:stix;font-size:16px;\\\\\" xmlns=\\\\\"http://www.w3.org/1998/Math/MathML\\\\\"><mstyle mathsize=\\\\\"16px\\\\\"><mi>S</mi><mi>c</mi><mi>o</mi><mi>r</mi><mi>e</mi><mo>&#xA0;</mo><mo>=</mo><mo>&#xA0;</mo><mfrac><mfenced><mrow><mi>b</mi><mo>-</mo><mi>a</mi></mrow></mfenced><mrow><mi>m</mi><mi>a</mi><mi>x</mi><mfenced><mrow><mi>a</mi><mo>,</mo><mi>b</mi></mrow></mfenced></mrow></mfrac></mstyle></math>\"}  S c o r e space equals space fraction numerator open parentheses b minus a close parentheses over denominator m a x open parentheses a comma b close parentheses end fraction \\n',\n",
       " '(4.4)\\n',\n",
       " 'a is the Mean distance between each point within a cluster,\\n',\n",
       " '\\n',\n",
       " '\\n',\n",
       " 'b is the mean of the inter-cluster distance\\n',\n",
       " '\\n',\n",
       " '\\n',\n",
       " '      2. Feature Scaling:\\n',\n",
       " '\\n',\n",
       " '\\n',\n",
       " 'Feature scaling plays a significant role in data pre-processing for Clustered Forest Fire Prediction using Neural Network with Effective Feature Scaling on Threshold Based Feature Selection. By transforming forest fire features to the same scale, feature scaling improves the performance of the neural network.\\n',\n",
       " '\\n',\n",
       " '\\n',\n",
       " 'There are various methods of feature scaling that can be used for this task, including normalization and standardization. In this context, it is also critical to ensure that the scaling method is applied consistently across all the features. By doing so, biases can be avoided in neural network training.\\n',\n",
       " '\\n',\n",
       " '\\n',\n",
       " 'Overall, effective feature scaling and selection can greatly improve the neural network performance for Clustered Forest Fire Prediction.\\n',\n",
       " '\\n',\n",
       " '\\n',\n",
       " 'Standardization\\n',\n",
       " 'It is done by subtracting the feature mean from each value and dividing it by the standard deviation. This scale\\'s feature has a mean of 0 and a standard deviation of 1. {\"mathml\":\"<math style=\\\\\"font-family:stix;font-size:16px;\\\\\" xmlns=\\\\\"http://www.w3.org/1998/Math/MathML\\\\\"><mstyle mathsize=\\\\\"16px\\\\\"><mover><mi>x</mi><mo>~</mo></mover><mo>&#xA0;</mo><mo>&#xA0;</mo><mo>=</mo><mo>&#xA0;</mo><mfrac><mfenced><mrow><mi>x</mi><mo>&#xA0;</mo><mo>-</mo><mo>&#xA0;</mo><mi>m</mi><mi>e</mi><mi>a</mi><mi>n</mi><mfenced><mi>x</mi></mfenced></mrow></mfenced><msqrt><mi>v</mi><mi>a</mi><mi>r</mi><mfenced><mi>x</mi></mfenced></msqrt></mfrac></mstyle></math>\"}  x with tilde on top space space equals space fraction numerator open parentheses x space minus space m e a n open parentheses x close parentheses close parentheses over denominator square root of v a r open parentheses x close parentheses end root end fraction \\n',\n",
       " '\\n',\n",
       " '\\n',\n",
       " '\\n',\n",
       " '\\n',\n",
       " '(4.5)\\n',\n",
       " '\\n',\n",
       " '\\n',\n",
       " 'Normalization\\n',\n",
       " \"Normalization is done by subtracting the least value of the feature from each value and then dividing by the feature scale. It is useful when feature distribution is not known or when there are outliers in the data. A feature's values are scaled between 0 and 1.\\n\",\n",
       " '1. Min-Max Scaling\\n',\n",
       " '\\n',\n",
       " '\\n',\n",
       " 'It is the method subtracts the least value of the feature and divides it by the difference between the highest and least values. In this method scales the data to a fixed scale, usually between 0 and 1.\\n',\n",
       " '\\n',\n",
       " '\\n',\n",
       " '\\n',\n",
       " '\\n',\n",
       " '(4.6) {\"mathml\":\"<math style=\\\\\"font-family:stix;font-size:16px;\\\\\" xmlns=\\\\\"http://www.w3.org/1998/Math/MathML\\\\\"><mstyle mathsize=\\\\\"16px\\\\\"><mover><mi>x</mi><mo>~</mo></mover><mo>&#xA0;</mo><mo>&#xA0;</mo><mo>=</mo><mo>&#xA0;</mo><mfrac><mfenced><mrow><mi>x</mi><mo>&#xA0;</mo><mo>-</mo><mo>&#xA0;</mo><mi>m</mi><mi>i</mi><mi>n</mi><mfenced><mi>x</mi></mfenced></mrow></mfenced><mrow><mi>m</mi><mi>a</mi><mi>x</mi><mfenced><mi>x</mi></mfenced><mo>&#xA0;</mo><mo>-</mo><mo>&#xA0;</mo><mi>m</mi><mi>i</mi><mi>n</mi><mfenced><mi>x</mi></mfenced></mrow></mfrac></mstyle></math>\"}  x with tilde on top space space equals space fraction numerator open parentheses x space minus space m i n open parentheses x close parentheses close parentheses over denominator m a x open parentheses x close parentheses space minus space m i n open parentheses x close parentheses end fraction \\n',\n",
       " '2. Max-Absolute scaling\\n',\n",
       " '\\n',\n",
       " '\\n',\n",
       " 'Max-Absolute Scaling method works by dividing each feature value by the maximum absolute value of that feature using the equ.(4.7). This ensures that the absolute value of each feature is no greater than 1, and that the relative values of the features are preserved. In this method ranges the data to a fixed scale, usually between -1 and 1. This method uses the maximum and hence it is also sensitive to outliers like MinMaxScaler.\\n',\n",
       " '\\n',\n",
       " '\\n',\n",
       " '(4.7) {\"mathml\":\"<math style=\\\\\"font-family:stix;font-size:16px;\\\\\" xmlns=\\\\\"http://www.w3.org/1998/Math/MathML\\\\\"><mstyle mathsize=\\\\\"16px\\\\\"><msup><msub><mi>X</mi><mi>i</mi></msub><mo>\\'</mo></msup><mo>=</mo><mo>&#xA0;</mo><mfrac><msub><mi>X</mi><mi>i</mi></msub><mrow><mi>a</mi><mi>b</mi><mi>s</mi><mfenced><msub><mi>X</mi><mrow><mi>m</mi><mi>a</mi><mi>x</mi></mrow></msub></mfenced></mrow></mfrac></mstyle></math>\"}  X subscript i to the power of apostrophe equals space fraction numerator X subscript i over denominator a b s open parentheses X subscript m a x end subscript close parentheses end fraction \\n',\n",
       " '\\n',\n",
       " '\\n',\n",
       " '3. Robust Scaling\\n',\n",
       " '\\n',\n",
       " '\\n',\n",
       " 'Robust scaling is designed to scale the data while minimizing the effect of outliers. It works by subtracting the median of each feature from each data point, and then dividing by the interquartile range (IQR) of the feature. The IQR is the subtracting the 75th percentile and the 25th percentile of the data using the equ.(4.8). So that it is less affected by outliers.\\n',\n",
       " '\\n',\n",
       " '\\n',\n",
       " '(4.8) {\"mathml\":\"<math style=\\\\\"font-family:stix;font-size:16px;\\\\\" xmlns=\\\\\"http://www.w3.org/1998/Math/MathML\\\\\"><mstyle mathsize=\\\\\"16px\\\\\"><msup><msub><mi>X</mi><mi>i</mi></msub><mo>\\'</mo></msup><mo>&#xA0;</mo><mo>&#xA0;</mo><mo>=</mo><mo>&#xA0;</mo><mfrac><mrow><mfenced><mrow><msub><mi>x</mi><mi>i</mi></msub><mo>&#xA0;</mo><mo>-</mo><mo>&#xA0;</mo><msub><mi>x</mi><mrow><mi>m</mi><mi>e</mi><mi>d</mi></mrow></msub></mrow></mfenced><mo>&#xA0;</mo></mrow><mrow><mi>I</mi><mi>Q</mi><mi>R</mi></mrow></mfrac><mo>&#xA0;</mo><mo>=</mo><mo>&#xA0;</mo><mfrac><mfenced><mrow><msub><mi>x</mi><mi>i</mi></msub><mo>&#xA0;</mo><mo>-</mo><mo>&#xA0;</mo><msub><mi>x</mi><mrow><mi>m</mi><mi>e</mi><mi>d</mi></mrow></msub></mrow></mfenced><mrow><msub><mi>x</mi><mrow><mn>75</mn><mo>&#xA0;</mo></mrow></msub><mo>&#xA0;</mo><mo>-</mo><mo>&#xA0;</mo><msub><mi>x</mi><mn>25</mn></msub><mo>&#xA0;</mo></mrow></mfrac></mstyle></math>\"}  X subscript i to the power of apostrophe space space equals space fraction numerator open parentheses x subscript i space minus space x subscript m e d end subscript close parentheses space over denominator I Q R end fraction space equals space fraction numerator open parentheses x subscript i space minus space x subscript m e d end subscript close parentheses over denominator x subscript 75 space end subscript space minus space x subscript 25 space end fraction \\n',\n",
       " '\\n',\n",
       " '\\n',\n",
       " '\\n',\n",
       " '\\n',\n",
       " '4. Quantile Transformer Scaling\\n',\n",
       " '\\n',\n",
       " '\\n',\n",
       " 'Quantile Transformer Scaling is designed to transform the data to follow a uniform or normal distribution. It is an inverse function of the cumulative Distribution Function (CDF). It is used to remove outliers or fit the Normal Distribution. This is defined on the unit interval (0, 1)\\n',\n",
       " '5. Power transformer Scaling\\n',\n",
       " '\\n',\n",
       " '\\n',\n",
       " 'Power transformer Scaling is designed to transform the data to follow a normal or Gaussian distribution. This method works by applying a power transformation to the data that can adjust the skewness of the data and make it more symmetric. The power function is defined on the unit interval (0, 1). In this scaling method uses two types of power transformations: the the Yeo-Johnson transformation and the Box-Cox transformation.\\n',\n",
       " 'Box-Cox transformation:\\n',\n",
       " '\\n',\n",
       " '\\n',\n",
       " '(4.9) {\"mathml\":\"<math style=\\\\\"font-family:stix;font-size:16px;\\\\\" xmlns=\\\\\"http://www.w3.org/1998/Math/MathML\\\\\"><mstyle mathsize=\\\\\"16px\\\\\"><mi>y</mi><mo>&#xA0;</mo><mo>=</mo><mo>&#xA0;</mo><mfrac><mfenced><mrow><msup><mi>x</mi><mi>&#x3BB;</mi></msup><mo>&#xA0;</mo><mo>-</mo><mo>&#xA0;</mo><mn>1</mn></mrow></mfenced><mi>&#x3BB;</mi></mfrac></mstyle></math>\"}  y space equals space fraction numerator open parentheses x to the power of lambda space minus space 1 close parentheses over denominator lambda end fraction \\n',\n",
       " 'where lambda is the power parameter.\\n',\n",
       " 'The power parameter is estimated using maximum likelihood estimation, which searches for the optimal value of lambda that makes the data as close to normal distribution as possible using equ.(4.9).\\n',\n",
       " '\\n',\n",
       " '\\n',\n",
       " 'Yeo-Johnson transformation:\\n',\n",
       " '\\n',\n",
       " '\\n',\n",
       " '(4.10) {\"mathml\":\"<math style=\\\\\"font-family:stix;font-size:16px;\\\\\" xmlns=\\\\\"http://www.w3.org/1998/Math/MathML\\\\\"><mstyle mathsize=\\\\\"16px\\\\\"><mi>y</mi><mo>&#xA0;</mo><mo>=</mo><mo>&#xA0;</mo><mfrac><mfenced open=\\\\\"[\\\\\" close=\\\\\"]\\\\\"><mrow><msup><mfenced><mrow><mi>x</mi><mo>+</mo><mn>1</mn></mrow></mfenced><mi>&#x3BB;</mi></msup><mo>&#xA0;</mo><mo>-</mo><mn>1</mn></mrow></mfenced><mi>&#x3BB;</mi></mfrac></mstyle></math>\"}  y space equals space fraction numerator open square brackets open parentheses x plus 1 close parentheses to the power of lambda space minus 1 close square brackets over denominator lambda end fraction \\n',\n",
       " '\\n',\n",
       " '\\n',\n",
       " 'for x >= 0\\n',\n",
       " '\\n',\n",
       " '\\n',\n",
       " '(4.11) {\"mathml\":\"<math style=\\\\\"font-family:stix;font-size:16px;\\\\\" xmlns=\\\\\"http://www.w3.org/1998/Math/MathML\\\\\"><mstyle mathsize=\\\\\"16px\\\\\"><mi>y</mi><mo>&#xA0;</mo><mo>=</mo><mo>&#xA0;</mo><mo>-</mo><mo>&#xA0;</mo><mfrac><mfenced open=\\\\\"[\\\\\" close=\\\\\"]\\\\\"><mrow><msup><mfenced><mrow><mo>|</mo><mi>x</mi><mo>|</mo><mo>&#xA0;</mo><mo>+</mo><mo>&#xA0;</mo><mn>1</mn></mrow></mfenced><mi>&#x3BB;</mi></msup><mo>&#xA0;</mo><mo>-</mo><mo>&#xA0;</mo><mn>1</mn></mrow></mfenced><mi>&#x3BB;</mi></mfrac></mstyle></math>\"}  y space equals space minus space fraction numerator open square brackets open parentheses vertical line x vertical line space plus space 1 close parentheses to the power of lambda space minus space 1 close square brackets over denominator lambda end fraction \\n',\n",
       " '\\n',\n",
       " '\\n',\n",
       " 'for x < 0\\n',\n",
       " 'abs(x) is the absolute value of x\\n',\n",
       " '\\n',\n",
       " '\\n',\n",
       " 'Root Mean Squared Error (RMSE)\\n',\n",
       " '\\n',\n",
       " '\\n',\n",
       " 'It can be used as an evaluation metric for evaluating the accuracy of a Clustered Forest Fire Prediction using Neural Network with effective Feature Scaling and Threshold Based Feature Selection. To use RMSE, divide the dataset into training and testing datasets. Perform feature scaling on the training dataset to ensure that all features are on the same scale. In addition, threshold-based feature selection should be utilized to find the most important and suitable features for predicting forest fires. During training, the RMSE should be used as an evaluation metric to monitor the model\\'s performance using the equ. (4.12). After training, the model\\'s performance on the testing dataset can be evaluated using the RMSE value, with a lower RMSE value indicating better performance. You can use RMSE to ensure that your Clustered Forest Fire Prediction model predicts forest fires as accurately as possible. {\"mathml\":\"<math style=\\\\\"font-family:stix;font-size:16px;\\\\\" xmlns=\\\\\"http://www.w3.org/1998/Math/MathML\\\\\"><mstyle mathsize=\\\\\"16px\\\\\"><mi>R</mi><mi>M</mi><mi>S</mi><mi>E</mi><mo>&#xA0;</mo><mo>=</mo><mo>&#xA0;</mo><msqrt><mfrac><mfenced><mrow><munderover><mo>&#x2211;</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi>N</mi></munderover><mo>&#xA0;</mo><mo>|</mo><mo>|</mo><mo>&#xA0;</mo><mi>y</mi><mfenced><mi>i</mi></mfenced><mo>&#xA0;</mo><mo>-</mo><mo>&#xA0;</mo><mstyle displaystyle=\\\\\"true\\\\\"><mover><mi>y</mi><mo>&#x2227;</mo></mover></mstyle><mo>&#xA0;</mo><mfenced><mi>i</mi></mfenced><mo>|</mo><msup><mo>|</mo><mn>2</mn></msup></mrow></mfenced><mi>N</mi></mfrac></msqrt></mstyle></math>\"}  R M S E space equals space square root of fraction numerator open parentheses sum from i equals 1 to N of space vertical line vertical line space y open parentheses i close parentheses space minus space y with logical and on top space open parentheses i close parentheses vertical line vertical line squared close parentheses over denominator N end fraction end root \\n',\n",
       " '\\n',\n",
       " '\\n',\n",
       " '\\n',\n",
       " '\\n',\n",
       " '(4.12)\\n',\n",
       " '\\n',\n",
       " '\\n',\n",
       " '\\n',\n",
       " '\\n',\n",
       " '* where N is the data points count,\\n',\n",
       " '* y(i) is the i-th measurement, and\\n',\n",
       " '* y ̂(i) is its corresponding prediction.\\n',\n",
       " '      1. Data Exploration:\\n',\n",
       " '\\n',\n",
       " '\\n',\n",
       " 'Data exploration is an critical step in any deep learning project, however Clustered Forest Fire Prediction Using Neural Network with Effective Feature Scaling and Threshold Based Feature Selection. To begin, it is critical to understand the data source and collection methods in order to identify any potential biases or limitations in the dataset. Once the data source has been identified, the various features available in the dataset must be identified.\\n',\n",
       " '\\n',\n",
       " '\\n',\n",
       " \"In this case, the target variable is forest fires. Checking for data quality issues, such as values in the dataset are missing or outliers, is also critical, as these can reduce the prediction model's accuracy. Data visualisation techniques such as plots and histograms can be used to identify plots and correlations in data. These patterns and correlations can be used to determine which features are most significant for predicting forest fires and also how they relate to each another. It is a critical factor in increasing the model's accuracy and creating a more effective prediction system.\\n\",\n",
       " '\\n',\n",
       " '\\n',\n",
       " 'Data source and collection\\n',\n",
       " '\\n',\n",
       " '\\n',\n",
       " 'Understanding the data source and collection methods is critical for identifying any biases or limitations in the dataset. This can be done by reviewing documentation or referring to information sources.\\n',\n",
       " 'Data quality\\n',\n",
       " '\\n',\n",
       " '\\n',\n",
       " \"Check for missing values, outliers, and any other issues with data quality that might impact the prediction model's accuracy. These issues can be addressed by imputing missing values, removing outliers, or employing statistical methods.\\n\",\n",
       " 'Data visualization\\n',\n",
       " '\\n',\n",
       " '\\n',\n",
       " 'To identify patterns and correlations in data by visualizing it with plots, histograms, and other visualization tools. This can help you understand which features are most relevant for predicting forest fires and how they are related to one another.\\n',\n",
       " 'Descriptive statistics\\n',\n",
       " '\\n',\n",
       " '\\n',\n",
       " 'Calculate descriptive statistics, such as average, median, and standard deviation, to better understand the distribution of features in the dataset.\\n',\n",
       " 'Correlation analysis\\n',\n",
       " '\\n',\n",
       " '\\n',\n",
       " 'Conduct correlation analysis to identify the relationship between features and the target variable.\\n',\n",
       " '\\n',\n",
       " '\\n',\n",
       " '   2. Feature Selection\\n',\n",
       " '\\n',\n",
       " '\\n',\n",
       " 'Extra Tree Classifier\\n',\n",
       " '\\n',\n",
       " '\\n',\n",
       " 'For choosing the features, a tree-based supervised model is used. It uses a filter-based approach. This approach builds several tree models at random from the training dataset. Next, rather than dividing the data into locally optimal values using Gini or entropy, it sorts the characteristics that have received the most votes. The choice of a splitting value at random for a feature is the distinguishing feature. It reduces variance and lessens the likelihood of overfitting the model.\\n',\n",
       " '\\n',\n",
       " '\\n',\n",
       " 'Chi Squared\\n',\n",
       " '\\n',\n",
       " '\\n',\n",
       " 'In statistical models, a chi-square test is employed to examine the dependency of features. The model calculates difference between anticipated and original value. Correlation between variables is stronger when the Chi-square value is higher and lower when the value is higher using the equ.(4.13). The initial assumption that the qualities are independent creates the null hypothesis.\\n',\n",
       " 'Chi Square Test:\\n',\n",
       " 'xc2 =\\n',\n",
       " '________________\\n',\n",
       " 'Σ(Oi − Ei )2\\n',\n",
       " 'Ei  \\n',\n",
       " '________________\\n',\n",
       " '\\n',\n",
       " '\\n',\n",
       " '\\n',\n",
       " '\\n',\n",
       " '(4.13)\\n',\n",
       " '\\n',\n",
       " '\\n',\n",
       " 'O = Observed Value, E = Expected Value, and c = Degrees of Freedom\\n',\n",
       " 'Forward Selection\\n',\n",
       " \"This model is a wrapper type. It assesses the combined predictive ability of the features. The best-performing features are returned as a set. The processes in this procedure include training n models using n features each separately, then evaluating the results. then decide on the variable that performs the best. Repeat the procedure, introducing each variable separately. The variable that results in the greatest improvement is kept. Continue doing this until the model's performance doesn't noticeably improve.\\n\",\n",
       " 'Pearson Correlation\\n',\n",
       " \"The linear relationship between two properties is measured by creating a correlation matrix. How closely related the two traits are to one another is indicated by a value between -1 and 1 using for fire prediction, a robust machine learning pipeline can incorporate Chi-Squared feature selection, clustering, and a Random Forest classifier to enhance model accuracy and interpretability. Initially, relevant environmental and meteorological data—such as temperature, humidity, wind speed, and vegetation indices—are collected and preprocessed. Chi-Squared feature selection is then applied to identify and retain the most statistically significant features related to the target variable, typically a binary indicator of fire occurrence. This step reduces dimensionality and helps the model focus on the most relevant inputs. To uncover underlying spatial or feature-based groupings in the data, clustering techniques like K-Means or DBSCAN can be employed; the resulting cluster labels may serve as an additional feature to enrich the model's contextual understanding. Finally, a Random Forest algorithm is used for prediction, leveraging its ability to handle non-linear relationships and feature interactions. This integrated approach—combining statistical filtering, unsupervised learning, and ensemble methods—can significantly improve the reliability of fire risk prediction models. The Chi-Squared test ensures that only the features with the strongest correlation to fire events are considered, which enhances model performance and reduces overfitting. Clustering can help in recognizing patterns such as high-risk zones or seasonal fire-prone areas, which might not be evident through raw features alone. The inclusion of cluster information adds a spatial or contextual layer to the dataset, making the model more sensitive to regional variations. The Random Forest classifier, known for its robustness and resistance to noise, can handle both numerical and categorical features efficiently. Its ensemble structure, made of multiple decision trees, ensures better generalization on unseen data. Feature importance scores provided by the Random Forest also offer valuable insights into which environmental factors contribute most to fire occurrence. When combined, these techniques support not only accurate prediction but also interpretable and actionable insights for early warning systems. This pipeline can be further improved by incorporating real-time data streams and satellite imagery for continuous learning. Ultimately, such models can aid authorities in resource allocation and timely interventions to mitigate wildfire risks. Moreover, periodic retraining of the model with updated data can help maintain its accuracy in changing environmental conditions. Integrating Geographic Information System (GIS) data can further enhance the spatial resolution of predictions. Additionally, visualization tools can be developed to display fire risk maps based on the model's output, aiding decision-makers and emergency responders. This comprehensive approach not only supports proactive fire management but also contributes to environmental protection and public safety.\\n\",\n",
       " 'the equ.(4.14). It determines the correlation between each feature and the desired outcome. A correlation between the features is shown by a value of 1, -1, or 0, depending on how they are related. Only metric variables should be used for Pearson correlations. It is sometimes indicated as \"correlation\" or the \"product moment correlation coefficient\" (PMCC). The formula for the correlation is:\\n',\n",
       " '\\n',\n",
       " '\\n',\n",
       " '\\n',\n",
       " '\\n',\n",
       " 'here,\\n',\n",
       " '________________\\n',\n",
       " '𝑟 =        𝑁𝛴𝑥𝑦−(𝛴𝑥)(𝛴𝑦)\\n',\n",
       " '√[𝑁𝛴𝑥2−(𝛴𝑥)2][𝑁𝛴𝑦2−(𝛴𝑦)2]  \\n',\n",
       " '________________\\n',\n",
       " '(4.14)\\n',\n",
       " ...]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.readlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "a90dfb4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# write \n",
    "x = open(\"chakravardhan.docx.txt\",mode = 'w')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "03de3cfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "y = \" my name is chakravardhan\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "d3a5de8f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "25"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.write(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "d9912d39",
   "metadata": {},
   "outputs": [],
   "source": [
    "x.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "7fac9337",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = open(\"chakravardhan.docx.txt\",mode = 'r')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "0947277c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[' my name is chakravardhan']"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.readlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "55484f42",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = open(\"chakravardhan.docx.txt\",mode = 'a')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "76c3620f",
   "metadata": {},
   "outputs": [],
   "source": [
    "y = \"iam studing in innomatrics\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "75c469bd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "26"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.write(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "14f411ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "x.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "4cadb5d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = open(\"chakravardhan.docx.txt\",mode = 'r')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "9e50fd6d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[' my name is chakravardhaniam studing in innomatrics']"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.readlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "324bec91",
   "metadata": {},
   "outputs": [],
   "source": [
    "x.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc180310",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
